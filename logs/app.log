2024-06-11 23:43:00,986 - INFO - Total parameters count: 4386178
2024-06-11 23:43:00,986 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-11 23:45:05,574 - INFO - Evaluation results: {'eval_loss': 0.6381635665893555, 'eval_accuracy': 0.8291284403669725, 'eval_runtime': 0.1986, 'eval_samples_per_second': 4390.66, 'eval_steps_per_second': 548.833, 'epoch': 3.0}
2024-06-11 23:52:00,719 - INFO - Total parameters count: 4386178
2024-06-11 23:52:00,719 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-11 23:53:53,752 - INFO - Evaluation results: {'eval_loss': 0.6673561334609985, 'eval_accuracy': 0.8268348623853211, 'eval_runtime': 0.1709, 'eval_samples_per_second': 5102.097, 'eval_steps_per_second': 637.762, 'epoch': 3.0}
2024-06-11 23:53:53,752 - INFO - Results saved to out/evaluation_results.json
2024-06-12 00:06:44,242 - INFO - Total parameters count: 4386178
2024-06-12 00:06:44,242 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-12 00:12:24,964 - INFO - Total parameters count: 4386178
2024-06-12 00:12:24,964 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-12 00:24:39,859 - INFO - Evaluation results: {'eval_loss': 0.4172626733779907, 'eval_accuracy': 0.8086322038090527, 'eval_f1': 0.7574380035740038, 'eval_runtime': 7.7254, 'eval_samples_per_second': 5233.368, 'eval_steps_per_second': 654.203, 'epoch': 3.0}
2024-06-12 00:24:39,859 - INFO - Results saved to out/evaluation_results.json
2024-06-12 00:30:01,625 - INFO - Total parameters count: 4386178
2024-06-12 00:30:01,625 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-12 00:30:22,018 - INFO - Total parameters count: 4386178
2024-06-12 00:30:22,018 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-12 09:27:30,962 - INFO - Evaluation results: {'eval_loss': 0.6683989763259888, 'eval_accuracy': 0.5969247666117518, 'eval_runtime': 3.4429, 'eval_samples_per_second': 1586.737, 'eval_steps_per_second': 49.667, 'epoch': 3.0}
2024-06-12 09:27:30,962 - INFO - Results saved to out/evaluation_results.json
2024-06-12 10:01:40,997 - INFO - Total parameters count: 4386307
2024-06-12 10:01:40,997 - INFO - Trainable parameters count: 387 (0.00882291184816749%)
2024-06-12 10:04:30,610 - INFO - Evaluation results: {'eval_loss': 1.0894393920898438, 'eval_accuracy': 0.3804381049414162, 'eval_runtime': 11.5101, 'eval_samples_per_second': 852.726, 'eval_steps_per_second': 26.672, 'epoch': 3.0}
2024-06-12 10:04:30,610 - INFO - Results saved to out/evaluation_results.json
2024-06-12 10:21:05,477 - INFO - Total parameters count: 4388227
2024-06-12 10:21:05,477 - INFO - Trainable parameters count: 2307 (0.05257248542520703%)
2024-06-12 11:04:51,467 - INFO - Total parameters count: 4386178
2024-06-12 11:04:51,467 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-12 13:02:09,081 - INFO - Total parameters count: 4390276
2024-06-12 13:02:09,081 - INFO - Trainable parameters count: 4098 (0.09334265089484124%)
2024-06-12 13:03:46,265 - INFO - Total parameters count: 4390276
2024-06-12 13:03:46,265 - INFO - Trainable parameters count: 4098 (0.09334265089484124%)
2024-06-12 13:05:12,235 - INFO - Total parameters count: 4390276
2024-06-12 13:05:12,235 - INFO - Trainable parameters count: 4098 (0.09334265089484124%)
2024-06-12 13:22:06,403 - INFO - Total parameters count: 4390276
2024-06-12 13:22:06,403 - INFO - Trainable parameters count: 4098 (0.09334265089484124%)
2024-06-12 13:22:27,919 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-12 13:23:27,810 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-12 13:29:03,320 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-12 13:30:09,595 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-12 13:32:21,187 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-12 13:34:05,831 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-12 13:36:13,552 - INFO - Evaluation results: {'eval_loss': 0.678795337677002, 'eval_accuracy': 0.5793520043931906, 'eval_runtime': 0.7982, 'eval_samples_per_second': 6844.261, 'eval_steps_per_second': 214.236, 'epoch': 3.0}
2024-06-12 13:36:13,554 - INFO - Results saved to out/evaluation_results.json
2024-06-12 13:46:56,261 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-12 13:48:10,200 - INFO - Evaluation results: {'eval_loss': 0.6836631298065186, 'eval_accuracy': 0.5744096650192202, 'eval_runtime': 0.6337, 'eval_samples_per_second': 8621.377, 'eval_steps_per_second': 269.862, 'epoch': 3.0}
2024-06-12 13:48:10,200 - INFO - Results saved to out/evaluation_results.json
2024-06-12 14:33:34,366 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-12 14:35:06,331 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-12 14:35:41,555 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-12 14:36:37,120 - INFO - Evaluation results: {'eval_loss': 0.515364944934845, 'eval_accuracy': 0.75, 'eval_runtime': 0.1071, 'eval_samples_per_second': 8141.415, 'eval_steps_per_second': 261.422, 'epoch': 3.0}
2024-06-12 14:36:37,120 - INFO - Results saved to out/evaluation_results.json
2024-06-12 14:45:54,215 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-12 14:48:54,836 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-12 14:49:52,680 - INFO - Evaluation results: {'eval_loss': 0.6426346302032471, 'eval_accuracy': 0.6479357798165137, 'eval_runtime': 0.0988, 'eval_samples_per_second': 8825.575, 'eval_steps_per_second': 283.39, 'epoch': 3.0}
2024-06-12 14:49:52,680 - INFO - Results saved to out/evaluation_results.json
2024-06-12 19:50:13,745 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-12 22:10:30,482 - INFO - New Parameters Added by Adapters: set()
2024-06-12 22:10:41,537 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias'}
2024-06-12 22:16:10,015 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight'}
2024-06-12 22:16:10,016 - INFO - Total parameters: 4726656 || Trainable parameters: 4726656 (100.0%)
2024-06-12 22:16:17,507 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias'}
2024-06-12 22:16:17,508 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-12 22:23:10,145 - INFO - New Parameters Added by Adapters: {'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight'}
2024-06-12 22:23:10,146 - INFO - Total parameters: 4726656 || Trainable parameters: 4726656 (100.0%)
2024-06-12 22:39:55,400 - INFO - New Parameters Added by Adapters: {'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias'}
2024-06-12 22:39:55,400 - INFO - Total parameters: 4726656 || Trainable parameters: 4726656 (100.0%)
2024-06-12 22:41:32,826 - INFO - New Parameters Added by Adapters: {'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight'}
2024-06-12 22:41:32,827 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957%)
2024-06-12 22:43:14,035 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A'}
2024-06-12 22:43:14,035 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-12 23:46:12,952 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-12 23:48:07,661 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-13 00:04:54,070 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-13 00:05:18,663 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-13 00:06:49,534 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-13 00:14:40,976 - INFO - Evaluation results: {'eval_loss': 0.5340753793716431, 'eval_accuracy': 0.7384221123924584, 'eval_runtime': 0.6481, 'eval_samples_per_second': 8428.757, 'eval_steps_per_second': 263.833, 'epoch': 21.0}
2024-06-13 00:14:40,976 - INFO - Results saved to results/evaluation_results.json
2024-06-13 00:14:43,500 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-13 00:42:28,656 - INFO - Evaluation results: {'eval_loss': 0.5409291386604309, 'eval_accuracy': 0.6959683403413307, 'eval_f1': 0.6423625254582485, 'eval_runtime': 4.5861, 'eval_samples_per_second': 8815.733, 'eval_steps_per_second': 275.614, 'epoch': 21.0}
2024-06-13 00:42:28,657 - INFO - Results saved to results/evaluation_results.json
2024-06-13 00:42:31,138 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-13 00:47:19,866 - INFO - Evaluation results: {'eval_loss': 0.5578415393829346, 'eval_accuracy': 0.6995412844036697, 'eval_runtime': 0.1032, 'eval_samples_per_second': 8446.162, 'eval_steps_per_second': 271.207, 'epoch': 21.0}
2024-06-13 00:47:19,866 - INFO - Results saved to results/evaluation_results.json
2024-06-13 00:47:22,460 - INFO - Total parameters count: 4386178
2024-06-13 00:47:22,460 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-13 00:48:52,653 - INFO - Total parameters count: 4386178
2024-06-13 00:48:52,653 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-13 00:56:46,398 - INFO - Evaluation results: {'eval_loss': 0.7276807427406311, 'eval_accuracy': 0.7587406187076697, 'eval_runtime': 0.5896, 'eval_samples_per_second': 9266.15, 'eval_steps_per_second': 290.044, 'epoch': 21.0}
2024-06-13 00:56:46,398 - INFO - Results saved to results/evaluation_results.json
2024-06-13 00:56:48,916 - INFO - Total parameters count: 4386178
2024-06-13 00:56:48,916 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-13 01:25:00,442 - INFO - Evaluation results: {'eval_loss': 0.47980043292045593, 'eval_accuracy': 0.8283947563690329, 'eval_f1': 0.7786780655863212, 'eval_runtime': 4.4255, 'eval_samples_per_second': 9135.749, 'eval_steps_per_second': 285.619, 'epoch': 21.0}
2024-06-13 01:25:00,442 - INFO - Results saved to results/evaluation_results.json
2024-06-13 01:25:03,479 - INFO - Total parameters count: 4386178
2024-06-13 01:25:03,479 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-13 01:29:56,484 - INFO - Evaluation results: {'eval_loss': 1.0453295707702637, 'eval_accuracy': 0.7958715596330275, 'eval_runtime': 0.098, 'eval_samples_per_second': 8895.531, 'eval_steps_per_second': 285.636, 'epoch': 21.0}
2024-06-13 01:29:56,484 - INFO - Results saved to results/evaluation_results.json
2024-06-13 01:29:58,941 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight'}
2024-06-13 01:29:58,942 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-13 01:31:29,461 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight'}
2024-06-13 01:31:29,461 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-13 01:39:17,028 - INFO - Evaluation results: {'eval_loss': 0.48473358154296875, 'eval_accuracy': 0.7691744462749405, 'eval_runtime': 0.6301, 'eval_samples_per_second': 8670.187, 'eval_steps_per_second': 271.39, 'epoch': 21.0}
2024-06-13 01:39:17,028 - INFO - Results saved to results/evaluation_results.json
2024-06-13 01:39:19,545 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.weight'}
2024-06-13 01:39:19,546 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-13 02:07:04,992 - INFO - Evaluation results: {'eval_loss': 0.49895861744880676, 'eval_accuracy': 0.7312886470442741, 'eval_f1': 0.6762812872467223, 'eval_runtime': 4.5569, 'eval_samples_per_second': 8872.198, 'eval_steps_per_second': 277.38, 'epoch': 21.0}
2024-06-13 02:07:04,992 - INFO - Results saved to results/evaluation_results.json
2024-06-13 02:07:07,551 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight'}
2024-06-13 02:07:07,551 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-13 02:11:57,341 - INFO - Evaluation results: {'eval_loss': 0.47959598898887634, 'eval_accuracy': 0.7649082568807339, 'eval_runtime': 0.1052, 'eval_samples_per_second': 8285.721, 'eval_steps_per_second': 266.055, 'epoch': 21.0}
2024-06-13 02:11:57,341 - INFO - Results saved to results/evaluation_results.json
2024-06-13 02:11:59,899 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight'}
2024-06-13 02:11:59,900 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 02:13:30,842 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias'}
2024-06-13 02:13:30,842 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 02:14:06,500 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B'}
2024-06-13 02:14:06,500 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 02:16:10,986 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight'}
2024-06-13 02:16:10,986 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 02:17:51,835 - INFO - Total parameters: 4386307 || Trainable parameters: 387 (0.00882291184816749%)
2024-06-13 02:33:43,247 - INFO - Evaluation results: {'eval_loss': 1.087297797203064, 'eval_accuracy': 0.38532857870606213, 'eval_runtime': 11.5329, 'eval_samples_per_second': 851.042, 'eval_steps_per_second': 26.619, 'epoch': 21.0}
2024-06-13 02:33:43,247 - INFO - Results saved to results/evaluation_results.json
2024-06-13 02:34:18,668 - INFO - Total parameters: 4386178 || Trainable parameters: 258 (0.005882114223362572%)
2024-06-13 02:38:22,676 - INFO - Evaluation results: {'eval_loss': 0.6606483459472656, 'eval_accuracy': 0.5998535603148453, 'eval_runtime': 3.4032, 'eval_samples_per_second': 1605.234, 'eval_steps_per_second': 50.246, 'epoch': 21.0}
2024-06-13 02:38:22,676 - INFO - Results saved to results/evaluation_results.json
2024-06-13 02:40:27,266 - INFO - Total parameters: 4386178 || Trainable parameters: 258 (0.005882114223362572%)
2024-06-13 02:55:23,256 - INFO - Total parameters: 4386178 || Trainable parameters: 258 (0.005882114223362572%)
2024-06-13 02:57:56,837 - INFO - Evaluation results: {'eval_loss': 0.5713301301002502, 'eval_accuracy': 0.7006880733944955, 'eval_runtime': 0.2079, 'eval_samples_per_second': 4193.405, 'eval_steps_per_second': 134.651, 'epoch': 21.0}
2024-06-13 02:57:56,837 - INFO - Results saved to results/evaluation_results.json
2024-06-13 07:48:19,222 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias'}
2024-06-13 07:48:19,222 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 07:48:26,588 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 07:48:29,070 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight'}
2024-06-13 07:48:29,070 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 07:48:35,393 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 07:48:37,925 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight'}
2024-06-13 07:48:37,925 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 07:48:44,653 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 07:48:47,175 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias'}
2024-06-13 07:48:47,175 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 07:48:53,353 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 07:49:24,201 - INFO - New Parameters Added by Adapters: {'base_model.classifier.modules_to_save.default.bias', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.embeddings.LayerNorm.bias', 'prompt_encoder.default.embedding.weight', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.classifier.modules_to_save.default.weight', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.value.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight', 'word_embeddings.weight', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'base_model.bert.pooler.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.classifier.original_module.weight', 'base_model.classifier.original_module.bias', 'base_model.bert.embeddings.token_type_embeddings.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.self.query.bias', 'base_model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.bert.pooler.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.query.weight', 'base_model.bert.encoder.layer.0.attention.self.key.bias'}
2024-06-13 07:49:24,201 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-13 07:50:52,125 - ERROR - Something went wrong CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2024-06-13 07:50:54,586 - INFO - New Parameters Added by Adapters: {'base_model.classifier.modules_to_save.default.bias', 'word_embeddings.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.bert.embeddings.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.classifier.original_module.bias', 'base_model.classifier.original_module.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.self.query.weight', 'base_model.bert.pooler.dense.bias', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.value.weight', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.encoder.layer.0.attention.self.query.bias', 'prompt_encoder.default.embedding.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.bert.embeddings.token_type_embeddings.weight', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.classifier.modules_to_save.default.weight', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.bert.pooler.dense.weight', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight'}
2024-06-13 07:50:54,586 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-13 07:58:34,379 - INFO - Evaluation results: {'eval_loss': 0.658924400806427, 'eval_accuracy': 0.615046677649643, 'eval_runtime': 0.6484, 'eval_samples_per_second': 8425.515, 'eval_steps_per_second': 263.731, 'epoch': 21.0}
2024-06-13 07:58:34,379 - INFO - Results saved to results/evaluation_results.json
2024-06-13 07:58:36,862 - INFO - New Parameters Added by Adapters: {'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.classifier.modules_to_save.default.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'word_embeddings.weight', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.self.query.bias', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'prompt_encoder.default.embedding.weight', 'base_model.bert.pooler.dense.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.classifier.original_module.bias', 'base_model.bert.pooler.dense.bias', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.encoder.layer.0.attention.self.value.weight', 'base_model.bert.encoder.layer.0.attention.self.query.weight', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.bert.embeddings.LayerNorm.bias', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.classifier.modules_to_save.default.bias', 'base_model.bert.embeddings.token_type_embeddings.weight', 'base_model.classifier.original_module.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight'}
2024-06-13 07:58:36,863 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-13 08:18:38,737 - INFO - Total parameters count: 4386178
2024-06-13 08:18:38,737 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-13 08:19:20,112 - INFO - Total parameters count: 4386178
2024-06-13 08:19:20,112 - INFO - Trainable parameters count: 0 (0.0%)
2024-06-13 08:26:26,155 - INFO - Evaluation results: {'eval_loss': 0.6214619874954224, 'eval_accuracy': 0.6490724709374227, 'eval_f1': 0.419000819000819, 'eval_runtime': 4.7761, 'eval_samples_per_second': 8465.119, 'eval_steps_per_second': 264.653, 'epoch': 21.0}
2024-06-13 08:26:26,155 - INFO - Results saved to results/evaluation_results.json
2024-06-13 08:26:28,844 - INFO - New Parameters Added by Adapters: {'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.self.query.weight', 'base_model.classifier.modules_to_save.default.weight', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.bert.pooler.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'prompt_encoder.default.embedding.weight', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.encoder.layer.0.attention.self.key.bias', 'word_embeddings.weight', 'base_model.bert.pooler.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.embeddings.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.classifier.original_module.bias', 'base_model.classifier.modules_to_save.default.bias', 'base_model.bert.embeddings.token_type_embeddings.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.classifier.original_module.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.value.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.query.bias'}
2024-06-13 08:26:28,844 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-13 08:29:13,314 - INFO - Total parameters count: 4386178
2024-06-13 08:29:13,314 - INFO - Trainable parameters count: 0 (0.0%)
2024-06-13 08:30:42,225 - INFO - Total parameters count: 4386178
2024-06-13 08:30:42,225 - INFO - Trainable parameters count: 0 (0.0%)
2024-06-13 08:30:42,225 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.learned_embedding', 'bert.embeddings.word_embeddings.wte.weight'}
2024-06-13 08:31:21,657 - INFO - Evaluation results: {'eval_loss': 0.6327722072601318, 'eval_accuracy': 0.6490825688073395, 'eval_runtime': 0.1059, 'eval_samples_per_second': 8236.924, 'eval_steps_per_second': 264.488, 'epoch': 21.0}
2024-06-13 08:31:21,658 - INFO - Results saved to results/evaluation_results.json
2024-06-13 08:31:24,175 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight'}
2024-06-13 08:31:24,175 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 08:31:28,965 - INFO - Total parameters count: 4386178
2024-06-13 08:31:28,965 - INFO - Trainable parameters count: 0 (0.0%)
2024-06-13 08:31:28,966 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.learned_embedding', 'bert.embeddings.word_embeddings.wte.weight'}
2024-06-13 08:31:31,196 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 08:31:33,783 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight'}
2024-06-13 08:31:33,783 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 08:31:40,736 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 08:31:43,256 - INFO - New Parameters Added by Adapters: {'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B'}
2024-06-13 08:31:43,256 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 08:31:50,586 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 08:31:53,100 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight'}
2024-06-13 08:31:53,100 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 08:32:00,078 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 09:18:10,044 - INFO - Total parameters count: 4386178
2024-06-13 09:18:10,044 - INFO - Trainable parameters count: 0 (0.0%)
2024-06-13 09:18:10,044 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.learned_embedding', 'bert.embeddings.word_embeddings.wte.weight'}
2024-06-13 09:19:29,162 - INFO - Total parameters count: 4386178
2024-06-13 09:19:29,162 - INFO - Trainable parameters count: 0 (0.0%)
2024-06-13 09:19:29,163 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.learned_embedding', 'bert.embeddings.word_embeddings.wte.weight'}
2024-06-13 09:23:24,643 - INFO - Evaluation results: {'eval_loss': 0.6949307322502136, 'eval_accuracy': 0.5149082568807339, 'eval_runtime': 0.0995, 'eval_samples_per_second': 8767.858, 'eval_steps_per_second': 281.537, 'epoch': 21.0}
2024-06-13 09:42:40,625 - INFO - Total parameters count: 4386178
2024-06-13 09:42:40,625 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-13 09:42:40,625 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.learned_embedding', 'bert.embeddings.word_embeddings.wte.weight'}
2024-06-13 09:43:21,512 - INFO - Total parameters count: 4386178
2024-06-13 09:43:21,512 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-13 09:43:21,512 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.wte.weight', 'bert.embeddings.word_embeddings.learned_embedding'}
2024-06-13 09:44:37,773 - INFO - Total parameters count: 4386178
2024-06-13 09:44:37,773 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-13 09:44:37,773 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.wte.weight', 'bert.embeddings.word_embeddings.learned_embedding'}
2024-06-13 09:44:59,852 - INFO - Total parameters count: 4388738
2024-06-13 09:44:59,852 - INFO - Trainable parameters count: 2560 (0.05833111933316594%)
2024-06-13 09:44:59,852 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.learned_embedding', 'bert.embeddings.word_embeddings.wte.weight'}
2024-06-13 10:14:42,594 - INFO - Total parameters count: 4388738
2024-06-13 10:14:42,595 - INFO - Trainable parameters count: 2560 (0.05833111933316594%)
2024-06-13 10:14:42,595 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.wte.weight', 'bert.embeddings.word_embeddings.learned_embedding'}
2024-06-13 18:24:45,047 - INFO - Total parameters count: 4387458
2024-06-13 18:24:45,047 - INFO - Trainable parameters count: 4387458 (100.0%)
2024-06-13 18:26:58,623 - INFO - Total parameters count: 4387458
2024-06-13 18:26:58,623 - INFO - Trainable parameters count: 1280 (0.029174068446923024%)
2024-06-13 18:28:04,554 - INFO - Total parameters count: 4387458
2024-06-13 18:28:04,554 - INFO - Trainable parameters count: 1280 (0.029174068446923024%)
2024-06-13 18:29:35,519 - INFO - Total parameters count: 4387458
2024-06-13 18:29:35,520 - INFO - Trainable parameters count: 1280 (0.029174068446923024%)
2024-06-13 18:30:43,054 - INFO - Total parameters count: 4387458
2024-06-13 18:30:43,055 - INFO - Trainable parameters count: 1280 (0.029174068446923024%)
2024-06-13 19:00:16,132 - INFO - Total parameters count: 4387458
2024-06-13 19:00:16,133 - INFO - Trainable parameters count: 1280 (0.029174068446923024%)
2024-06-13 19:01:13,749 - INFO - Total parameters count: 4387458
2024-06-13 19:01:13,749 - INFO - Trainable parameters count: 1280 (0.029174068446923024%)
2024-06-13 19:02:43,192 - INFO - Total parameters count: 4387458
2024-06-13 19:02:43,192 - INFO - Trainable parameters count: 1280 (0.029174068446923024%)
2024-06-13 19:06:09,480 - INFO - Evaluation results: {'eval_loss': 0.6909008622169495, 'eval_accuracy': 0.5103211009174312, 'eval_runtime': 0.0799, 'eval_samples_per_second': 10914.583, 'eval_steps_per_second': 350.468, 'epoch': 21.0}
2024-06-13 19:06:09,481 - INFO - Results saved to out/evaluation_results.json
2024-06-13 19:13:28,713 - INFO - New Parameters Added by Adapters: {'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.classifier.bias', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.classifier.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight'}
2024-06-13 19:16:26,707 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.classifier.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.classifier.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.key.bias'}
2024-06-13 19:16:26,707 - INFO - Total parameters count: 4392578
2024-06-13 19:16:26,707 - INFO - Trainable parameters count: 6400 (0.14570031539565148%)
2024-06-13 19:17:50,468 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'soft_prompt.soft_prompts', 'bert.bert.embeddings.position_embeddings.weight', 'bert.classifier.weight', 'bert.classifier.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.pooler.dense.weight', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.weight'}
2024-06-13 19:17:50,468 - INFO - Total parameters count: 4392578
2024-06-13 19:17:50,468 - INFO - Trainable parameters count: 6400 (0.14570031539565148%)
2024-06-13 19:22:12,096 - INFO - New Parameters Added by Adapters: {'bert.classifier.bias', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.pooler.dense.bias', 'bert.classifier.weight', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight'}
2024-06-13 21:04:23,223 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias'}
2024-06-13 21:04:23,223 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 21:04:30,417 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 21:13:21,369 - INFO - New Parameters Added by Adapters: {'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B'}
2024-06-13 21:13:21,369 - INFO - Total parameters: 4743426 || Trainable parameters: 340736 (7.183331204070645 %)
2024-06-13 21:18:09,772 - INFO - Evaluation results: {'eval_runtime': 0.1108, 'eval_samples_per_second': 7871.643, 'eval_steps_per_second': 252.759, 'epoch': 21.0}
2024-06-13 21:18:09,772 - INFO - Results saved to results/evaluation_results.json
2024-06-14 07:43:58,927 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'heads.mrpc.1.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'heads.mrpc.4.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'heads.mrpc.1.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'heads.mrpc.4.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight'}
2024-06-14 07:43:58,927 - INFO - Total parameters: 4743426 || Trainable parameters: 357506 (7.536873137685715 %)
2024-06-14 07:44:58,089 - INFO - New Parameters Added by Adapters: {'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'heads.mrpc.1.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'heads.mrpc.1.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'heads.mrpc.4.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'heads.mrpc.4.weight'}
2024-06-14 07:44:58,089 - INFO - Total parameters: 4743426 || Trainable parameters: 357506 (7.536873137685715 %)
2024-06-14 07:45:24,765 - INFO - Evaluation results: {'eval_runtime': 0.1133, 'eval_samples_per_second': 7693.915, 'eval_steps_per_second': 247.052, 'epoch': 2.0}
2024-06-14 07:45:24,765 - INFO - Results saved to results/evaluation_results.json
2024-06-14 07:57:08,657 - INFO - New Parameters Added by Adapters: {'heads.mrpc.1.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'heads.mrpc.4.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'heads.mrpc.4.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'heads.mrpc.1.bias'}
2024-06-14 07:57:08,658 - INFO - Total parameters: 4743426 || Trainable parameters: 357506 (7.536873137685715 %)
2024-06-14 07:57:36,656 - INFO - Evaluation results: {'eval_runtime': 0.1136, 'eval_samples_per_second': 7675.862, 'eval_steps_per_second': 246.473, 'epoch': 2.0}
2024-06-14 07:57:36,656 - INFO - Results saved to results/evaluation_results.json
2024-06-14 08:10:45,964 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'heads.mrpc.1.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'heads.mrpc.1.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'heads.mrpc.4.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'heads.mrpc.4.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias'}
2024-06-14 08:10:45,965 - INFO - Total parameters: 4743426 || Trainable parameters: 357506 (7.536873137685715 %)
2024-06-14 08:11:59,423 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-14 08:11:59,606 - INFO - Total parameters: 4743426 || Trainable parameters: 357506 (7.536873137685715 %)
2024-06-14 08:12:25,958 - INFO - Evaluation results: {'eval_runtime': 0.1099, 'eval_samples_per_second': 7936.07, 'eval_steps_per_second': 254.828, 'epoch': 2.0}
2024-06-14 08:12:25,958 - INFO - Results saved to results/evaluation_results.json
2024-06-14 08:14:24,525 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-14 08:14:24,756 - INFO - Total parameters: 4743426 || Trainable parameters: 357506 (7.536873137685715 %)
2024-06-14 08:15:34,993 - INFO - Evaluation results: {'eval_runtime': 0.1525, 'eval_samples_per_second': 5719.753, 'eval_steps_per_second': 183.662, 'epoch': 2.0}
2024-06-14 08:15:34,999 - INFO - Results saved to results/evaluation_results.json
2024-06-14 08:15:49,666 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-14 08:15:49,845 - INFO - Total parameters: 4743426 || Trainable parameters: 357506 (7.536873137685715 %)
2024-06-14 08:16:18,252 - INFO - Evaluation results: {'eval_runtime': 0.1098, 'eval_samples_per_second': 7942.912, 'eval_steps_per_second': 255.048, 'epoch': 2.0}
2024-06-14 08:16:18,252 - INFO - Results saved to results/evaluation_results.json
2024-06-14 10:42:59,135 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-14 10:42:59,302 - ERROR - Something went wrong 'BertForSequenceClassification' object has no attribute 'add_classification_head'
2024-06-14 12:37:14,700 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 12:37:52,129 - INFO - Train results: TrainOutput(global_step=4210, training_loss=0.6696402109150649, metrics={'train_runtime': 25.653, 'train_samples_per_second': 5250.774, 'train_steps_per_second': 164.113, 'train_loss': 0.6696402109150649, 'epoch': 2.0})
2024-06-14 12:37:52,231 - INFO - Evaluation results: {'eval_loss': 0.6591554880142212, 'eval_accuracy': 0.6089449541284404, 'eval_runtime': 0.101, 'eval_samples_per_second': 8633.372, 'eval_steps_per_second': 277.218, 'epoch': 2.0}
2024-06-14 12:37:52,232 - ERROR - Something went wrong 'dict' object has no attribute 'metrics'
2024-06-14 12:38:58,163 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 12:40:02,637 - INFO - Train results: TrainOutput(global_step=4210, training_loss=0.6674497369915743, metrics={'train_runtime': 56.1942, 'train_samples_per_second': 2397.007, 'train_steps_per_second': 74.919, 'train_loss': 0.6674497369915743, 'epoch': 2.0})
2024-06-14 12:43:23,390 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 12:44:27,556 - INFO - Train results: TrainOutput(global_step=4210, training_loss=0.6615947322437712, metrics={'train_runtime': 55.8042, 'train_samples_per_second': 2413.762, 'train_steps_per_second': 75.442, 'train_loss': 0.6615947322437712, 'epoch': 2.0})
2024-06-14 12:44:27,703 - INFO - Evaluation results: {'eval_loss': 0.6497509479522705, 'eval_accuracy': 0.6410550458715596, 'eval_runtime': 0.1433, 'eval_samples_per_second': 6085.356, 'eval_steps_per_second': 195.401, 'epoch': 2.0}
2024-06-14 12:44:27,720 - ERROR - Something went wrong CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2024-06-14 12:45:03,811 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 12:45:36,303 - INFO - Train results: TrainOutput(global_step=4210, training_loss=0.6670617228165941, metrics={'train_runtime': 26.0814, 'train_samples_per_second': 5164.521, 'train_steps_per_second': 161.418, 'train_loss': 0.6670617228165941, 'epoch': 2.0})
2024-06-14 12:45:36,407 - INFO - Evaluation results: {'eval_loss': 0.6566453576087952, 'eval_accuracy': 0.6135321100917431, 'eval_runtime': 0.1024, 'eval_samples_per_second': 8519.468, 'eval_steps_per_second': 273.561, 'epoch': 2.0}
2024-06-14 12:45:36,418 - ERROR - Something went wrong CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2024-06-14 12:48:15,975 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 12:49:21,275 - INFO - Train results: TrainOutput(global_step=4210, training_loss=0.6616583896646024, metrics={'train_runtime': 56.8925, 'train_samples_per_second': 2367.587, 'train_steps_per_second': 73.999, 'train_loss': 0.6616583896646024, 'epoch': 2.0})
2024-06-14 12:49:21,432 - INFO - Evaluation results: {'eval_loss': 0.6505487561225891, 'eval_accuracy': 0.6376146788990825, 'eval_runtime': 0.1533, 'eval_samples_per_second': 5688.218, 'eval_steps_per_second': 182.649, 'epoch': 2.0}
2024-06-14 13:12:17,393 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 13:13:21,644 - INFO - Train results: {'train_runtime': 55.8174, 'train_samples_per_second': 2413.19, 'train_steps_per_second': 75.424, 'train_loss': 0.6681003965844451, 'epoch': 2.0}
2024-06-14 13:13:21,900 - INFO - Evaluation results: {'eval_loss': 0.6605035662651062, 'eval_accuracy': 0.6032110091743119, 'eval_runtime': 0.1449, 'eval_samples_per_second': 6019.327, 'eval_steps_per_second': 193.281, 'epoch': 2.0}
2024-06-14 13:13:21,915 - ERROR - Something went wrong CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2024-06-14 13:14:56,130 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 13:15:37,135 - INFO - Train results: {'train_runtime': 34.6254, 'train_samples_per_second': 3890.146, 'train_steps_per_second': 121.587, 'train_loss': 0.6668938443100084, 'epoch': 2.0}
2024-06-14 13:15:37,309 - INFO - Evaluation results: {'eval_loss': 0.6569451689720154, 'eval_accuracy': 0.6227064220183486, 'eval_runtime': 0.1691, 'eval_samples_per_second': 5156.129, 'eval_steps_per_second': 165.564, 'epoch': 2.0}
2024-06-14 13:15:37,324 - ERROR - Something went wrong CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2024-06-14 13:16:34,839 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 13:17:14,890 - INFO - Train results: {'train_runtime': 33.5504, 'train_samples_per_second': 4014.801, 'train_steps_per_second': 125.483, 'train_loss': 0.6659569287243479, 'epoch': 2.0}
2024-06-14 13:17:15,023 - INFO - Evaluation results: {'eval_loss': 0.658628523349762, 'eval_accuracy': 0.6077981651376146, 'eval_runtime': 0.1285, 'eval_samples_per_second': 6786.9, 'eval_steps_per_second': 217.928, 'epoch': 2.0}
2024-06-14 13:17:15,038 - ERROR - Something went wrong CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2024-06-14 13:25:15,836 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 18:52:59,433 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 19:29:52,978 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-14 19:29:53,166 - ERROR - Something went wrong 'BertForSequenceClassification' object has no attribute 'add_classification_head'
2024-06-14 19:30:42,141 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-14 19:30:51,180 - INFO - Total parameters: 4726914 || Trainable parameters: 340736 (7.208423931554499 %)
2024-06-14 19:30:51,320 - ERROR - Something went wrong The pre-trained model weights are not frozen. For training adapters, please call the train_adapter() method
2024-06-14 21:09:02,009 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-14 21:09:02,179 - INFO - Total parameters: 4726914 || Trainable parameters: 332544 (7.035118472644097 %)
2024-06-14 21:09:28,574 - INFO - Train results: {'train_runtime': 26.2945, 'train_samples_per_second': 5122.666, 'train_steps_per_second': 160.109, 'train_loss': 0.6330607516182291, 'epoch': 2.0}
2024-06-14 21:09:28,701 - INFO - Evaluation results: {'eval_loss': 0.5988166332244873, 'eval_accuracy': 0.694954128440367, 'eval_runtime': 0.1229, 'eval_samples_per_second': 7094.483, 'eval_steps_per_second': 227.805, 'epoch': 2.0}
2024-06-14 21:09:28,702 - INFO - Results saved to results/evaluation_results.json
2024-06-14 21:32:38,339 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.classifier.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.pooler.dense.bias', 'bert.bert.embeddings.position_embeddings.weight', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.classifier.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight'}
2024-06-14 21:32:38,339 - INFO - Total parameters count: 4392578
2024-06-14 21:32:38,339 - INFO - Trainable parameters count: 6400 (0.14570031539565148%)
2024-06-14 21:32:38,514 - ERROR - Something went wrong CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
2024-06-14 21:33:30,117 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.classifier.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.pooler.dense.bias', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.classifier.bias', 'bert.bert.encoder.layer.1.output.dense.weight'}
2024-06-14 21:33:30,119 - INFO - Total parameters count: 4392578
2024-06-14 21:34:32,753 - INFO - Trainable parameters count: 6400 (0.14570031539565148%)
2024-06-14 21:34:58,819 - ERROR - Something went wrong CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2024-06-14 21:35:31,514 - INFO - New Parameters Added by Adapters: {'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'soft_prompt.soft_prompts', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.classifier.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.classifier.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.pooler.dense.weight'}
2024-06-14 21:35:31,517 - INFO - Total parameters count: 4392578
2024-06-14 21:35:31,517 - INFO - Trainable parameters count: 6400 (0.14570031539565148%)
2024-06-14 21:38:22,089 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.0.output.dense.bias', 'soft_prompt.soft_prompts', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.classifier.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.classifier.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias'}
2024-06-14 21:38:22,091 - INFO - Total parameters count: 4392578
2024-06-14 21:38:22,091 - INFO - Trainable parameters count: 6400 (0.14570031539565148%)
2024-06-14 21:38:22,092 - INFO - Unique labels: {0, 1, 2}
2024-06-14 21:42:46,360 - ERROR - Something went wrong CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
2024-06-14 21:46:16,237 - INFO - New Parameters Added by Adapters: {'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.classifier.bias', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.classifier.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'soft_prompt.soft_prompts'}
2024-06-14 21:46:16,237 - INFO - Total parameters count: 4392707
2024-06-14 21:46:16,237 - INFO - Trainable parameters count: 6400 (0.1456960366352684%)
2024-06-14 21:46:16,241 - INFO - Unique labels: {0, 1, 2}
2024-06-14 21:46:19,608 - INFO - Train results: {'train_runtime': 3.2602, 'train_samples_per_second': 6134.667, 'train_steps_per_second': 192.015, 'train_loss': 1.1163551144706556, 'epoch': 2.0}
2024-06-14 21:46:19,706 - INFO - Evaluation results: {'eval_loss': 1.1069685220718384, 'eval_accuracy': 0.324, 'eval_runtime': 0.0959, 'eval_samples_per_second': 10427.263, 'eval_steps_per_second': 333.672, 'epoch': 2.0}
2024-06-14 21:46:19,706 - INFO - Results saved to results/evaluation_results.json
2024-06-14 22:34:50,874 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.classifier.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.classifier.weight', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.0.attention.self.key.bias'}
2024-06-14 22:34:50,875 - ERROR - Something went wrong Please specify `target_modules` in `peft_config`
2024-06-14 22:35:42,771 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.classifier.bias', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.classifier.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.1.output.dense.bias'}
2024-06-14 22:35:42,772 - ERROR - Something went wrong Please specify `target_modules` in `peft_config`
2024-06-14 22:38:22,122 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.classifier.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.classifier.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.1.attention.self.value.weight'}
2024-06-14 22:46:24,354 - INFO - New Parameters Added by Adapters: {'bert.base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'bert.base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'bert.base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'soft_prompt.soft_prompts', 'bert.base_model.model.classifier.original_module.bias', 'bert.base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'bert.base_model.model.bert.embeddings.LayerNorm.weight', 'bert.base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'bert.base_model.model.bert.embeddings.word_embeddings.weight', 'bert.base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'bert.base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'bert.base_model.model.bert.encoder.layer.0.output.dense.weight', 'bert.base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'bert.base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'bert.base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'bert.base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.base_model.model.bert.embeddings.token_type_embeddings.weight', 'bert.base_model.model.bert.embeddings.LayerNorm.bias', 'bert.base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'bert.base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'bert.base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'bert.base_model.model.classifier.modules_to_save.default.weight', 'bert.base_model.model.bert.embeddings.position_embeddings.weight', 'bert.base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.base_model.model.bert.pooler.dense.weight', 'bert.base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'bert.base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'bert.base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'bert.base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'bert.base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'bert.base_model.model.classifier.modules_to_save.default.bias', 'bert.base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'bert.base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'bert.base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'bert.base_model.model.bert.encoder.layer.0.output.dense.bias', 'bert.base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'bert.base_model.model.bert.encoder.layer.1.output.dense.weight', 'bert.base_model.model.classifier.original_module.weight', 'bert.base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.base_model.model.bert.encoder.layer.1.output.dense.bias', 'bert.base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'bert.base_model.model.bert.pooler.dense.bias', 'bert.base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'bert.base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'bert.base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'bert.base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'bert.base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'bert.base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight'}
2024-06-14 22:46:24,354 - INFO - Total parameters count: 4401028
2024-06-14 22:46:24,354 - INFO - Trainable parameters count: 14850 (0.33742116614572776%)
2024-06-14 22:46:24,356 - INFO - Unique labels: {0, 1}
2024-06-14 22:46:28,350 - INFO - Train results: {'train_runtime': 3.8803, 'train_samples_per_second': 5154.223, 'train_steps_per_second': 161.327, 'train_loss': 0.6984596709473826, 'epoch': 2.0}
2024-06-14 22:46:28,451 - INFO - Evaluation results: {'eval_loss': 0.6930413842201233, 'eval_accuracy': 0.493, 'eval_runtime': 0.0989, 'eval_samples_per_second': 10109.972, 'eval_steps_per_second': 323.519, 'epoch': 2.0}
2024-06-14 22:46:28,451 - INFO - Results saved to results/evaluation_results.json
2024-06-14 22:50:38,018 - ERROR - Something went wrong 'BertForSequenceClassificationWithSoftPromptPeft' object has no attribute 'bertstate_dict'
2024-06-14 22:51:07,251 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight'}
2024-06-14 22:51:07,252 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight'}
2024-06-14 22:51:07,252 - ERROR - Something went wrong name 'base_model_parameters' is not defined
2024-06-14 22:52:40,753 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight'}
2024-06-14 22:52:40,753 - INFO - New Parameters Added by Adapters: set()
2024-06-14 22:52:40,753 - INFO - Total parameters count: 4401028
2024-06-14 22:52:40,753 - INFO - Trainable parameters count: 14850 (0.33742116614572776%)
2024-06-14 22:52:40,755 - INFO - Unique labels: {0, 1}
2024-06-14 22:52:44,562 - INFO - Train results: {'train_runtime': 3.7036, 'train_samples_per_second': 5400.196, 'train_steps_per_second': 169.026, 'train_loss': 0.6954565124389843, 'epoch': 2.0}
2024-06-14 22:52:44,660 - INFO - Evaluation results: {'eval_loss': 0.6938176155090332, 'eval_accuracy': 0.482, 'eval_runtime': 0.0967, 'eval_samples_per_second': 10337.087, 'eval_steps_per_second': 330.787, 'epoch': 2.0}
2024-06-14 22:52:44,661 - INFO - Results saved to results/evaluation_results.json
2024-06-14 22:55:25,277 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias'}
2024-06-14 22:55:25,277 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-14 22:55:29,509 - INFO - Train results: {'train_runtime': 4.1281, 'train_samples_per_second': 4844.819, 'train_steps_per_second': 151.643, 'train_loss': 0.6923354296638562, 'epoch': 2.0}
2024-06-14 22:55:29,637 - INFO - Evaluation results: {'eval_loss': 0.6673650741577148, 'eval_accuracy': 0.635, 'eval_runtime': 0.1269, 'eval_samples_per_second': 7881.497, 'eval_steps_per_second': 252.208, 'epoch': 2.0}
2024-06-14 22:55:29,638 - INFO - Results saved to results/evaluation_results.json
