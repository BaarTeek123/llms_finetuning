2024-06-11 23:43:00,986 - INFO - Total parameters count: 4386178
2024-06-11 23:43:00,986 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-11 23:45:05,574 - INFO - Evaluation results: {'eval_loss': 0.6381635665893555, 'eval_accuracy': 0.8291284403669725, 'eval_runtime': 0.1986, 'eval_samples_per_second': 4390.66, 'eval_steps_per_second': 548.833, 'epoch': 3.0}
2024-06-11 23:52:00,719 - INFO - Total parameters count: 4386178
2024-06-11 23:52:00,719 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-11 23:53:53,752 - INFO - Evaluation results: {'eval_loss': 0.6673561334609985, 'eval_accuracy': 0.8268348623853211, 'eval_runtime': 0.1709, 'eval_samples_per_second': 5102.097, 'eval_steps_per_second': 637.762, 'epoch': 3.0}
2024-06-11 23:53:53,752 - INFO - Results saved to out/evaluation_results.json
2024-06-12 00:06:44,242 - INFO - Total parameters count: 4386178
2024-06-12 00:06:44,242 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-12 00:12:24,964 - INFO - Total parameters count: 4386178
2024-06-12 00:12:24,964 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-12 00:24:39,859 - INFO - Evaluation results: {'eval_loss': 0.4172626733779907, 'eval_accuracy': 0.8086322038090527, 'eval_f1': 0.7574380035740038, 'eval_runtime': 7.7254, 'eval_samples_per_second': 5233.368, 'eval_steps_per_second': 654.203, 'epoch': 3.0}
2024-06-12 00:24:39,859 - INFO - Results saved to out/evaluation_results.json
2024-06-12 00:30:01,625 - INFO - Total parameters count: 4386178
2024-06-12 00:30:01,625 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-12 00:30:22,018 - INFO - Total parameters count: 4386178
2024-06-12 00:30:22,018 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-12 09:27:30,962 - INFO - Evaluation results: {'eval_loss': 0.6683989763259888, 'eval_accuracy': 0.5969247666117518, 'eval_runtime': 3.4429, 'eval_samples_per_second': 1586.737, 'eval_steps_per_second': 49.667, 'epoch': 3.0}
2024-06-12 09:27:30,962 - INFO - Results saved to out/evaluation_results.json
2024-06-12 10:01:40,997 - INFO - Total parameters count: 4386307
2024-06-12 10:01:40,997 - INFO - Trainable parameters count: 387 (0.00882291184816749%)
2024-06-12 10:04:30,610 - INFO - Evaluation results: {'eval_loss': 1.0894393920898438, 'eval_accuracy': 0.3804381049414162, 'eval_runtime': 11.5101, 'eval_samples_per_second': 852.726, 'eval_steps_per_second': 26.672, 'epoch': 3.0}
2024-06-12 10:04:30,610 - INFO - Results saved to out/evaluation_results.json
2024-06-12 10:21:05,477 - INFO - Total parameters count: 4388227
2024-06-12 10:21:05,477 - INFO - Trainable parameters count: 2307 (0.05257248542520703%)
2024-06-12 11:04:51,467 - INFO - Total parameters count: 4386178
2024-06-12 11:04:51,467 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-12 13:02:09,081 - INFO - Total parameters count: 4390276
2024-06-12 13:02:09,081 - INFO - Trainable parameters count: 4098 (0.09334265089484124%)
2024-06-12 13:03:46,265 - INFO - Total parameters count: 4390276
2024-06-12 13:03:46,265 - INFO - Trainable parameters count: 4098 (0.09334265089484124%)
2024-06-12 13:05:12,235 - INFO - Total parameters count: 4390276
2024-06-12 13:05:12,235 - INFO - Trainable parameters count: 4098 (0.09334265089484124%)
2024-06-12 13:22:06,403 - INFO - Total parameters count: 4390276
2024-06-12 13:22:06,403 - INFO - Trainable parameters count: 4098 (0.09334265089484124%)
2024-06-12 13:22:27,919 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-12 13:23:27,810 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-12 13:29:03,320 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-12 13:30:09,595 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-12 13:32:21,187 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-12 13:34:05,831 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-12 13:36:13,552 - INFO - Evaluation results: {'eval_loss': 0.678795337677002, 'eval_accuracy': 0.5793520043931906, 'eval_runtime': 0.7982, 'eval_samples_per_second': 6844.261, 'eval_steps_per_second': 214.236, 'epoch': 3.0}
2024-06-12 13:36:13,554 - INFO - Results saved to out/evaluation_results.json
2024-06-12 13:46:56,261 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-12 13:48:10,200 - INFO - Evaluation results: {'eval_loss': 0.6836631298065186, 'eval_accuracy': 0.5744096650192202, 'eval_runtime': 0.6337, 'eval_samples_per_second': 8621.377, 'eval_steps_per_second': 269.862, 'epoch': 3.0}
2024-06-12 13:48:10,200 - INFO - Results saved to out/evaluation_results.json
2024-06-12 14:33:34,366 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-12 14:35:06,331 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-12 14:35:41,555 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-12 14:36:37,120 - INFO - Evaluation results: {'eval_loss': 0.515364944934845, 'eval_accuracy': 0.75, 'eval_runtime': 0.1071, 'eval_samples_per_second': 8141.415, 'eval_steps_per_second': 261.422, 'epoch': 3.0}
2024-06-12 14:36:37,120 - INFO - Results saved to out/evaluation_results.json
2024-06-12 14:45:54,215 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-12 14:48:54,836 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-12 14:49:52,680 - INFO - Evaluation results: {'eval_loss': 0.6426346302032471, 'eval_accuracy': 0.6479357798165137, 'eval_runtime': 0.0988, 'eval_samples_per_second': 8825.575, 'eval_steps_per_second': 283.39, 'epoch': 3.0}
2024-06-12 14:49:52,680 - INFO - Results saved to out/evaluation_results.json
2024-06-12 19:50:13,745 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-12 22:10:30,482 - INFO - New Parameters Added by Adapters: set()
2024-06-12 22:10:41,537 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias'}
2024-06-12 22:16:10,015 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight'}
2024-06-12 22:16:10,016 - INFO - Total parameters: 4726656 || Trainable parameters: 4726656 (100.0%)
2024-06-12 22:16:17,507 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias'}
2024-06-12 22:16:17,508 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-12 22:23:10,145 - INFO - New Parameters Added by Adapters: {'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight'}
2024-06-12 22:23:10,146 - INFO - Total parameters: 4726656 || Trainable parameters: 4726656 (100.0%)
2024-06-12 22:39:55,400 - INFO - New Parameters Added by Adapters: {'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias'}
2024-06-12 22:39:55,400 - INFO - Total parameters: 4726656 || Trainable parameters: 4726656 (100.0%)
2024-06-12 22:41:32,826 - INFO - New Parameters Added by Adapters: {'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight'}
2024-06-12 22:41:32,827 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957%)
2024-06-12 22:43:14,035 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A'}
2024-06-12 22:43:14,035 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-12 23:46:12,952 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-12 23:48:07,661 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-13 00:04:54,070 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-13 00:05:18,663 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-13 00:06:49,534 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-13 00:14:40,976 - INFO - Evaluation results: {'eval_loss': 0.5340753793716431, 'eval_accuracy': 0.7384221123924584, 'eval_runtime': 0.6481, 'eval_samples_per_second': 8428.757, 'eval_steps_per_second': 263.833, 'epoch': 21.0}
2024-06-13 00:14:40,976 - INFO - Results saved to results/evaluation_results.json
2024-06-13 00:14:43,500 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-13 00:42:28,656 - INFO - Evaluation results: {'eval_loss': 0.5409291386604309, 'eval_accuracy': 0.6959683403413307, 'eval_f1': 0.6423625254582485, 'eval_runtime': 4.5861, 'eval_samples_per_second': 8815.733, 'eval_steps_per_second': 275.614, 'epoch': 21.0}
2024-06-13 00:42:28,657 - INFO - Results saved to results/evaluation_results.json
2024-06-13 00:42:31,138 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-13 00:47:19,866 - INFO - Evaluation results: {'eval_loss': 0.5578415393829346, 'eval_accuracy': 0.6995412844036697, 'eval_runtime': 0.1032, 'eval_samples_per_second': 8446.162, 'eval_steps_per_second': 271.207, 'epoch': 21.0}
2024-06-13 00:47:19,866 - INFO - Results saved to results/evaluation_results.json
2024-06-13 00:47:22,460 - INFO - Total parameters count: 4386178
2024-06-13 00:47:22,460 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-13 00:48:52,653 - INFO - Total parameters count: 4386178
2024-06-13 00:48:52,653 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-13 00:56:46,398 - INFO - Evaluation results: {'eval_loss': 0.7276807427406311, 'eval_accuracy': 0.7587406187076697, 'eval_runtime': 0.5896, 'eval_samples_per_second': 9266.15, 'eval_steps_per_second': 290.044, 'epoch': 21.0}
2024-06-13 00:56:46,398 - INFO - Results saved to results/evaluation_results.json
2024-06-13 00:56:48,916 - INFO - Total parameters count: 4386178
2024-06-13 00:56:48,916 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-13 01:25:00,442 - INFO - Evaluation results: {'eval_loss': 0.47980043292045593, 'eval_accuracy': 0.8283947563690329, 'eval_f1': 0.7786780655863212, 'eval_runtime': 4.4255, 'eval_samples_per_second': 9135.749, 'eval_steps_per_second': 285.619, 'epoch': 21.0}
2024-06-13 01:25:00,442 - INFO - Results saved to results/evaluation_results.json
2024-06-13 01:25:03,479 - INFO - Total parameters count: 4386178
2024-06-13 01:25:03,479 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-13 01:29:56,484 - INFO - Evaluation results: {'eval_loss': 1.0453295707702637, 'eval_accuracy': 0.7958715596330275, 'eval_runtime': 0.098, 'eval_samples_per_second': 8895.531, 'eval_steps_per_second': 285.636, 'epoch': 21.0}
2024-06-13 01:29:56,484 - INFO - Results saved to results/evaluation_results.json
2024-06-13 01:29:58,941 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight'}
2024-06-13 01:29:58,942 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-13 01:31:29,461 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight'}
2024-06-13 01:31:29,461 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-13 01:39:17,028 - INFO - Evaluation results: {'eval_loss': 0.48473358154296875, 'eval_accuracy': 0.7691744462749405, 'eval_runtime': 0.6301, 'eval_samples_per_second': 8670.187, 'eval_steps_per_second': 271.39, 'epoch': 21.0}
2024-06-13 01:39:17,028 - INFO - Results saved to results/evaluation_results.json
2024-06-13 01:39:19,545 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.weight'}
2024-06-13 01:39:19,546 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-13 02:07:04,992 - INFO - Evaluation results: {'eval_loss': 0.49895861744880676, 'eval_accuracy': 0.7312886470442741, 'eval_f1': 0.6762812872467223, 'eval_runtime': 4.5569, 'eval_samples_per_second': 8872.198, 'eval_steps_per_second': 277.38, 'epoch': 21.0}
2024-06-13 02:07:04,992 - INFO - Results saved to results/evaluation_results.json
2024-06-13 02:07:07,551 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight'}
2024-06-13 02:07:07,551 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-13 02:11:57,341 - INFO - Evaluation results: {'eval_loss': 0.47959598898887634, 'eval_accuracy': 0.7649082568807339, 'eval_runtime': 0.1052, 'eval_samples_per_second': 8285.721, 'eval_steps_per_second': 266.055, 'epoch': 21.0}
2024-06-13 02:11:57,341 - INFO - Results saved to results/evaluation_results.json
2024-06-13 02:11:59,899 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight'}
2024-06-13 02:11:59,900 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 02:13:30,842 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias'}
2024-06-13 02:13:30,842 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 02:14:06,500 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B'}
2024-06-13 02:14:06,500 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 02:16:10,986 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight'}
2024-06-13 02:16:10,986 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 02:17:51,835 - INFO - Total parameters: 4386307 || Trainable parameters: 387 (0.00882291184816749%)
2024-06-13 02:33:43,247 - INFO - Evaluation results: {'eval_loss': 1.087297797203064, 'eval_accuracy': 0.38532857870606213, 'eval_runtime': 11.5329, 'eval_samples_per_second': 851.042, 'eval_steps_per_second': 26.619, 'epoch': 21.0}
2024-06-13 02:33:43,247 - INFO - Results saved to results/evaluation_results.json
2024-06-13 02:34:18,668 - INFO - Total parameters: 4386178 || Trainable parameters: 258 (0.005882114223362572%)
2024-06-13 02:38:22,676 - INFO - Evaluation results: {'eval_loss': 0.6606483459472656, 'eval_accuracy': 0.5998535603148453, 'eval_runtime': 3.4032, 'eval_samples_per_second': 1605.234, 'eval_steps_per_second': 50.246, 'epoch': 21.0}
2024-06-13 02:38:22,676 - INFO - Results saved to results/evaluation_results.json
2024-06-13 02:40:27,266 - INFO - Total parameters: 4386178 || Trainable parameters: 258 (0.005882114223362572%)
2024-06-13 02:55:23,256 - INFO - Total parameters: 4386178 || Trainable parameters: 258 (0.005882114223362572%)
2024-06-13 02:57:56,837 - INFO - Evaluation results: {'eval_loss': 0.5713301301002502, 'eval_accuracy': 0.7006880733944955, 'eval_runtime': 0.2079, 'eval_samples_per_second': 4193.405, 'eval_steps_per_second': 134.651, 'epoch': 21.0}
2024-06-13 02:57:56,837 - INFO - Results saved to results/evaluation_results.json
2024-06-13 07:48:19,222 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias'}
2024-06-13 07:48:19,222 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 07:48:26,588 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 07:48:29,070 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight'}
2024-06-13 07:48:29,070 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 07:48:35,393 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 07:48:37,925 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight'}
2024-06-13 07:48:37,925 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 07:48:44,653 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 07:48:47,175 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias'}
2024-06-13 07:48:47,175 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 07:48:53,353 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 07:49:24,201 - INFO - New Parameters Added by Adapters: {'base_model.classifier.modules_to_save.default.bias', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.embeddings.LayerNorm.bias', 'prompt_encoder.default.embedding.weight', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.classifier.modules_to_save.default.weight', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.value.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight', 'word_embeddings.weight', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'base_model.bert.pooler.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.classifier.original_module.weight', 'base_model.classifier.original_module.bias', 'base_model.bert.embeddings.token_type_embeddings.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.self.query.bias', 'base_model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.bert.pooler.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.query.weight', 'base_model.bert.encoder.layer.0.attention.self.key.bias'}
2024-06-13 07:49:24,201 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-13 07:50:52,125 - ERROR - Something went wrong CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2024-06-13 07:50:54,586 - INFO - New Parameters Added by Adapters: {'base_model.classifier.modules_to_save.default.bias', 'word_embeddings.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.bert.embeddings.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.classifier.original_module.bias', 'base_model.classifier.original_module.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.self.query.weight', 'base_model.bert.pooler.dense.bias', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.value.weight', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.encoder.layer.0.attention.self.query.bias', 'prompt_encoder.default.embedding.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.bert.embeddings.token_type_embeddings.weight', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.classifier.modules_to_save.default.weight', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.bert.pooler.dense.weight', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight'}
2024-06-13 07:50:54,586 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-13 07:58:34,379 - INFO - Evaluation results: {'eval_loss': 0.658924400806427, 'eval_accuracy': 0.615046677649643, 'eval_runtime': 0.6484, 'eval_samples_per_second': 8425.515, 'eval_steps_per_second': 263.731, 'epoch': 21.0}
2024-06-13 07:58:34,379 - INFO - Results saved to results/evaluation_results.json
2024-06-13 07:58:36,862 - INFO - New Parameters Added by Adapters: {'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.classifier.modules_to_save.default.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'word_embeddings.weight', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.self.query.bias', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'prompt_encoder.default.embedding.weight', 'base_model.bert.pooler.dense.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.classifier.original_module.bias', 'base_model.bert.pooler.dense.bias', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.encoder.layer.0.attention.self.value.weight', 'base_model.bert.encoder.layer.0.attention.self.query.weight', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.bert.embeddings.LayerNorm.bias', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.classifier.modules_to_save.default.bias', 'base_model.bert.embeddings.token_type_embeddings.weight', 'base_model.classifier.original_module.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight'}
2024-06-13 07:58:36,863 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-13 08:18:38,737 - INFO - Total parameters count: 4386178
2024-06-13 08:18:38,737 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-13 08:19:20,112 - INFO - Total parameters count: 4386178
2024-06-13 08:19:20,112 - INFO - Trainable parameters count: 0 (0.0%)
2024-06-13 08:26:26,155 - INFO - Evaluation results: {'eval_loss': 0.6214619874954224, 'eval_accuracy': 0.6490724709374227, 'eval_f1': 0.419000819000819, 'eval_runtime': 4.7761, 'eval_samples_per_second': 8465.119, 'eval_steps_per_second': 264.653, 'epoch': 21.0}
2024-06-13 08:26:26,155 - INFO - Results saved to results/evaluation_results.json
2024-06-13 08:26:28,844 - INFO - New Parameters Added by Adapters: {'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.self.query.weight', 'base_model.classifier.modules_to_save.default.weight', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.bert.pooler.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'prompt_encoder.default.embedding.weight', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.encoder.layer.0.attention.self.key.bias', 'word_embeddings.weight', 'base_model.bert.pooler.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.embeddings.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.classifier.original_module.bias', 'base_model.classifier.modules_to_save.default.bias', 'base_model.bert.embeddings.token_type_embeddings.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.classifier.original_module.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.value.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.query.bias'}
2024-06-13 08:26:28,844 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-13 08:29:13,314 - INFO - Total parameters count: 4386178
2024-06-13 08:29:13,314 - INFO - Trainable parameters count: 0 (0.0%)
2024-06-13 08:30:42,225 - INFO - Total parameters count: 4386178
2024-06-13 08:30:42,225 - INFO - Trainable parameters count: 0 (0.0%)
2024-06-13 08:30:42,225 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.learned_embedding', 'bert.embeddings.word_embeddings.wte.weight'}
2024-06-13 08:31:21,657 - INFO - Evaluation results: {'eval_loss': 0.6327722072601318, 'eval_accuracy': 0.6490825688073395, 'eval_runtime': 0.1059, 'eval_samples_per_second': 8236.924, 'eval_steps_per_second': 264.488, 'epoch': 21.0}
2024-06-13 08:31:21,658 - INFO - Results saved to results/evaluation_results.json
2024-06-13 08:31:24,175 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight'}
2024-06-13 08:31:24,175 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 08:31:28,965 - INFO - Total parameters count: 4386178
2024-06-13 08:31:28,965 - INFO - Trainable parameters count: 0 (0.0%)
2024-06-13 08:31:28,966 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.learned_embedding', 'bert.embeddings.word_embeddings.wte.weight'}
2024-06-13 08:31:31,196 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 08:31:33,783 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight'}
2024-06-13 08:31:33,783 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 08:31:40,736 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 08:31:43,256 - INFO - New Parameters Added by Adapters: {'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B'}
2024-06-13 08:31:43,256 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 08:31:50,586 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 08:31:53,100 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight'}
2024-06-13 08:31:53,100 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 08:32:00,078 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 09:18:10,044 - INFO - Total parameters count: 4386178
2024-06-13 09:18:10,044 - INFO - Trainable parameters count: 0 (0.0%)
2024-06-13 09:18:10,044 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.learned_embedding', 'bert.embeddings.word_embeddings.wte.weight'}
2024-06-13 09:19:29,162 - INFO - Total parameters count: 4386178
2024-06-13 09:19:29,162 - INFO - Trainable parameters count: 0 (0.0%)
2024-06-13 09:19:29,163 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.learned_embedding', 'bert.embeddings.word_embeddings.wte.weight'}
2024-06-13 09:23:24,643 - INFO - Evaluation results: {'eval_loss': 0.6949307322502136, 'eval_accuracy': 0.5149082568807339, 'eval_runtime': 0.0995, 'eval_samples_per_second': 8767.858, 'eval_steps_per_second': 281.537, 'epoch': 21.0}
2024-06-13 09:42:40,625 - INFO - Total parameters count: 4386178
2024-06-13 09:42:40,625 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-13 09:42:40,625 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.learned_embedding', 'bert.embeddings.word_embeddings.wte.weight'}
2024-06-13 09:43:21,512 - INFO - Total parameters count: 4386178
2024-06-13 09:43:21,512 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-13 09:43:21,512 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.wte.weight', 'bert.embeddings.word_embeddings.learned_embedding'}
2024-06-13 09:44:37,773 - INFO - Total parameters count: 4386178
2024-06-13 09:44:37,773 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-13 09:44:37,773 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.wte.weight', 'bert.embeddings.word_embeddings.learned_embedding'}
2024-06-13 09:44:59,852 - INFO - Total parameters count: 4388738
2024-06-13 09:44:59,852 - INFO - Trainable parameters count: 2560 (0.05833111933316594%)
2024-06-13 09:44:59,852 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.learned_embedding', 'bert.embeddings.word_embeddings.wte.weight'}
2024-06-13 10:14:42,594 - INFO - Total parameters count: 4388738
2024-06-13 10:14:42,595 - INFO - Trainable parameters count: 2560 (0.05833111933316594%)
2024-06-13 10:14:42,595 - INFO - New Parameters Added by Adapters: {'bert.embeddings.word_embeddings.wte.weight', 'bert.embeddings.word_embeddings.learned_embedding'}
2024-06-13 18:24:45,047 - INFO - Total parameters count: 4387458
2024-06-13 18:24:45,047 - INFO - Trainable parameters count: 4387458 (100.0%)
2024-06-13 18:26:58,623 - INFO - Total parameters count: 4387458
2024-06-13 18:26:58,623 - INFO - Trainable parameters count: 1280 (0.029174068446923024%)
2024-06-13 18:28:04,554 - INFO - Total parameters count: 4387458
2024-06-13 18:28:04,554 - INFO - Trainable parameters count: 1280 (0.029174068446923024%)
2024-06-13 18:29:35,519 - INFO - Total parameters count: 4387458
2024-06-13 18:29:35,520 - INFO - Trainable parameters count: 1280 (0.029174068446923024%)
2024-06-13 18:30:43,054 - INFO - Total parameters count: 4387458
2024-06-13 18:30:43,055 - INFO - Trainable parameters count: 1280 (0.029174068446923024%)
2024-06-13 19:00:16,132 - INFO - Total parameters count: 4387458
2024-06-13 19:00:16,133 - INFO - Trainable parameters count: 1280 (0.029174068446923024%)
2024-06-13 19:01:13,749 - INFO - Total parameters count: 4387458
2024-06-13 19:01:13,749 - INFO - Trainable parameters count: 1280 (0.029174068446923024%)
2024-06-13 19:02:43,192 - INFO - Total parameters count: 4387458
2024-06-13 19:02:43,192 - INFO - Trainable parameters count: 1280 (0.029174068446923024%)
2024-06-13 19:06:09,480 - INFO - Evaluation results: {'eval_loss': 0.6909008622169495, 'eval_accuracy': 0.5103211009174312, 'eval_runtime': 0.0799, 'eval_samples_per_second': 10914.583, 'eval_steps_per_second': 350.468, 'epoch': 21.0}
2024-06-13 19:06:09,481 - INFO - Results saved to out/evaluation_results.json
2024-06-13 19:13:28,713 - INFO - New Parameters Added by Adapters: {'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.classifier.bias', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.classifier.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight'}
2024-06-13 19:16:26,707 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.classifier.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.classifier.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.key.bias'}
2024-06-13 19:16:26,707 - INFO - Total parameters count: 4392578
2024-06-13 19:16:26,707 - INFO - Trainable parameters count: 6400 (0.14570031539565148%)
2024-06-13 19:17:50,468 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'soft_prompt.soft_prompts', 'bert.bert.embeddings.position_embeddings.weight', 'bert.classifier.weight', 'bert.classifier.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.pooler.dense.weight', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.weight'}
2024-06-13 19:17:50,468 - INFO - Total parameters count: 4392578
2024-06-13 19:17:50,468 - INFO - Trainable parameters count: 6400 (0.14570031539565148%)
2024-06-13 19:22:12,096 - INFO - New Parameters Added by Adapters: {'bert.classifier.bias', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.pooler.dense.bias', 'bert.classifier.weight', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight'}
2024-06-13 21:04:23,223 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias'}
2024-06-13 21:04:23,223 - INFO - Total parameters: 4726656 || Trainable parameters: 340736 (7.208817396484957 %)
2024-06-13 21:04:30,417 - ERROR - Something went wrong The model did not return a loss from the inputs, only the following keys: last_hidden_state,pooler_output. For reference, the inputs it received are labels,input_ids,token_type_ids,attention_mask.
2024-06-13 21:13:21,369 - INFO - New Parameters Added by Adapters: {'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B'}
2024-06-13 21:13:21,369 - INFO - Total parameters: 4743426 || Trainable parameters: 340736 (7.183331204070645 %)
2024-06-13 21:18:09,772 - INFO - Evaluation results: {'eval_runtime': 0.1108, 'eval_samples_per_second': 7871.643, 'eval_steps_per_second': 252.759, 'epoch': 21.0}
2024-06-13 21:18:09,772 - INFO - Results saved to results/evaluation_results.json
2024-06-14 07:43:58,927 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'heads.mrpc.1.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'heads.mrpc.4.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'heads.mrpc.1.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'heads.mrpc.4.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight'}
2024-06-14 07:43:58,927 - INFO - Total parameters: 4743426 || Trainable parameters: 357506 (7.536873137685715 %)
2024-06-14 07:44:58,089 - INFO - New Parameters Added by Adapters: {'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'heads.mrpc.1.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'heads.mrpc.1.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'heads.mrpc.4.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'heads.mrpc.4.weight'}
2024-06-14 07:44:58,089 - INFO - Total parameters: 4743426 || Trainable parameters: 357506 (7.536873137685715 %)
2024-06-14 07:45:24,765 - INFO - Evaluation results: {'eval_runtime': 0.1133, 'eval_samples_per_second': 7693.915, 'eval_steps_per_second': 247.052, 'epoch': 2.0}
2024-06-14 07:45:24,765 - INFO - Results saved to results/evaluation_results.json
2024-06-14 07:57:08,657 - INFO - New Parameters Added by Adapters: {'heads.mrpc.1.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'heads.mrpc.4.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'heads.mrpc.4.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'heads.mrpc.1.bias'}
2024-06-14 07:57:08,658 - INFO - Total parameters: 4743426 || Trainable parameters: 357506 (7.536873137685715 %)
2024-06-14 07:57:36,656 - INFO - Evaluation results: {'eval_runtime': 0.1136, 'eval_samples_per_second': 7675.862, 'eval_steps_per_second': 246.473, 'epoch': 2.0}
2024-06-14 07:57:36,656 - INFO - Results saved to results/evaluation_results.json
2024-06-14 08:10:45,964 - INFO - New Parameters Added by Adapters: {'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.query.loras.lora.lora_B', 'heads.mrpc.1.bias', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_A', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_B', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'heads.mrpc.1.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.1.attention.self.value.loras.lora.lora_B', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_A', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.bias', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'heads.mrpc.4.bias', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.weight', 'bert.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.0.bias', 'bert.encoder.layer.0.attention.self.query.loras.lora.lora_B', 'bert.encoder.layer.0.attention.self.value.loras.lora.lora_A', 'bert.encoder.layer.1.attention.self.prefix_tuning.pool.prefix_tunings.prefix.self_prefix.control_trans.2.weight', 'heads.mrpc.4.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.wte.weight', 'bert.prefix_tuning.prefix_tunings.prefix.self_prefix.control_trans.2.bias'}
2024-06-14 08:10:45,965 - INFO - Total parameters: 4743426 || Trainable parameters: 357506 (7.536873137685715 %)
2024-06-14 08:11:59,423 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-14 08:11:59,606 - INFO - Total parameters: 4743426 || Trainable parameters: 357506 (7.536873137685715 %)
2024-06-14 08:12:25,958 - INFO - Evaluation results: {'eval_runtime': 0.1099, 'eval_samples_per_second': 7936.07, 'eval_steps_per_second': 254.828, 'epoch': 2.0}
2024-06-14 08:12:25,958 - INFO - Results saved to results/evaluation_results.json
2024-06-14 08:14:24,525 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-14 08:14:24,756 - INFO - Total parameters: 4743426 || Trainable parameters: 357506 (7.536873137685715 %)
2024-06-14 08:15:34,993 - INFO - Evaluation results: {'eval_runtime': 0.1525, 'eval_samples_per_second': 5719.753, 'eval_steps_per_second': 183.662, 'epoch': 2.0}
2024-06-14 08:15:34,999 - INFO - Results saved to results/evaluation_results.json
2024-06-14 08:15:49,666 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-14 08:15:49,845 - INFO - Total parameters: 4743426 || Trainable parameters: 357506 (7.536873137685715 %)
2024-06-14 08:16:18,252 - INFO - Evaluation results: {'eval_runtime': 0.1098, 'eval_samples_per_second': 7942.912, 'eval_steps_per_second': 255.048, 'epoch': 2.0}
2024-06-14 08:16:18,252 - INFO - Results saved to results/evaluation_results.json
2024-06-14 10:42:59,135 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-14 10:42:59,302 - ERROR - Something went wrong 'BertForSequenceClassification' object has no attribute 'add_classification_head'
2024-06-14 12:37:14,700 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 12:37:52,129 - INFO - Train results: TrainOutput(global_step=4210, training_loss=0.6696402109150649, metrics={'train_runtime': 25.653, 'train_samples_per_second': 5250.774, 'train_steps_per_second': 164.113, 'train_loss': 0.6696402109150649, 'epoch': 2.0})
2024-06-14 12:37:52,231 - INFO - Evaluation results: {'eval_loss': 0.6591554880142212, 'eval_accuracy': 0.6089449541284404, 'eval_runtime': 0.101, 'eval_samples_per_second': 8633.372, 'eval_steps_per_second': 277.218, 'epoch': 2.0}
2024-06-14 12:37:52,232 - ERROR - Something went wrong 'dict' object has no attribute 'metrics'
2024-06-14 12:38:58,163 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 12:40:02,637 - INFO - Train results: TrainOutput(global_step=4210, training_loss=0.6674497369915743, metrics={'train_runtime': 56.1942, 'train_samples_per_second': 2397.007, 'train_steps_per_second': 74.919, 'train_loss': 0.6674497369915743, 'epoch': 2.0})
2024-06-14 12:43:23,390 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 12:44:27,556 - INFO - Train results: TrainOutput(global_step=4210, training_loss=0.6615947322437712, metrics={'train_runtime': 55.8042, 'train_samples_per_second': 2413.762, 'train_steps_per_second': 75.442, 'train_loss': 0.6615947322437712, 'epoch': 2.0})
2024-06-14 12:44:27,703 - INFO - Evaluation results: {'eval_loss': 0.6497509479522705, 'eval_accuracy': 0.6410550458715596, 'eval_runtime': 0.1433, 'eval_samples_per_second': 6085.356, 'eval_steps_per_second': 195.401, 'epoch': 2.0}
2024-06-14 12:44:27,720 - ERROR - Something went wrong CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2024-06-14 12:45:03,811 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 12:45:36,303 - INFO - Train results: TrainOutput(global_step=4210, training_loss=0.6670617228165941, metrics={'train_runtime': 26.0814, 'train_samples_per_second': 5164.521, 'train_steps_per_second': 161.418, 'train_loss': 0.6670617228165941, 'epoch': 2.0})
2024-06-14 12:45:36,407 - INFO - Evaluation results: {'eval_loss': 0.6566453576087952, 'eval_accuracy': 0.6135321100917431, 'eval_runtime': 0.1024, 'eval_samples_per_second': 8519.468, 'eval_steps_per_second': 273.561, 'epoch': 2.0}
2024-06-14 12:45:36,418 - ERROR - Something went wrong CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2024-06-14 12:48:15,975 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 12:49:21,275 - INFO - Train results: TrainOutput(global_step=4210, training_loss=0.6616583896646024, metrics={'train_runtime': 56.8925, 'train_samples_per_second': 2367.587, 'train_steps_per_second': 73.999, 'train_loss': 0.6616583896646024, 'epoch': 2.0})
2024-06-14 12:49:21,432 - INFO - Evaluation results: {'eval_loss': 0.6505487561225891, 'eval_accuracy': 0.6376146788990825, 'eval_runtime': 0.1533, 'eval_samples_per_second': 5688.218, 'eval_steps_per_second': 182.649, 'epoch': 2.0}
2024-06-14 13:12:17,393 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 13:13:21,644 - INFO - Train results: {'train_runtime': 55.8174, 'train_samples_per_second': 2413.19, 'train_steps_per_second': 75.424, 'train_loss': 0.6681003965844451, 'epoch': 2.0}
2024-06-14 13:13:21,900 - INFO - Evaluation results: {'eval_loss': 0.6605035662651062, 'eval_accuracy': 0.6032110091743119, 'eval_runtime': 0.1449, 'eval_samples_per_second': 6019.327, 'eval_steps_per_second': 193.281, 'epoch': 2.0}
2024-06-14 13:13:21,915 - ERROR - Something went wrong CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2024-06-14 13:14:56,130 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 13:15:37,135 - INFO - Train results: {'train_runtime': 34.6254, 'train_samples_per_second': 3890.146, 'train_steps_per_second': 121.587, 'train_loss': 0.6668938443100084, 'epoch': 2.0}
2024-06-14 13:15:37,309 - INFO - Evaluation results: {'eval_loss': 0.6569451689720154, 'eval_accuracy': 0.6227064220183486, 'eval_runtime': 0.1691, 'eval_samples_per_second': 5156.129, 'eval_steps_per_second': 165.564, 'epoch': 2.0}
2024-06-14 13:15:37,324 - ERROR - Something went wrong CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2024-06-14 13:16:34,839 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 13:17:14,890 - INFO - Train results: {'train_runtime': 33.5504, 'train_samples_per_second': 4014.801, 'train_steps_per_second': 125.483, 'train_loss': 0.6659569287243479, 'epoch': 2.0}
2024-06-14 13:17:15,023 - INFO - Evaluation results: {'eval_loss': 0.658628523349762, 'eval_accuracy': 0.6077981651376146, 'eval_runtime': 0.1285, 'eval_samples_per_second': 6786.9, 'eval_steps_per_second': 217.928, 'epoch': 2.0}
2024-06-14 13:17:15,038 - ERROR - Something went wrong CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2024-06-14 13:25:15,836 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 18:52:59,433 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-14 19:29:52,978 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-14 19:29:53,166 - ERROR - Something went wrong 'BertForSequenceClassification' object has no attribute 'add_classification_head'
2024-06-14 19:30:42,141 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-14 19:30:51,180 - INFO - Total parameters: 4726914 || Trainable parameters: 340736 (7.208423931554499 %)
2024-06-14 19:30:51,320 - ERROR - Something went wrong The pre-trained model weights are not frozen. For training adapters, please call the train_adapter() method
2024-06-14 21:09:02,009 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-14 21:09:02,179 - INFO - Total parameters: 4726914 || Trainable parameters: 332544 (7.035118472644097 %)
2024-06-14 21:09:28,574 - INFO - Train results: {'train_runtime': 26.2945, 'train_samples_per_second': 5122.666, 'train_steps_per_second': 160.109, 'train_loss': 0.6330607516182291, 'epoch': 2.0}
2024-06-14 21:09:28,701 - INFO - Evaluation results: {'eval_loss': 0.5988166332244873, 'eval_accuracy': 0.694954128440367, 'eval_runtime': 0.1229, 'eval_samples_per_second': 7094.483, 'eval_steps_per_second': 227.805, 'epoch': 2.0}
2024-06-14 21:09:28,702 - INFO - Results saved to results/evaluation_results.json
2024-06-14 21:32:38,339 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.classifier.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.pooler.dense.bias', 'bert.bert.embeddings.position_embeddings.weight', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.classifier.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight'}
2024-06-14 21:32:38,339 - INFO - Total parameters count: 4392578
2024-06-14 21:32:38,339 - INFO - Trainable parameters count: 6400 (0.14570031539565148%)
2024-06-14 21:32:38,514 - ERROR - Something went wrong CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
2024-06-14 21:33:30,117 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.classifier.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.pooler.dense.bias', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.classifier.bias', 'bert.bert.encoder.layer.1.output.dense.weight'}
2024-06-14 21:33:30,119 - INFO - Total parameters count: 4392578
2024-06-14 21:34:32,753 - INFO - Trainable parameters count: 6400 (0.14570031539565148%)
2024-06-14 21:34:58,819 - ERROR - Something went wrong CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2024-06-14 21:35:31,514 - INFO - New Parameters Added by Adapters: {'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'soft_prompt.soft_prompts', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.classifier.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.classifier.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.pooler.dense.weight'}
2024-06-14 21:35:31,517 - INFO - Total parameters count: 4392578
2024-06-14 21:35:31,517 - INFO - Trainable parameters count: 6400 (0.14570031539565148%)
2024-06-14 21:38:22,089 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.0.output.dense.bias', 'soft_prompt.soft_prompts', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.classifier.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.classifier.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias'}
2024-06-14 21:38:22,091 - INFO - Total parameters count: 4392578
2024-06-14 21:38:22,091 - INFO - Trainable parameters count: 6400 (0.14570031539565148%)
2024-06-14 21:38:22,092 - INFO - Unique labels: {0, 1, 2}
2024-06-14 21:42:46,360 - ERROR - Something went wrong CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
2024-06-14 21:46:16,237 - INFO - New Parameters Added by Adapters: {'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.classifier.bias', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.classifier.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'soft_prompt.soft_prompts'}
2024-06-14 21:46:16,237 - INFO - Total parameters count: 4392707
2024-06-14 21:46:16,237 - INFO - Trainable parameters count: 6400 (0.1456960366352684%)
2024-06-14 21:46:16,241 - INFO - Unique labels: {0, 1, 2}
2024-06-14 21:46:19,608 - INFO - Train results: {'train_runtime': 3.2602, 'train_samples_per_second': 6134.667, 'train_steps_per_second': 192.015, 'train_loss': 1.1163551144706556, 'epoch': 2.0}
2024-06-14 21:46:19,706 - INFO - Evaluation results: {'eval_loss': 1.1069685220718384, 'eval_accuracy': 0.324, 'eval_runtime': 0.0959, 'eval_samples_per_second': 10427.263, 'eval_steps_per_second': 333.672, 'epoch': 2.0}
2024-06-14 21:46:19,706 - INFO - Results saved to results/evaluation_results.json
2024-06-14 22:34:50,874 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.classifier.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.classifier.weight', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.0.attention.self.key.bias'}
2024-06-14 22:34:50,875 - ERROR - Something went wrong Please specify `target_modules` in `peft_config`
2024-06-14 22:35:42,771 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.classifier.bias', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.classifier.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.1.output.dense.bias'}
2024-06-14 22:35:42,772 - ERROR - Something went wrong Please specify `target_modules` in `peft_config`
2024-06-14 22:38:22,122 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.classifier.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.classifier.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.1.attention.self.value.weight'}
2024-06-14 22:46:24,354 - INFO - New Parameters Added by Adapters: {'bert.base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'bert.base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'bert.base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'soft_prompt.soft_prompts', 'bert.base_model.model.classifier.original_module.bias', 'bert.base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'bert.base_model.model.bert.embeddings.LayerNorm.weight', 'bert.base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'bert.base_model.model.bert.embeddings.word_embeddings.weight', 'bert.base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'bert.base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'bert.base_model.model.bert.encoder.layer.0.output.dense.weight', 'bert.base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'bert.base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'bert.base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'bert.base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.base_model.model.bert.embeddings.token_type_embeddings.weight', 'bert.base_model.model.bert.embeddings.LayerNorm.bias', 'bert.base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'bert.base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'bert.base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'bert.base_model.model.classifier.modules_to_save.default.weight', 'bert.base_model.model.bert.embeddings.position_embeddings.weight', 'bert.base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.base_model.model.bert.pooler.dense.weight', 'bert.base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'bert.base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'bert.base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'bert.base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'bert.base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'bert.base_model.model.classifier.modules_to_save.default.bias', 'bert.base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'bert.base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'bert.base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'bert.base_model.model.bert.encoder.layer.0.output.dense.bias', 'bert.base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'bert.base_model.model.bert.encoder.layer.1.output.dense.weight', 'bert.base_model.model.classifier.original_module.weight', 'bert.base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.base_model.model.bert.encoder.layer.1.output.dense.bias', 'bert.base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'bert.base_model.model.bert.pooler.dense.bias', 'bert.base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'bert.base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'bert.base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'bert.base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'bert.base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'bert.base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight'}
2024-06-14 22:46:24,354 - INFO - Total parameters count: 4401028
2024-06-14 22:46:24,354 - INFO - Trainable parameters count: 14850 (0.33742116614572776%)
2024-06-14 22:46:24,356 - INFO - Unique labels: {0, 1}
2024-06-14 22:46:28,350 - INFO - Train results: {'train_runtime': 3.8803, 'train_samples_per_second': 5154.223, 'train_steps_per_second': 161.327, 'train_loss': 0.6984596709473826, 'epoch': 2.0}
2024-06-14 22:46:28,451 - INFO - Evaluation results: {'eval_loss': 0.6930413842201233, 'eval_accuracy': 0.493, 'eval_runtime': 0.0989, 'eval_samples_per_second': 10109.972, 'eval_steps_per_second': 323.519, 'epoch': 2.0}
2024-06-14 22:46:28,451 - INFO - Results saved to results/evaluation_results.json
2024-06-14 22:50:38,018 - ERROR - Something went wrong 'BertForSequenceClassificationWithSoftPromptPeft' object has no attribute 'bertstate_dict'
2024-06-14 22:51:07,251 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight'}
2024-06-14 22:51:07,252 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight'}
2024-06-14 22:51:07,252 - ERROR - Something went wrong name 'base_model_parameters' is not defined
2024-06-14 22:52:40,753 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight'}
2024-06-14 22:52:40,753 - INFO - New Parameters Added by Adapters: set()
2024-06-14 22:52:40,753 - INFO - Total parameters count: 4401028
2024-06-14 22:52:40,753 - INFO - Trainable parameters count: 14850 (0.33742116614572776%)
2024-06-14 22:52:40,755 - INFO - Unique labels: {0, 1}
2024-06-14 22:52:44,562 - INFO - Train results: {'train_runtime': 3.7036, 'train_samples_per_second': 5400.196, 'train_steps_per_second': 169.026, 'train_loss': 0.6954565124389843, 'epoch': 2.0}
2024-06-14 22:52:44,660 - INFO - Evaluation results: {'eval_loss': 0.6938176155090332, 'eval_accuracy': 0.482, 'eval_runtime': 0.0967, 'eval_samples_per_second': 10337.087, 'eval_steps_per_second': 330.787, 'epoch': 2.0}
2024-06-14 22:52:44,661 - INFO - Results saved to results/evaluation_results.json
2024-06-14 22:55:25,277 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias'}
2024-06-14 22:55:25,277 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-14 22:55:29,509 - INFO - Train results: {'train_runtime': 4.1281, 'train_samples_per_second': 4844.819, 'train_steps_per_second': 151.643, 'train_loss': 0.6923354296638562, 'epoch': 2.0}
2024-06-14 22:55:29,637 - INFO - Evaluation results: {'eval_loss': 0.6673650741577148, 'eval_accuracy': 0.635, 'eval_runtime': 0.1269, 'eval_samples_per_second': 7881.497, 'eval_steps_per_second': 252.208, 'epoch': 2.0}
2024-06-14 22:55:29,638 - INFO - Results saved to results/evaluation_results.json
2024-06-15 10:20:48,752 - INFO - Total parameters count: 4386178
2024-06-15 10:20:48,752 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 10:20:49,262 - ERROR - Something went wrong The privacy budget is too low.
2024-06-15 10:21:52,627 - INFO - Total parameters count: 4386178
2024-06-15 10:21:52,627 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 10:21:52,628 - ERROR - Something went wrong [IllegalModuleConfigurationError('Model needs to be in training mode')]
2024-06-15 10:22:52,606 - INFO - Total parameters count: 4386178
2024-06-15 10:22:52,606 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 10:22:59,738 - ERROR - Something went wrong [IllegalModuleConfigurationError('Model needs to be in training mode')]
2024-06-15 10:26:20,303 - INFO - Total parameters count: 4386178
2024-06-15 10:26:20,303 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 10:26:20,304 - ERROR - Something went wrong 'Dataset' object has no attribute 'dataset'
2024-06-15 10:26:53,060 - INFO - Total parameters count: 4386178
2024-06-15 10:26:53,060 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 10:28:44,918 - INFO - Total parameters count: 4386178
2024-06-15 10:28:44,918 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 10:28:50,419 - INFO - Total parameters count: 4386178
2024-06-15 10:28:50,419 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 10:29:32,512 - INFO - Total parameters count: 4386178
2024-06-15 10:29:32,512 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 10:30:08,962 - INFO - Total parameters count: 4386178
2024-06-15 10:30:08,962 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 10:31:35,760 - INFO - Total parameters count: 4386178
2024-06-15 10:31:35,760 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 11:13:39,156 - INFO - Total parameters count: 4386178
2024-06-15 11:13:39,156 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 11:14:30,925 - INFO - Total parameters count: 4386178
2024-06-15 11:14:30,925 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 11:24:39,233 - INFO - Total parameters count: 4386178
2024-06-15 11:24:39,233 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 18:18:15,516 - INFO - Total parameters count: 4386178
2024-06-15 18:18:15,516 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 18:19:19,849 - INFO - Total parameters count: 4386178
2024-06-15 18:19:19,849 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 18:22:28,999 - INFO - Total parameters count: 4386178
2024-06-15 18:22:28,999 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 18:22:34,565 - INFO - Total parameters count: 4386178
2024-06-15 18:22:34,565 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 18:23:01,718 - INFO - Total parameters count: 4386178
2024-06-15 18:23:01,718 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 18:23:07,388 - INFO - Total parameters count: 4386178
2024-06-15 18:23:07,389 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 18:24:36,326 - INFO - Total parameters count: 4386178
2024-06-15 18:24:36,326 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 18:25:19,854 - INFO - Total parameters count: 4386178
2024-06-15 18:25:19,855 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 23:22:35,084 - INFO - Total parameters count: 4386178
2024-06-15 23:22:35,084 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 23:26:31,784 - INFO - Total parameters count: 4386178
2024-06-15 23:26:31,784 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 23:31:06,413 - INFO - Total parameters count: 4386178
2024-06-15 23:31:06,413 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 23:34:52,344 - INFO - Total parameters count: 4386178
2024-06-15 23:34:52,344 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-15 23:36:02,525 - INFO - Total parameters count: 4386178
2024-06-15 23:36:02,525 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-16 22:12:38,551 - INFO - Total parameters: 4386178 || Trainable parameters: 4386178 (100.0%)
2024-06-16 22:13:13,265 - INFO - Total parameters: 4386178 || Trainable parameters: 4386178 (100.0%)
2024-06-16 22:13:49,343 - INFO - Epoch: 1 | Step: 5000 | Train loss: 0.701 | Eval loss: 0.718 | Eval accuracy: 0.495 | ɛ: 4.48
2024-06-16 22:14:28,393 - INFO - Epoch: 1 | Step: 10000 | Train loss: 0.702 | Eval loss: 0.701 | Eval accuracy: 0.496 | ɛ: 4.99
2024-06-16 22:15:05,232 - INFO - Epoch: 1 | Step: 15000 | Train loss: 0.697 | Eval loss: 0.693 | Eval accuracy: 0.533 | ɛ: 5.29
2024-06-16 22:15:40,720 - INFO - Epoch: 1 | Step: 20000 | Train loss: 0.697 | Eval loss: 0.722 | Eval accuracy: 0.564 | ɛ: 5.50
2024-06-16 22:16:17,926 - INFO - Epoch: 1 | Step: 25000 | Train loss: 0.696 | Eval loss: 0.749 | Eval accuracy: 0.640 | ɛ: 5.67
2024-06-16 22:16:52,791 - INFO - Epoch: 1 | Step: 30000 | Train loss: 0.701 | Eval loss: 0.803 | Eval accuracy: 0.672 | ɛ: 5.82
2024-06-16 22:17:26,964 - INFO - Epoch: 1 | Step: 35000 | Train loss: 0.719 | Eval loss: 0.998 | Eval accuracy: 0.673 | ɛ: 5.94
2024-06-16 22:18:01,582 - INFO - Epoch: 1 | Step: 40000 | Train loss: 0.760 | Eval loss: 1.262 | Eval accuracy: 0.672 | ɛ: 6.05
2024-06-16 22:18:36,169 - INFO - Epoch: 1 | Step: 45000 | Train loss: 0.817 | Eval loss: 1.533 | Eval accuracy: 0.674 | ɛ: 6.15
2024-06-16 22:19:11,042 - INFO - Epoch: 1 | Step: 50000 | Train loss: 0.886 | Eval loss: 1.670 | Eval accuracy: 0.680 | ɛ: 6.24
2024-06-16 22:19:45,697 - INFO - Epoch: 1 | Step: 55000 | Train loss: 0.956 | Eval loss: 1.751 | Eval accuracy: 0.683 | ɛ: 6.33
2024-06-16 22:20:20,937 - INFO - Epoch: 1 | Step: 60000 | Train loss: 1.016 | Eval loss: 1.812 | Eval accuracy: 0.686 | ɛ: 6.41
2024-06-16 22:20:57,161 - INFO - Epoch: 1 | Step: 65000 | Train loss: 1.072 | Eval loss: 1.861 | Eval accuracy: 0.687 | ɛ: 6.49
2024-06-16 22:21:32,644 - INFO - Epoch: 1 | Step: 70000 | Train loss: 1.126 | Eval loss: 1.907 | Eval accuracy: 0.686 | ɛ: 6.57
2024-06-16 22:22:07,801 - INFO - Epoch: 1 | Step: 75000 | Train loss: 1.171 | Eval loss: 1.916 | Eval accuracy: 0.688 | ɛ: 6.64
2024-06-16 22:22:42,568 - INFO - Epoch: 1 | Step: 80000 | Train loss: 1.211 | Eval loss: 1.905 | Eval accuracy: 0.689 | ɛ: 6.71
2024-06-16 22:23:17,000 - INFO - Epoch: 1 | Step: 85000 | Train loss: 1.250 | Eval loss: 1.914 | Eval accuracy: 0.688 | ɛ: 6.77
2024-06-16 22:23:52,228 - INFO - Epoch: 1 | Step: 90000 | Train loss: 1.282 | Eval loss: 1.934 | Eval accuracy: 0.684 | ɛ: 6.83
2024-06-16 22:24:27,691 - INFO - Epoch: 1 | Step: 95000 | Train loss: 1.311 | Eval loss: 1.914 | Eval accuracy: 0.689 | ɛ: 6.89
2024-06-16 22:25:03,092 - INFO - Epoch: 1 | Step: 100000 | Train loss: 1.338 | Eval loss: 1.931 | Eval accuracy: 0.686 | ɛ: 6.95
2024-06-16 22:26:13,288 - INFO - Epoch: 2 | Step: 5000 | Train loss: 1.854 | Eval loss: 1.926 | Eval accuracy: 0.690 | ɛ: 7.06
2024-06-16 22:26:47,575 - INFO - Epoch: 2 | Step: 10000 | Train loss: 1.815 | Eval loss: 1.895 | Eval accuracy: 0.695 | ɛ: 7.11
2024-06-16 22:27:21,764 - INFO - Epoch: 2 | Step: 15000 | Train loss: 1.801 | Eval loss: 1.945 | Eval accuracy: 0.688 | ɛ: 7.17
2024-06-16 22:27:56,160 - INFO - Epoch: 2 | Step: 20000 | Train loss: 1.799 | Eval loss: 1.952 | Eval accuracy: 0.688 | ɛ: 7.22
2024-06-16 22:28:31,592 - INFO - Epoch: 2 | Step: 25000 | Train loss: 1.802 | Eval loss: 1.907 | Eval accuracy: 0.693 | ɛ: 7.27
2024-06-16 22:29:07,386 - INFO - Epoch: 2 | Step: 30000 | Train loss: 1.796 | Eval loss: 1.903 | Eval accuracy: 0.695 | ɛ: 7.32
2024-06-16 22:29:43,184 - INFO - Epoch: 2 | Step: 35000 | Train loss: 1.786 | Eval loss: 1.909 | Eval accuracy: 0.694 | ɛ: 7.37
2024-06-16 22:30:19,346 - INFO - Epoch: 2 | Step: 40000 | Train loss: 1.790 | Eval loss: 1.887 | Eval accuracy: 0.696 | ɛ: 7.42
2024-06-16 22:30:55,688 - INFO - Epoch: 2 | Step: 45000 | Train loss: 1.796 | Eval loss: 1.872 | Eval accuracy: 0.696 | ɛ: 7.47
2024-06-16 22:31:32,367 - INFO - Total parameters: 4386178 || Trainable parameters: 258 (0.005882114223362572%)
2024-06-16 22:43:14,249 - INFO - Total parameters: 4386178 || Trainable parameters: 258 (0.005882114223362572%)
2024-06-16 22:43:21,319 - INFO - Epoch: 1 | Step: 5000 | Train loss: 0.805 | Eval loss: 0.830 | Eval accuracy: 0.504 | ɛ: 4.48
2024-06-16 22:43:28,789 - INFO - Epoch: 1 | Step: 10000 | Train loss: 0.802 | Eval loss: 0.790 | Eval accuracy: 0.517 | ɛ: 4.99
2024-06-16 22:43:36,440 - INFO - Epoch: 1 | Step: 15000 | Train loss: 0.785 | Eval loss: 0.779 | Eval accuracy: 0.526 | ɛ: 5.29
2024-06-16 22:43:44,214 - INFO - Epoch: 1 | Step: 20000 | Train loss: 0.778 | Eval loss: 0.826 | Eval accuracy: 0.522 | ɛ: 5.50
2024-06-16 22:43:51,167 - INFO - Epoch: 1 | Step: 25000 | Train loss: 0.777 | Eval loss: 0.876 | Eval accuracy: 0.520 | ɛ: 5.67
2024-06-16 22:43:58,675 - INFO - Epoch: 1 | Step: 30000 | Train loss: 0.781 | Eval loss: 0.849 | Eval accuracy: 0.533 | ɛ: 5.81
2024-06-16 22:44:05,633 - INFO - Epoch: 1 | Step: 35000 | Train loss: 0.783 | Eval loss: 0.873 | Eval accuracy: 0.538 | ɛ: 5.94
2024-06-16 22:44:13,953 - INFO - Epoch: 1 | Step: 40000 | Train loss: 0.786 | Eval loss: 0.939 | Eval accuracy: 0.535 | ɛ: 6.05
2024-06-16 22:44:21,536 - INFO - Epoch: 1 | Step: 45000 | Train loss: 0.793 | Eval loss: 0.954 | Eval accuracy: 0.541 | ɛ: 6.15
2024-06-16 22:44:28,661 - INFO - Epoch: 1 | Step: 50000 | Train loss: 0.797 | Eval loss: 0.931 | Eval accuracy: 0.558 | ɛ: 6.24
2024-06-16 22:44:36,175 - INFO - Epoch: 1 | Step: 55000 | Train loss: 0.800 | Eval loss: 0.972 | Eval accuracy: 0.556 | ɛ: 6.32
2024-06-16 22:44:43,571 - INFO - Epoch: 1 | Step: 60000 | Train loss: 0.805 | Eval loss: 1.035 | Eval accuracy: 0.552 | ɛ: 6.41
2024-06-16 22:44:51,084 - INFO - Epoch: 1 | Step: 65000 | Train loss: 0.813 | Eval loss: 1.065 | Eval accuracy: 0.557 | ɛ: 6.48
2024-06-16 22:44:58,293 - INFO - Epoch: 1 | Step: 70000 | Train loss: 0.820 | Eval loss: 1.091 | Eval accuracy: 0.558 | ɛ: 6.56
2024-06-16 22:45:05,456 - INFO - Epoch: 1 | Step: 75000 | Train loss: 0.827 | Eval loss: 1.146 | Eval accuracy: 0.559 | ɛ: 6.63
2024-06-16 22:45:13,214 - INFO - Epoch: 1 | Step: 80000 | Train loss: 0.835 | Eval loss: 1.185 | Eval accuracy: 0.560 | ɛ: 6.70
2024-06-16 22:45:20,709 - INFO - Epoch: 1 | Step: 85000 | Train loss: 0.842 | Eval loss: 1.159 | Eval accuracy: 0.573 | ɛ: 6.76
2024-06-16 22:45:27,858 - INFO - Epoch: 1 | Step: 90000 | Train loss: 0.849 | Eval loss: 1.170 | Eval accuracy: 0.576 | ɛ: 6.82
2024-06-16 22:45:35,585 - INFO - Epoch: 1 | Step: 95000 | Train loss: 0.855 | Eval loss: 1.170 | Eval accuracy: 0.584 | ɛ: 6.88
2024-06-16 22:45:43,416 - INFO - Epoch: 1 | Step: 100000 | Train loss: 0.862 | Eval loss: 1.134 | Eval accuracy: 0.594 | ɛ: 6.94
2024-06-16 22:45:51,737 - INFO - Epoch: 1 | Step: 105000 | Train loss: 0.868 | Eval loss: 1.081 | Eval accuracy: 0.599 | ɛ: 7.00
2024-06-16 22:46:07,207 - INFO - Epoch: 2 | Step: 5000 | Train loss: 0.923 | Eval loss: 1.095 | Eval accuracy: 0.602 | ɛ: 7.06
2024-06-16 22:46:14,765 - INFO - Epoch: 2 | Step: 10000 | Train loss: 0.955 | Eval loss: 1.127 | Eval accuracy: 0.602 | ɛ: 7.11
2024-06-16 22:46:22,531 - INFO - Epoch: 2 | Step: 15000 | Train loss: 0.962 | Eval loss: 1.146 | Eval accuracy: 0.602 | ɛ: 7.17
2024-06-16 22:46:29,682 - INFO - Epoch: 2 | Step: 20000 | Train loss: 0.979 | Eval loss: 1.139 | Eval accuracy: 0.606 | ɛ: 7.22
2024-06-16 22:46:36,838 - INFO - Epoch: 2 | Step: 25000 | Train loss: 0.983 | Eval loss: 1.110 | Eval accuracy: 0.614 | ɛ: 7.27
2024-06-16 22:46:44,661 - INFO - Epoch: 2 | Step: 30000 | Train loss: 0.996 | Eval loss: 1.084 | Eval accuracy: 0.625 | ɛ: 7.32
2024-06-16 22:46:52,347 - INFO - Epoch: 2 | Step: 35000 | Train loss: 1.002 | Eval loss: 1.111 | Eval accuracy: 0.624 | ɛ: 7.37
2024-06-16 22:46:59,343 - INFO - Epoch: 2 | Step: 40000 | Train loss: 1.012 | Eval loss: 1.107 | Eval accuracy: 0.628 | ɛ: 7.42
2024-06-16 22:47:07,706 - INFO - Epoch: 2 | Step: 45000 | Train loss: 1.015 | Eval loss: 1.123 | Eval accuracy: 0.628 | ɛ: 7.47
2024-06-16 22:47:16,351 - INFO - Epoch: 2 | Step: 50000 | Train loss: 1.021 | Eval loss: 1.112 | Eval accuracy: 0.632 | ɛ: 7.51
2024-06-16 22:47:23,991 - INFO - Epoch: 2 | Step: 55000 | Train loss: 1.030 | Eval loss: 1.148 | Eval accuracy: 0.629 | ɛ: 7.56
2024-06-16 22:47:32,316 - INFO - Epoch: 2 | Step: 60000 | Train loss: 1.038 | Eval loss: 1.161 | Eval accuracy: 0.630 | ɛ: 7.61
2024-06-16 22:47:41,223 - INFO - Epoch: 2 | Step: 65000 | Train loss: 1.049 | Eval loss: 1.135 | Eval accuracy: 0.634 | ɛ: 7.65
2024-06-16 22:47:48,371 - INFO - Epoch: 2 | Step: 70000 | Train loss: 1.056 | Eval loss: 1.157 | Eval accuracy: 0.633 | ɛ: 7.69
2024-06-16 22:47:56,408 - INFO - Epoch: 2 | Step: 75000 | Train loss: 1.063 | Eval loss: 1.175 | Eval accuracy: 0.633 | ɛ: 7.74
2024-06-16 22:48:03,864 - INFO - Epoch: 2 | Step: 80000 | Train loss: 1.068 | Eval loss: 1.198 | Eval accuracy: 0.632 | ɛ: 7.78
2024-06-16 22:48:11,679 - INFO - Epoch: 2 | Step: 85000 | Train loss: 1.075 | Eval loss: 1.191 | Eval accuracy: 0.633 | ɛ: 7.83
2024-06-16 22:48:19,809 - INFO - Epoch: 2 | Step: 90000 | Train loss: 1.081 | Eval loss: 1.214 | Eval accuracy: 0.632 | ɛ: 7.87
2024-06-16 22:48:28,500 - INFO - Epoch: 2 | Step: 95000 | Train loss: 1.085 | Eval loss: 1.233 | Eval accuracy: 0.631 | ɛ: 7.91
2024-06-16 22:48:36,024 - INFO - Epoch: 2 | Step: 100000 | Train loss: 1.092 | Eval loss: 1.242 | Eval accuracy: 0.631 | ɛ: 7.95
2024-06-16 22:48:44,695 - INFO - Epoch: 2 | Step: 105000 | Train loss: 1.096 | Eval loss: 1.225 | Eval accuracy: 0.634 | ɛ: 7.99
2024-06-16 22:48:53,195 - INFO - Results saved to results/evaluation_results.json
2024-06-16 23:13:17,709 - INFO - Total parameters: 4386307 || Trainable parameters: 387 (0.00882291184816749%)
2024-06-16 23:13:25,505 - INFO - Epoch: 1 | Step: 5000 | Train loss: 1.171 | Eval loss: 1.147 | Eval accuracy: 0.364 | ɛ: 0.01
2024-06-16 23:13:33,449 - INFO - Epoch: 1 | Step: 10000 | Train loss: 1.169 | Eval loss: 1.144 | Eval accuracy: 0.365 | ɛ: 0.01
2024-06-16 23:13:41,343 - INFO - Epoch: 1 | Step: 15000 | Train loss: 1.172 | Eval loss: 1.151 | Eval accuracy: 0.372 | ɛ: 0.01
2024-06-16 23:13:48,654 - INFO - Epoch: 1 | Step: 20000 | Train loss: 1.174 | Eval loss: 1.166 | Eval accuracy: 0.370 | ɛ: 0.01
2024-06-16 23:13:55,954 - INFO - Epoch: 1 | Step: 25000 | Train loss: 1.178 | Eval loss: 1.183 | Eval accuracy: 0.374 | ɛ: 0.01
2024-06-16 23:14:03,550 - INFO - Epoch: 1 | Step: 30000 | Train loss: 1.184 | Eval loss: 1.202 | Eval accuracy: 0.373 | ɛ: 0.01
2024-06-16 23:14:11,007 - INFO - Epoch: 1 | Step: 35000 | Train loss: 1.190 | Eval loss: 1.192 | Eval accuracy: 0.376 | ɛ: 0.01
2024-06-16 23:14:19,392 - INFO - Epoch: 1 | Step: 40000 | Train loss: 1.193 | Eval loss: 1.157 | Eval accuracy: 0.389 | ɛ: 0.01
2024-06-16 23:14:27,625 - INFO - Epoch: 1 | Step: 45000 | Train loss: 1.195 | Eval loss: 1.149 | Eval accuracy: 0.392 | ɛ: 0.01
2024-06-16 23:14:35,549 - INFO - Epoch: 1 | Step: 50000 | Train loss: 1.196 | Eval loss: 1.150 | Eval accuracy: 0.387 | ɛ: 0.01
2024-06-16 23:14:43,310 - INFO - Epoch: 1 | Step: 55000 | Train loss: 1.196 | Eval loss: 1.149 | Eval accuracy: 0.385 | ɛ: 0.01
2024-06-16 23:14:51,379 - INFO - Epoch: 1 | Step: 60000 | Train loss: 1.198 | Eval loss: 1.169 | Eval accuracy: 0.395 | ɛ: 0.01
2024-06-16 23:15:00,225 - INFO - Epoch: 1 | Step: 65000 | Train loss: 1.200 | Eval loss: 1.192 | Eval accuracy: 0.395 | ɛ: 0.01
2024-06-16 23:15:08,524 - INFO - Epoch: 1 | Step: 70000 | Train loss: 1.203 | Eval loss: 1.236 | Eval accuracy: 0.388 | ɛ: 0.01
2024-06-16 23:15:16,839 - INFO - Epoch: 1 | Step: 75000 | Train loss: 1.205 | Eval loss: 1.205 | Eval accuracy: 0.389 | ɛ: 0.01
2024-06-16 23:15:24,115 - INFO - Epoch: 1 | Step: 80000 | Train loss: 1.208 | Eval loss: 1.193 | Eval accuracy: 0.392 | ɛ: 0.01
2024-06-16 23:15:31,805 - INFO - Epoch: 1 | Step: 85000 | Train loss: 1.211 | Eval loss: 1.228 | Eval accuracy: 0.386 | ɛ: 0.01
2024-06-16 23:15:39,397 - INFO - Epoch: 1 | Step: 90000 | Train loss: 1.214 | Eval loss: 1.201 | Eval accuracy: 0.391 | ɛ: 0.01
2024-06-16 23:15:46,992 - INFO - Epoch: 1 | Step: 95000 | Train loss: 1.216 | Eval loss: 1.199 | Eval accuracy: 0.390 | ɛ: 0.01
2024-06-16 23:15:56,512 - INFO - Epoch: 1 | Step: 100000 | Train loss: 1.220 | Eval loss: 1.272 | Eval accuracy: 0.392 | ɛ: 0.01
2024-06-16 23:16:04,820 - INFO - Epoch: 1 | Step: 105000 | Train loss: 1.225 | Eval loss: 1.238 | Eval accuracy: 0.387 | ɛ: 0.01
2024-06-16 23:16:12,919 - INFO - Epoch: 1 | Step: 110000 | Train loss: 1.229 | Eval loss: 1.302 | Eval accuracy: 0.392 | ɛ: 0.01
2024-06-16 23:16:20,812 - INFO - Epoch: 1 | Step: 115000 | Train loss: 1.233 | Eval loss: 1.267 | Eval accuracy: 0.393 | ɛ: 0.01
2024-06-16 23:16:28,400 - INFO - Epoch: 1 | Step: 120000 | Train loss: 1.237 | Eval loss: 1.293 | Eval accuracy: 0.388 | ɛ: 0.01
2024-06-16 23:16:35,843 - INFO - Epoch: 1 | Step: 125000 | Train loss: 1.241 | Eval loss: 1.280 | Eval accuracy: 0.388 | ɛ: 0.01
2024-06-16 23:16:43,153 - INFO - Epoch: 1 | Step: 130000 | Train loss: 1.245 | Eval loss: 1.345 | Eval accuracy: 0.393 | ɛ: 0.01
2024-06-16 23:16:50,831 - INFO - Epoch: 1 | Step: 135000 | Train loss: 1.249 | Eval loss: 1.277 | Eval accuracy: 0.395 | ɛ: 0.01
2024-06-16 23:16:58,652 - INFO - Epoch: 1 | Step: 140000 | Train loss: 1.253 | Eval loss: 1.276 | Eval accuracy: 0.394 | ɛ: 0.01
2024-06-16 23:17:07,154 - INFO - Epoch: 1 | Step: 145000 | Train loss: 1.256 | Eval loss: 1.282 | Eval accuracy: 0.393 | ɛ: 0.01
2024-06-16 23:17:14,777 - INFO - Epoch: 1 | Step: 150000 | Train loss: 1.259 | Eval loss: 1.228 | Eval accuracy: 0.391 | ɛ: 0.01
2024-06-16 23:17:21,794 - INFO - Epoch: 1 | Step: 155000 | Train loss: 1.261 | Eval loss: 1.245 | Eval accuracy: 0.390 | ɛ: 0.01
2024-06-16 23:17:30,834 - INFO - Epoch: 1 | Step: 160000 | Train loss: 1.264 | Eval loss: 1.253 | Eval accuracy: 0.385 | ɛ: 0.01
2024-06-16 23:17:38,045 - INFO - Epoch: 1 | Step: 165000 | Train loss: 1.266 | Eval loss: 1.271 | Eval accuracy: 0.387 | ɛ: 0.01
2024-06-16 23:17:45,360 - INFO - Epoch: 1 | Step: 170000 | Train loss: 1.269 | Eval loss: 1.272 | Eval accuracy: 0.389 | ɛ: 0.01
2024-06-16 23:17:52,307 - INFO - Epoch: 1 | Step: 175000 | Train loss: 1.271 | Eval loss: 1.273 | Eval accuracy: 0.391 | ɛ: 0.01
2024-06-16 23:18:00,713 - INFO - Epoch: 1 | Step: 180000 | Train loss: 1.274 | Eval loss: 1.270 | Eval accuracy: 0.391 | ɛ: 0.01
2024-06-16 23:18:08,791 - INFO - Epoch: 1 | Step: 185000 | Train loss: 1.276 | Eval loss: 1.269 | Eval accuracy: 0.391 | ɛ: 0.01
2024-06-16 23:18:16,006 - INFO - Epoch: 1 | Step: 190000 | Train loss: 1.279 | Eval loss: 1.272 | Eval accuracy: 0.391 | ɛ: 0.01
2024-06-16 23:18:24,012 - INFO - Epoch: 1 | Step: 195000 | Train loss: 1.282 | Eval loss: 1.309 | Eval accuracy: 0.391 | ɛ: 0.01
2024-06-16 23:18:31,346 - INFO - Epoch: 1 | Step: 200000 | Train loss: 1.285 | Eval loss: 1.328 | Eval accuracy: 0.390 | ɛ: 0.01
2024-06-16 23:18:38,962 - INFO - Epoch: 1 | Step: 205000 | Train loss: 1.287 | Eval loss: 1.288 | Eval accuracy: 0.393 | ɛ: 0.01
2024-06-16 23:18:46,260 - INFO - Epoch: 1 | Step: 210000 | Train loss: 1.289 | Eval loss: 1.266 | Eval accuracy: 0.392 | ɛ: 0.01
2024-06-16 23:18:53,912 - INFO - Epoch: 1 | Step: 215000 | Train loss: 1.291 | Eval loss: 1.296 | Eval accuracy: 0.388 | ɛ: 0.01
2024-06-16 23:19:01,297 - INFO - Epoch: 1 | Step: 220000 | Train loss: 1.293 | Eval loss: 1.284 | Eval accuracy: 0.387 | ɛ: 0.01
2024-06-16 23:19:08,524 - INFO - Epoch: 1 | Step: 225000 | Train loss: 1.294 | Eval loss: 1.246 | Eval accuracy: 0.392 | ɛ: 0.01
2024-06-16 23:19:15,588 - INFO - Epoch: 1 | Step: 230000 | Train loss: 1.296 | Eval loss: 1.272 | Eval accuracy: 0.391 | ɛ: 0.01
2024-06-16 23:19:23,133 - INFO - Epoch: 1 | Step: 235000 | Train loss: 1.298 | Eval loss: 1.281 | Eval accuracy: 0.391 | ɛ: 0.01
2024-06-16 23:19:30,460 - INFO - Epoch: 1 | Step: 240000 | Train loss: 1.301 | Eval loss: 1.257 | Eval accuracy: 0.390 | ɛ: 0.01
2024-06-16 23:19:37,351 - INFO - Epoch: 1 | Step: 245000 | Train loss: 1.302 | Eval loss: 1.253 | Eval accuracy: 0.390 | ɛ: 0.01
2024-06-16 23:19:44,502 - INFO - Epoch: 1 | Step: 250000 | Train loss: 1.304 | Eval loss: 1.316 | Eval accuracy: 0.394 | ɛ: 0.01
2024-06-16 23:19:51,951 - INFO - Epoch: 1 | Step: 255000 | Train loss: 1.306 | Eval loss: 1.272 | Eval accuracy: 0.394 | ɛ: 0.01
2024-06-16 23:20:00,109 - INFO - Epoch: 1 | Step: 260000 | Train loss: 1.308 | Eval loss: 1.290 | Eval accuracy: 0.393 | ɛ: 0.01
2024-06-16 23:20:07,335 - INFO - Epoch: 1 | Step: 265000 | Train loss: 1.311 | Eval loss: 1.288 | Eval accuracy: 0.396 | ɛ: 0.01
2024-06-16 23:20:15,094 - INFO - Epoch: 1 | Step: 270000 | Train loss: 1.313 | Eval loss: 1.310 | Eval accuracy: 0.398 | ɛ: 0.01
2024-06-16 23:20:23,168 - INFO - Epoch: 1 | Step: 275000 | Train loss: 1.316 | Eval loss: 1.346 | Eval accuracy: 0.403 | ɛ: 0.01
2024-06-16 23:20:31,138 - INFO - Epoch: 1 | Step: 280000 | Train loss: 1.319 | Eval loss: 1.333 | Eval accuracy: 0.396 | ɛ: 0.01
2024-06-16 23:20:38,777 - INFO - Epoch: 1 | Step: 285000 | Train loss: 1.323 | Eval loss: 1.319 | Eval accuracy: 0.395 | ɛ: 0.01
2024-06-16 23:20:46,030 - INFO - Epoch: 1 | Step: 290000 | Train loss: 1.325 | Eval loss: 1.302 | Eval accuracy: 0.391 | ɛ: 0.01
2024-06-16 23:20:53,655 - INFO - Epoch: 1 | Step: 295000 | Train loss: 1.327 | Eval loss: 1.311 | Eval accuracy: 0.397 | ɛ: 0.01
2024-06-16 23:21:00,864 - INFO - Epoch: 1 | Step: 300000 | Train loss: 1.330 | Eval loss: 1.298 | Eval accuracy: 0.398 | ɛ: 0.01
2024-06-16 23:21:08,117 - INFO - Epoch: 1 | Step: 305000 | Train loss: 1.333 | Eval loss: 1.327 | Eval accuracy: 0.393 | ɛ: 0.01
2024-06-16 23:21:15,367 - INFO - Epoch: 1 | Step: 310000 | Train loss: 1.335 | Eval loss: 1.385 | Eval accuracy: 0.395 | ɛ: 0.01
2024-06-16 23:21:22,488 - INFO - Epoch: 1 | Step: 315000 | Train loss: 1.338 | Eval loss: 1.370 | Eval accuracy: 0.398 | ɛ: 0.01
2024-06-16 23:21:29,460 - INFO - Epoch: 1 | Step: 320000 | Train loss: 1.341 | Eval loss: 1.364 | Eval accuracy: 0.394 | ɛ: 0.01
2024-06-16 23:21:36,599 - INFO - Epoch: 1 | Step: 325000 | Train loss: 1.344 | Eval loss: 1.371 | Eval accuracy: 0.398 | ɛ: 0.01
2024-06-16 23:21:43,690 - INFO - Epoch: 1 | Step: 330000 | Train loss: 1.347 | Eval loss: 1.388 | Eval accuracy: 0.401 | ɛ: 0.01
2024-06-16 23:21:50,847 - INFO - Epoch: 1 | Step: 335000 | Train loss: 1.349 | Eval loss: 1.366 | Eval accuracy: 0.397 | ɛ: 0.01
2024-06-16 23:21:58,191 - INFO - Epoch: 1 | Step: 340000 | Train loss: 1.352 | Eval loss: 1.355 | Eval accuracy: 0.398 | ɛ: 0.01
2024-06-16 23:22:05,254 - INFO - Epoch: 1 | Step: 345000 | Train loss: 1.354 | Eval loss: 1.360 | Eval accuracy: 0.401 | ɛ: 0.01
2024-06-16 23:22:12,842 - INFO - Epoch: 1 | Step: 350000 | Train loss: 1.357 | Eval loss: 1.382 | Eval accuracy: 0.400 | ɛ: 0.01
2024-06-16 23:22:21,709 - INFO - Epoch: 1 | Step: 355000 | Train loss: 1.360 | Eval loss: 1.402 | Eval accuracy: 0.400 | ɛ: 0.01
2024-06-16 23:22:29,828 - INFO - Epoch: 1 | Step: 360000 | Train loss: 1.362 | Eval loss: 1.422 | Eval accuracy: 0.399 | ɛ: 0.01
2024-06-16 23:22:38,302 - INFO - Epoch: 1 | Step: 365000 | Train loss: 1.365 | Eval loss: 1.417 | Eval accuracy: 0.396 | ɛ: 0.01
2024-06-16 23:22:46,766 - INFO - Epoch: 1 | Step: 370000 | Train loss: 1.367 | Eval loss: 1.347 | Eval accuracy: 0.395 | ɛ: 0.01
2024-06-16 23:22:54,128 - INFO - Epoch: 1 | Step: 375000 | Train loss: 1.370 | Eval loss: 1.383 | Eval accuracy: 0.400 | ɛ: 0.01
2024-06-16 23:23:02,147 - INFO - Epoch: 1 | Step: 380000 | Train loss: 1.372 | Eval loss: 1.358 | Eval accuracy: 0.399 | ɛ: 0.01
2024-06-16 23:23:09,693 - INFO - Epoch: 1 | Step: 385000 | Train loss: 1.375 | Eval loss: 1.349 | Eval accuracy: 0.402 | ɛ: 0.01
2024-06-16 23:23:17,213 - INFO - Epoch: 1 | Step: 390000 | Train loss: 1.377 | Eval loss: 1.348 | Eval accuracy: 0.396 | ɛ: 0.01
2024-06-16 23:24:00,265 - INFO - Epoch: 2 | Step: 5000 | Train loss: 1.599 | Eval loss: 1.385 | Eval accuracy: 0.404 | ɛ: 0.01
2024-06-16 23:24:07,368 - INFO - Epoch: 2 | Step: 10000 | Train loss: 1.590 | Eval loss: 1.356 | Eval accuracy: 0.403 | ɛ: 0.01
2024-06-16 23:24:14,681 - INFO - Epoch: 2 | Step: 15000 | Train loss: 1.576 | Eval loss: 1.365 | Eval accuracy: 0.404 | ɛ: 0.01
2024-06-16 23:24:21,942 - INFO - Epoch: 2 | Step: 20000 | Train loss: 1.576 | Eval loss: 1.394 | Eval accuracy: 0.406 | ɛ: 0.01
2024-06-16 23:24:29,429 - INFO - Epoch: 2 | Step: 25000 | Train loss: 1.576 | Eval loss: 1.392 | Eval accuracy: 0.404 | ɛ: 0.01
2024-06-16 23:24:37,507 - INFO - Epoch: 2 | Step: 30000 | Train loss: 1.577 | Eval loss: 1.380 | Eval accuracy: 0.397 | ɛ: 0.01
2024-06-16 23:24:46,185 - INFO - Epoch: 2 | Step: 35000 | Train loss: 1.577 | Eval loss: 1.380 | Eval accuracy: 0.396 | ɛ: 0.01
2024-06-16 23:24:54,665 - INFO - Epoch: 2 | Step: 40000 | Train loss: 1.579 | Eval loss: 1.404 | Eval accuracy: 0.398 | ɛ: 0.01
2024-06-16 23:25:01,891 - INFO - Epoch: 2 | Step: 45000 | Train loss: 1.585 | Eval loss: 1.410 | Eval accuracy: 0.400 | ɛ: 0.01
2024-06-16 23:25:09,785 - INFO - Epoch: 2 | Step: 50000 | Train loss: 1.588 | Eval loss: 1.397 | Eval accuracy: 0.395 | ɛ: 0.01
2024-06-16 23:25:17,434 - INFO - Epoch: 2 | Step: 55000 | Train loss: 1.587 | Eval loss: 1.381 | Eval accuracy: 0.392 | ɛ: 0.01
2024-06-16 23:25:26,401 - INFO - Epoch: 2 | Step: 60000 | Train loss: 1.589 | Eval loss: 1.388 | Eval accuracy: 0.396 | ɛ: 0.01
2024-06-16 23:25:35,052 - INFO - Epoch: 2 | Step: 65000 | Train loss: 1.589 | Eval loss: 1.382 | Eval accuracy: 0.399 | ɛ: 0.01
2024-06-16 23:25:43,865 - INFO - Epoch: 2 | Step: 70000 | Train loss: 1.593 | Eval loss: 1.397 | Eval accuracy: 0.397 | ɛ: 0.01
2024-06-16 23:25:51,834 - INFO - Epoch: 2 | Step: 75000 | Train loss: 1.595 | Eval loss: 1.406 | Eval accuracy: 0.394 | ɛ: 0.01
2024-06-16 23:25:59,679 - INFO - Epoch: 2 | Step: 80000 | Train loss: 1.598 | Eval loss: 1.433 | Eval accuracy: 0.395 | ɛ: 0.01
2024-06-16 23:26:07,044 - INFO - Epoch: 2 | Step: 85000 | Train loss: 1.602 | Eval loss: 1.445 | Eval accuracy: 0.399 | ɛ: 0.01
2024-06-16 23:26:14,286 - INFO - Epoch: 2 | Step: 90000 | Train loss: 1.605 | Eval loss: 1.440 | Eval accuracy: 0.396 | ɛ: 0.01
2024-06-16 23:26:21,821 - INFO - Epoch: 2 | Step: 95000 | Train loss: 1.607 | Eval loss: 1.438 | Eval accuracy: 0.396 | ɛ: 0.01
2024-06-16 23:26:29,996 - INFO - Epoch: 2 | Step: 100000 | Train loss: 1.610 | Eval loss: 1.446 | Eval accuracy: 0.397 | ɛ: 0.01
2024-06-16 23:26:38,610 - INFO - Epoch: 2 | Step: 105000 | Train loss: 1.611 | Eval loss: 1.449 | Eval accuracy: 0.401 | ɛ: 0.01
2024-06-16 23:26:47,358 - INFO - Epoch: 2 | Step: 110000 | Train loss: 1.611 | Eval loss: 1.454 | Eval accuracy: 0.397 | ɛ: 0.01
2024-06-16 23:26:55,376 - INFO - Epoch: 2 | Step: 115000 | Train loss: 1.610 | Eval loss: 1.418 | Eval accuracy: 0.398 | ɛ: 0.01
2024-06-16 23:27:03,217 - INFO - Epoch: 2 | Step: 120000 | Train loss: 1.612 | Eval loss: 1.436 | Eval accuracy: 0.398 | ɛ: 0.01
2024-06-16 23:27:11,254 - INFO - Epoch: 2 | Step: 125000 | Train loss: 1.613 | Eval loss: 1.435 | Eval accuracy: 0.395 | ɛ: 0.01
2024-06-16 23:27:18,800 - INFO - Epoch: 2 | Step: 130000 | Train loss: 1.615 | Eval loss: 1.448 | Eval accuracy: 0.401 | ɛ: 0.01
2024-06-16 23:27:26,895 - INFO - Epoch: 2 | Step: 135000 | Train loss: 1.616 | Eval loss: 1.415 | Eval accuracy: 0.395 | ɛ: 0.01
2024-06-16 23:27:34,423 - INFO - Epoch: 2 | Step: 140000 | Train loss: 1.617 | Eval loss: 1.437 | Eval accuracy: 0.397 | ɛ: 0.01
2024-06-16 23:27:43,450 - INFO - Epoch: 2 | Step: 145000 | Train loss: 1.617 | Eval loss: 1.446 | Eval accuracy: 0.399 | ɛ: 0.01
2024-06-16 23:27:51,800 - INFO - Epoch: 2 | Step: 150000 | Train loss: 1.617 | Eval loss: 1.456 | Eval accuracy: 0.405 | ɛ: 0.01
2024-06-16 23:28:00,675 - INFO - Epoch: 2 | Step: 155000 | Train loss: 1.618 | Eval loss: 1.493 | Eval accuracy: 0.404 | ɛ: 0.01
