2024-06-17 14:43:20,907 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,907 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,907 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,910 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,910 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,910 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,913 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,913 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,913 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,916 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,916 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,916 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,919 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,919 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,919 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,922 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,922 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,922 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,925 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,925 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,925 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,928 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,928 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,928 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,931 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,931 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,931 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,934 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,934 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,934 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,937 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,937 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,938 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,940 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,941 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,941 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,945 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,945 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,945 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,949 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,949 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,949 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,952 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,952 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,952 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,956 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,956 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,956 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,960 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,960 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,960 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,963 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,963 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,963 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,967 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,967 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,967 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,970 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,970 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,970 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,973 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,973 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,973 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,976 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,976 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,976 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,980 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,981 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,981 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,984 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,984 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,984 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,987 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,987 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,987 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,991 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,991 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,991 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,994 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,994 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,994 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:20,997 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:20,997 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:20,997 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,000 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,000 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,000 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,004 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,005 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,005 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,008 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,008 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,008 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,011 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,011 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,011 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,015 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,015 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,015 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,018 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,018 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,018 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,021 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,021 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,021 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,024 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,024 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,024 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,029 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,029 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,029 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,032 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,032 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,032 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,035 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,035 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,035 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,038 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,038 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,038 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,041 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,041 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,041 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,044 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,044 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,044 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,047 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,047 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,047 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,050 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,050 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,050 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,053 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,053 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,053 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,056 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,056 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,056 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,059 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,059 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,059 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,062 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,062 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,062 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,065 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,065 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,065 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,068 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,068 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,068 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,071 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,071 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,071 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,074 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,074 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,074 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,077 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,077 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,077 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,080 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,080 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,080 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,083 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,083 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,083 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,086 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,086 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,086 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,089 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,089 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,089 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,092 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,092 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,092 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,095 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,095 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,095 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,098 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,098 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,098 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,101 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,101 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,101 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,104 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,104 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,104 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,107 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,107 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,107 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,110 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,110 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,110 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,113 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,113 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,113 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,116 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,116 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,116 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,119 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,119 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,119 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,122 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,122 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,123 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,125 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,126 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,126 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,129 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,129 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,129 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,132 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,132 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,132 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,137 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,137 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,137 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,140 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,140 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,140 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,144 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,144 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,144 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,149 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,149 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,149 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,152 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,152 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,152 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,155 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,155 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,155 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,160 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,160 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,160 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,164 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,164 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,164 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,170 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,170 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,170 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,176 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,176 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,176 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,181 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,181 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,181 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,188 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,188 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,188 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,192 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,192 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,192 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,197 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,197 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,198 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,201 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,201 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,201 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,204 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,204 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,204 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,208 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,208 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,208 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,211 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,211 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,211 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,214 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,214 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,214 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,217 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,217 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,218 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,223 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,223 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,223 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,226 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,226 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,226 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,231 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,231 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,231 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,234 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,234 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,234 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,237 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,237 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,238 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,241 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,241 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,241 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,244 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,244 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,244 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,247 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,248 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,248 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,251 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,251 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,251 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,254 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,255 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,255 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,258 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,258 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,258 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,262 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,262 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,262 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,265 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,265 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,265 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,268 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,268 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,268 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,271 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,271 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,271 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,274 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,274 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,274 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,277 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,277 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,277 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,281 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,281 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,281 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,284 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,284 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,285 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,288 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,288 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,288 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,291 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,291 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,292 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,295 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,295 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,295 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,298 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,298 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,298 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,301 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,302 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,302 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,305 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,305 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,305 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,308 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,308 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,308 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,311 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,311 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,311 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,314 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,314 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,314 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,317 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,317 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,317 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,320 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,320 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,320 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,323 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,323 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,323 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,326 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,326 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,326 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,329 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,329 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,329 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,332 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,332 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,332 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,335 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,335 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,335 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,338 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,338 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,338 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,344 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,344 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,344 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,347 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,347 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,347 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,352 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,352 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,352 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,355 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,355 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,355 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,358 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,358 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,358 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,363 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,363 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,363 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,366 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,366 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,366 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,369 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,369 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,369 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,374 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,374 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,374 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,377 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,377 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,377 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,380 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,380 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,381 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,384 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,384 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,384 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,388 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,389 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,389 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,392 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,392 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,392 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,396 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,396 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,396 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,399 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,399 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,399 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,404 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,404 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,404 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,407 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,408 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,408 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,411 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,411 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,411 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,416 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,416 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,416 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,419 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,419 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,420 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,422 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,423 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,423 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,428 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,428 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,428 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,431 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,431 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,431 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,434 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,434 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,434 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,439 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,439 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,439 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,442 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,442 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,442 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,445 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,445 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,445 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,448 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,448 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,448 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,451 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,451 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,451 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,454 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,455 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,455 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,457 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,458 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,458 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,460 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,461 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,461 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,463 - INFO - Input embeddings size: torch.Size([32, 128, 128])
2024-06-17 14:43:21,463 - INFO - Inputs embeds after soft prompt size: torch.Size([32, 178, 128])
2024-06-17 14:43:21,463 - INFO - Attention mask size after concat: torch.Size([32, 178])
2024-06-17 14:43:21,464 - INFO - Input embeddings size: torch.Size([23, 128, 128])
2024-06-17 14:43:21,464 - INFO - Inputs embeds after soft prompt size: torch.Size([23, 178, 128])
2024-06-17 14:43:21,464 - INFO - Attention mask size after concat: torch.Size([23, 178])
2024-06-17 14:43:21,479 - INFO - Evaluation results: {'eval_loss': 1.0980849266052246, 'eval_accuracy': 0.34803871625063676, 'eval_runtime': 1.0432, 'eval_samples_per_second': 9408.103, 'eval_steps_per_second': 294.273, 'epoch': 2.0}
2024-06-17 14:43:21,480 - INFO - Results saved to results/evaluation_results.json
2024-06-17 20:49:22,245 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.classifier.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.classifier.weight', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.0.attention.self.key.weight'}
2024-06-17 20:49:22,245 - INFO - Total parameters count: 4392707
2024-06-17 20:49:22,245 - INFO - Trainable parameters count: 6400 (0.1456960366352684%)
2024-06-17 20:49:22,298 - INFO - Unique labels: {0, 1, 2}
2024-06-17 22:30:23,751 - INFO - Total parameters: 4386178 || Trainable parameters: 258 (0.005882114223362572%)
2024-06-17 22:30:31,153 - INFO - Epoch: 1 | Step: 5000 | Train loss: 0.755 | Eval loss: 0.849 | Eval accuracy: 0.510 | ɛ: 0.03
2024-06-17 22:30:39,406 - INFO - Epoch: 1 | Step: 10000 | Train loss: 0.835 | Eval loss: 1.200 | Eval accuracy: 0.509 | ɛ: 0.03
2024-06-17 22:30:47,275 - INFO - Epoch: 1 | Step: 15000 | Train loss: 0.954 | Eval loss: 1.526 | Eval accuracy: 0.509 | ɛ: 0.04
2024-06-17 22:30:54,293 - INFO - Epoch: 1 | Step: 20000 | Train loss: 1.085 | Eval loss: 1.929 | Eval accuracy: 0.509 | ɛ: 0.04
2024-06-17 22:31:01,750 - INFO - Epoch: 1 | Step: 25000 | Train loss: 1.229 | Eval loss: 2.259 | Eval accuracy: 0.509 | ɛ: 0.04
2024-06-17 22:31:08,770 - INFO - Epoch: 1 | Step: 30000 | Train loss: 1.356 | Eval loss: 2.494 | Eval accuracy: 0.509 | ɛ: 0.05
2024-06-17 22:31:16,389 - INFO - Epoch: 1 | Step: 35000 | Train loss: 1.465 | Eval loss: 2.510 | Eval accuracy: 0.509 | ɛ: 0.05
2024-06-17 22:31:24,017 - INFO - Epoch: 1 | Step: 40000 | Train loss: 1.548 | Eval loss: 2.454 | Eval accuracy: 0.509 | ɛ: 0.05
2024-06-17 22:31:31,343 - INFO - Epoch: 1 | Step: 45000 | Train loss: 1.608 | Eval loss: 2.448 | Eval accuracy: 0.509 | ɛ: 0.06
2024-06-17 22:31:38,740 - INFO - Epoch: 1 | Step: 50000 | Train loss: 1.655 | Eval loss: 2.433 | Eval accuracy: 0.509 | ɛ: 0.06
2024-06-17 22:31:45,970 - INFO - Epoch: 1 | Step: 55000 | Train loss: 1.688 | Eval loss: 2.486 | Eval accuracy: 0.509 | ɛ: 0.06
2024-06-17 22:31:53,805 - INFO - Epoch: 1 | Step: 60000 | Train loss: 1.719 | Eval loss: 2.436 | Eval accuracy: 0.509 | ɛ: 0.06
2024-06-17 22:32:01,514 - INFO - Epoch: 1 | Step: 65000 | Train loss: 1.741 | Eval loss: 2.422 | Eval accuracy: 0.509 | ɛ: 0.07
2024-06-17 22:32:14,439 - INFO - Epoch: 2 | Step: 5000 | Train loss: 2.049 | Eval loss: 2.277 | Eval accuracy: 0.509 | ɛ: 0.07
2024-06-17 22:32:21,356 - INFO - Epoch: 2 | Step: 10000 | Train loss: 1.993 | Eval loss: 2.257 | Eval accuracy: 0.509 | ɛ: 0.07
2024-06-17 22:32:29,521 - INFO - Epoch: 2 | Step: 15000 | Train loss: 1.969 | Eval loss: 2.272 | Eval accuracy: 0.509 | ɛ: 0.07
2024-06-17 22:32:37,156 - INFO - Epoch: 2 | Step: 20000 | Train loss: 1.965 | Eval loss: 2.232 | Eval accuracy: 0.509 | ɛ: 0.08
2024-06-17 22:32:45,197 - INFO - Epoch: 2 | Step: 25000 | Train loss: 1.950 | Eval loss: 2.211 | Eval accuracy: 0.509 | ɛ: 0.08
2024-06-17 22:32:52,494 - INFO - Epoch: 2 | Step: 30000 | Train loss: 1.938 | Eval loss: 2.174 | Eval accuracy: 0.509 | ɛ: 0.08
2024-06-17 22:33:00,260 - INFO - Epoch: 2 | Step: 35000 | Train loss: 1.930 | Eval loss: 2.109 | Eval accuracy: 0.509 | ɛ: 0.08
2024-06-17 22:33:07,919 - INFO - Epoch: 2 | Step: 40000 | Train loss: 1.917 | Eval loss: 2.091 | Eval accuracy: 0.509 | ɛ: 0.08
2024-06-17 22:33:15,695 - INFO - Epoch: 2 | Step: 45000 | Train loss: 1.906 | Eval loss: 2.054 | Eval accuracy: 0.509 | ɛ: 0.09
2024-06-17 22:33:23,368 - INFO - Epoch: 2 | Step: 50000 | Train loss: 1.894 | Eval loss: 1.990 | Eval accuracy: 0.509 | ɛ: 0.09
2024-06-17 22:33:30,715 - INFO - Epoch: 2 | Step: 55000 | Train loss: 1.879 | Eval loss: 1.994 | Eval accuracy: 0.509 | ɛ: 0.09
2024-06-17 22:33:38,346 - INFO - Epoch: 2 | Step: 60000 | Train loss: 1.866 | Eval loss: 1.950 | Eval accuracy: 0.509 | ɛ: 0.09
2024-06-17 22:33:45,782 - INFO - Epoch: 2 | Step: 65000 | Train loss: 1.853 | Eval loss: 1.991 | Eval accuracy: 0.509 | ɛ: 0.09
2024-06-17 22:33:54,028 - INFO - Results saved to results/evaluation_results_dp.json
2024-06-17 22:38:14,284 - INFO - Total parameters: 4386307 || Trainable parameters: 387 (0.00882291184816749%)
2024-06-17 22:38:21,484 - INFO - Epoch: 1 | Step: 5000 | Train loss: 1.149 | Eval loss: 1.142 | Eval accuracy: 0.361 | ɛ: 0.02
2024-06-17 22:38:28,638 - INFO - Epoch: 1 | Step: 10000 | Train loss: 1.154 | Eval loss: 1.135 | Eval accuracy: 0.367 | ɛ: 0.02
2024-06-17 22:38:36,782 - INFO - Epoch: 1 | Step: 15000 | Train loss: 1.156 | Eval loss: 1.128 | Eval accuracy: 0.375 | ɛ: 0.02
2024-06-17 22:38:44,910 - INFO - Epoch: 1 | Step: 20000 | Train loss: 1.155 | Eval loss: 1.122 | Eval accuracy: 0.384 | ɛ: 0.02
2024-06-17 22:38:52,051 - INFO - Epoch: 1 | Step: 25000 | Train loss: 1.151 | Eval loss: 1.118 | Eval accuracy: 0.384 | ɛ: 0.03
2024-06-17 22:38:59,375 - INFO - Epoch: 1 | Step: 30000 | Train loss: 1.148 | Eval loss: 1.113 | Eval accuracy: 0.393 | ɛ: 0.03
2024-06-17 22:39:06,759 - INFO - Epoch: 1 | Step: 35000 | Train loss: 1.147 | Eval loss: 1.110 | Eval accuracy: 0.389 | ɛ: 0.03
2024-06-17 22:39:14,262 - INFO - Epoch: 1 | Step: 40000 | Train loss: 1.145 | Eval loss: 1.107 | Eval accuracy: 0.398 | ɛ: 0.03
2024-06-17 22:39:21,864 - INFO - Epoch: 1 | Step: 45000 | Train loss: 1.144 | Eval loss: 1.107 | Eval accuracy: 0.404 | ɛ: 0.03
2024-06-17 22:39:30,143 - INFO - Epoch: 1 | Step: 50000 | Train loss: 1.143 | Eval loss: 1.105 | Eval accuracy: 0.405 | ɛ: 0.03
2024-06-17 22:39:38,845 - INFO - Epoch: 1 | Step: 55000 | Train loss: 1.141 | Eval loss: 1.104 | Eval accuracy: 0.405 | ɛ: 0.03
2024-06-17 22:39:46,532 - INFO - Epoch: 1 | Step: 60000 | Train loss: 1.140 | Eval loss: 1.100 | Eval accuracy: 0.406 | ɛ: 0.03
2024-06-17 22:39:53,890 - INFO - Epoch: 1 | Step: 65000 | Train loss: 1.139 | Eval loss: 1.098 | Eval accuracy: 0.407 | ɛ: 0.03
2024-06-17 22:40:01,121 - INFO - Epoch: 1 | Step: 70000 | Train loss: 1.137 | Eval loss: 1.100 | Eval accuracy: 0.414 | ɛ: 0.04
2024-06-17 22:40:09,810 - INFO - Epoch: 1 | Step: 75000 | Train loss: 1.137 | Eval loss: 1.097 | Eval accuracy: 0.411 | ɛ: 0.04
2024-06-17 22:40:17,930 - INFO - Epoch: 1 | Step: 80000 | Train loss: 1.136 | Eval loss: 1.096 | Eval accuracy: 0.409 | ɛ: 0.04
2024-06-17 22:40:25,389 - INFO - Epoch: 1 | Step: 85000 | Train loss: 1.135 | Eval loss: 1.094 | Eval accuracy: 0.411 | ɛ: 0.04
2024-06-17 22:40:33,740 - INFO - Epoch: 1 | Step: 90000 | Train loss: 1.134 | Eval loss: 1.097 | Eval accuracy: 0.413 | ɛ: 0.04
2024-06-17 22:40:41,532 - INFO - Epoch: 1 | Step: 95000 | Train loss: 1.133 | Eval loss: 1.106 | Eval accuracy: 0.416 | ɛ: 0.04
2024-06-17 22:40:50,992 - INFO - Epoch: 1 | Step: 100000 | Train loss: 1.133 | Eval loss: 1.097 | Eval accuracy: 0.411 | ɛ: 0.04
2024-06-17 22:40:59,906 - INFO - Epoch: 1 | Step: 105000 | Train loss: 1.132 | Eval loss: 1.095 | Eval accuracy: 0.414 | ɛ: 0.04
2024-06-17 22:41:07,778 - INFO - Epoch: 1 | Step: 110000 | Train loss: 1.132 | Eval loss: 1.097 | Eval accuracy: 0.414 | ɛ: 0.04
2024-06-17 22:41:15,916 - INFO - Epoch: 1 | Step: 115000 | Train loss: 1.132 | Eval loss: 1.095 | Eval accuracy: 0.413 | ɛ: 0.04
2024-06-17 22:41:24,087 - INFO - Epoch: 1 | Step: 120000 | Train loss: 1.131 | Eval loss: 1.099 | Eval accuracy: 0.413 | ɛ: 0.04
2024-06-17 22:41:33,242 - INFO - Epoch: 1 | Step: 125000 | Train loss: 1.131 | Eval loss: 1.098 | Eval accuracy: 0.412 | ɛ: 0.04
2024-06-17 22:41:41,189 - INFO - Epoch: 1 | Step: 130000 | Train loss: 1.131 | Eval loss: 1.102 | Eval accuracy: 0.416 | ɛ: 0.04
2024-06-17 22:41:48,315 - INFO - Epoch: 1 | Step: 135000 | Train loss: 1.130 | Eval loss: 1.102 | Eval accuracy: 0.418 | ɛ: 0.04
2024-06-17 22:41:55,416 - INFO - Epoch: 1 | Step: 140000 | Train loss: 1.130 | Eval loss: 1.097 | Eval accuracy: 0.418 | ɛ: 0.05
2024-06-17 22:42:02,739 - INFO - Epoch: 1 | Step: 145000 | Train loss: 1.130 | Eval loss: 1.104 | Eval accuracy: 0.419 | ɛ: 0.05
2024-06-17 22:42:10,069 - INFO - Epoch: 1 | Step: 150000 | Train loss: 1.129 | Eval loss: 1.099 | Eval accuracy: 0.419 | ɛ: 0.05
2024-06-17 22:42:17,223 - INFO - Epoch: 1 | Step: 155000 | Train loss: 1.129 | Eval loss: 1.097 | Eval accuracy: 0.422 | ɛ: 0.05
2024-06-17 22:42:25,597 - INFO - Epoch: 1 | Step: 160000 | Train loss: 1.129 | Eval loss: 1.095 | Eval accuracy: 0.422 | ɛ: 0.05
2024-06-17 22:42:32,876 - INFO - Epoch: 1 | Step: 165000 | Train loss: 1.128 | Eval loss: 1.096 | Eval accuracy: 0.427 | ɛ: 0.05
2024-06-17 22:42:40,618 - INFO - Epoch: 1 | Step: 170000 | Train loss: 1.128 | Eval loss: 1.096 | Eval accuracy: 0.424 | ɛ: 0.05
2024-06-17 22:42:49,044 - INFO - Epoch: 1 | Step: 175000 | Train loss: 1.128 | Eval loss: 1.090 | Eval accuracy: 0.426 | ɛ: 0.05
2024-06-17 22:42:56,692 - INFO - Epoch: 1 | Step: 180000 | Train loss: 1.128 | Eval loss: 1.096 | Eval accuracy: 0.424 | ɛ: 0.05
2024-06-17 22:43:04,008 - INFO - Epoch: 1 | Step: 185000 | Train loss: 1.128 | Eval loss: 1.094 | Eval accuracy: 0.426 | ɛ: 0.05
2024-06-17 22:43:11,444 - INFO - Epoch: 1 | Step: 190000 | Train loss: 1.127 | Eval loss: 1.095 | Eval accuracy: 0.424 | ɛ: 0.05
2024-06-17 22:43:19,223 - INFO - Epoch: 1 | Step: 195000 | Train loss: 1.127 | Eval loss: 1.094 | Eval accuracy: 0.426 | ɛ: 0.05
2024-06-17 22:43:26,485 - INFO - Epoch: 1 | Step: 200000 | Train loss: 1.127 | Eval loss: 1.099 | Eval accuracy: 0.424 | ɛ: 0.05
2024-06-17 22:43:33,755 - INFO - Epoch: 1 | Step: 205000 | Train loss: 1.127 | Eval loss: 1.094 | Eval accuracy: 0.427 | ɛ: 0.05
2024-06-17 22:43:42,095 - INFO - Epoch: 1 | Step: 210000 | Train loss: 1.127 | Eval loss: 1.097 | Eval accuracy: 0.427 | ɛ: 0.05
2024-06-17 22:43:49,750 - INFO - Epoch: 1 | Step: 215000 | Train loss: 1.127 | Eval loss: 1.092 | Eval accuracy: 0.429 | ɛ: 0.05
2024-06-17 22:43:57,649 - INFO - Epoch: 1 | Step: 220000 | Train loss: 1.126 | Eval loss: 1.093 | Eval accuracy: 0.431 | ɛ: 0.05
2024-06-17 22:44:05,273 - INFO - Epoch: 1 | Step: 225000 | Train loss: 1.126 | Eval loss: 1.090 | Eval accuracy: 0.430 | ɛ: 0.05
2024-06-17 22:44:12,414 - INFO - Epoch: 1 | Step: 230000 | Train loss: 1.126 | Eval loss: 1.094 | Eval accuracy: 0.430 | ɛ: 0.05
2024-06-17 22:44:20,089 - INFO - Epoch: 1 | Step: 235000 | Train loss: 1.125 | Eval loss: 1.096 | Eval accuracy: 0.430 | ɛ: 0.06
2024-06-17 22:44:27,937 - INFO - Epoch: 1 | Step: 240000 | Train loss: 1.125 | Eval loss: 1.089 | Eval accuracy: 0.431 | ɛ: 0.06
2024-06-17 22:44:36,014 - INFO - Epoch: 1 | Step: 245000 | Train loss: 1.125 | Eval loss: 1.097 | Eval accuracy: 0.430 | ɛ: 0.06
2024-06-17 22:44:43,200 - INFO - Epoch: 1 | Step: 250000 | Train loss: 1.125 | Eval loss: 1.098 | Eval accuracy: 0.429 | ɛ: 0.06
2024-06-17 22:44:50,540 - INFO - Epoch: 1 | Step: 255000 | Train loss: 1.125 | Eval loss: 1.098 | Eval accuracy: 0.430 | ɛ: 0.06
2024-06-17 22:44:57,901 - INFO - Epoch: 1 | Step: 260000 | Train loss: 1.125 | Eval loss: 1.091 | Eval accuracy: 0.431 | ɛ: 0.06
2024-06-17 22:45:05,815 - INFO - Epoch: 1 | Step: 265000 | Train loss: 1.124 | Eval loss: 1.101 | Eval accuracy: 0.428 | ɛ: 0.06
2024-06-17 22:45:12,793 - INFO - Epoch: 1 | Step: 270000 | Train loss: 1.124 | Eval loss: 1.092 | Eval accuracy: 0.431 | ɛ: 0.06
2024-06-17 22:45:20,207 - INFO - Epoch: 1 | Step: 275000 | Train loss: 1.124 | Eval loss: 1.097 | Eval accuracy: 0.430 | ɛ: 0.06
2024-06-17 22:45:27,810 - INFO - Epoch: 1 | Step: 280000 | Train loss: 1.124 | Eval loss: 1.093 | Eval accuracy: 0.430 | ɛ: 0.06
2024-06-17 22:45:35,569 - INFO - Epoch: 1 | Step: 285000 | Train loss: 1.124 | Eval loss: 1.096 | Eval accuracy: 0.431 | ɛ: 0.06
2024-06-17 22:45:43,634 - INFO - Epoch: 1 | Step: 290000 | Train loss: 1.124 | Eval loss: 1.094 | Eval accuracy: 0.431 | ɛ: 0.06
2024-06-17 22:45:52,689 - INFO - Epoch: 1 | Step: 295000 | Train loss: 1.124 | Eval loss: 1.098 | Eval accuracy: 0.430 | ɛ: 0.06
2024-06-17 22:46:00,438 - INFO - Epoch: 1 | Step: 300000 | Train loss: 1.124 | Eval loss: 1.094 | Eval accuracy: 0.433 | ɛ: 0.06
2024-06-17 22:46:07,429 - INFO - Epoch: 1 | Step: 305000 | Train loss: 1.124 | Eval loss: 1.095 | Eval accuracy: 0.432 | ɛ: 0.06
2024-06-17 22:46:14,973 - INFO - Epoch: 1 | Step: 310000 | Train loss: 1.124 | Eval loss: 1.098 | Eval accuracy: 0.432 | ɛ: 0.06
2024-06-17 22:46:22,218 - INFO - Epoch: 1 | Step: 315000 | Train loss: 1.123 | Eval loss: 1.100 | Eval accuracy: 0.430 | ɛ: 0.06
2024-06-17 22:46:29,575 - INFO - Epoch: 1 | Step: 320000 | Train loss: 1.123 | Eval loss: 1.093 | Eval accuracy: 0.434 | ɛ: 0.06
2024-06-17 22:46:36,583 - INFO - Epoch: 1 | Step: 325000 | Train loss: 1.123 | Eval loss: 1.092 | Eval accuracy: 0.435 | ɛ: 0.06
2024-06-17 22:46:43,893 - INFO - Epoch: 1 | Step: 330000 | Train loss: 1.123 | Eval loss: 1.093 | Eval accuracy: 0.434 | ɛ: 0.06
2024-06-17 22:46:51,019 - INFO - Epoch: 1 | Step: 335000 | Train loss: 1.123 | Eval loss: 1.097 | Eval accuracy: 0.433 | ɛ: 0.06
2024-06-17 22:46:58,333 - INFO - Epoch: 1 | Step: 340000 | Train loss: 1.123 | Eval loss: 1.097 | Eval accuracy: 0.433 | ɛ: 0.06
2024-06-17 22:47:05,730 - INFO - Epoch: 1 | Step: 345000 | Train loss: 1.123 | Eval loss: 1.094 | Eval accuracy: 0.434 | ɛ: 0.06
2024-06-17 22:47:13,358 - INFO - Epoch: 1 | Step: 350000 | Train loss: 1.123 | Eval loss: 1.095 | Eval accuracy: 0.433 | ɛ: 0.07
2024-06-17 22:47:20,695 - INFO - Epoch: 1 | Step: 355000 | Train loss: 1.123 | Eval loss: 1.095 | Eval accuracy: 0.434 | ɛ: 0.07
2024-06-17 22:47:28,263 - INFO - Epoch: 1 | Step: 360000 | Train loss: 1.123 | Eval loss: 1.097 | Eval accuracy: 0.434 | ɛ: 0.07
2024-06-17 22:47:36,250 - INFO - Epoch: 1 | Step: 365000 | Train loss: 1.122 | Eval loss: 1.092 | Eval accuracy: 0.436 | ɛ: 0.07
2024-06-17 22:47:45,019 - INFO - Epoch: 1 | Step: 370000 | Train loss: 1.123 | Eval loss: 1.098 | Eval accuracy: 0.434 | ɛ: 0.07
2024-06-17 22:47:52,365 - INFO - Epoch: 1 | Step: 375000 | Train loss: 1.122 | Eval loss: 1.110 | Eval accuracy: 0.433 | ɛ: 0.07
2024-06-17 22:47:59,903 - INFO - Epoch: 1 | Step: 380000 | Train loss: 1.122 | Eval loss: 1.102 | Eval accuracy: 0.432 | ɛ: 0.07
2024-06-17 22:48:08,224 - INFO - Epoch: 1 | Step: 385000 | Train loss: 1.122 | Eval loss: 1.113 | Eval accuracy: 0.431 | ɛ: 0.07
2024-06-17 22:48:17,319 - INFO - Epoch: 1 | Step: 390000 | Train loss: 1.122 | Eval loss: 1.103 | Eval accuracy: 0.434 | ɛ: 0.07
2024-06-17 22:49:14,619 - INFO - Epoch: 2 | Step: 5000 | Train loss: 1.124 | Eval loss: 1.104 | Eval accuracy: 0.434 | ɛ: 0.07
2024-06-17 22:49:21,839 - INFO - Epoch: 2 | Step: 10000 | Train loss: 1.119 | Eval loss: 1.108 | Eval accuracy: 0.434 | ɛ: 0.07
2024-06-17 22:49:29,062 - INFO - Epoch: 2 | Step: 15000 | Train loss: 1.121 | Eval loss: 1.118 | Eval accuracy: 0.432 | ɛ: 0.07
2024-06-17 22:49:36,866 - INFO - Epoch: 2 | Step: 20000 | Train loss: 1.125 | Eval loss: 1.097 | Eval accuracy: 0.433 | ɛ: 0.07
2024-06-17 22:49:43,854 - INFO - Epoch: 2 | Step: 25000 | Train loss: 1.121 | Eval loss: 1.099 | Eval accuracy: 0.436 | ɛ: 0.07
2024-06-17 22:49:51,191 - INFO - Epoch: 2 | Step: 30000 | Train loss: 1.117 | Eval loss: 1.094 | Eval accuracy: 0.437 | ɛ: 0.07
2024-06-17 22:49:59,182 - INFO - Epoch: 2 | Step: 35000 | Train loss: 1.118 | Eval loss: 1.104 | Eval accuracy: 0.434 | ɛ: 0.07
2024-06-17 22:50:06,770 - INFO - Epoch: 2 | Step: 40000 | Train loss: 1.117 | Eval loss: 1.108 | Eval accuracy: 0.433 | ɛ: 0.07
2024-06-17 22:50:13,982 - INFO - Epoch: 2 | Step: 45000 | Train loss: 1.116 | Eval loss: 1.112 | Eval accuracy: 0.435 | ɛ: 0.07
2024-06-17 22:50:21,637 - INFO - Epoch: 2 | Step: 50000 | Train loss: 1.117 | Eval loss: 1.106 | Eval accuracy: 0.434 | ɛ: 0.07
2024-06-17 22:50:29,231 - INFO - Epoch: 2 | Step: 55000 | Train loss: 1.118 | Eval loss: 1.102 | Eval accuracy: 0.436 | ɛ: 0.07
2024-06-17 22:50:36,385 - INFO - Epoch: 2 | Step: 60000 | Train loss: 1.118 | Eval loss: 1.110 | Eval accuracy: 0.435 | ɛ: 0.07
2024-06-17 22:50:43,493 - INFO - Epoch: 2 | Step: 65000 | Train loss: 1.117 | Eval loss: 1.112 | Eval accuracy: 0.436 | ɛ: 0.07
2024-06-17 22:50:50,897 - INFO - Epoch: 2 | Step: 70000 | Train loss: 1.119 | Eval loss: 1.103 | Eval accuracy: 0.436 | ɛ: 0.07
2024-06-17 22:50:58,305 - INFO - Epoch: 2 | Step: 75000 | Train loss: 1.119 | Eval loss: 1.103 | Eval accuracy: 0.436 | ɛ: 0.07
2024-06-17 22:51:05,325 - INFO - Epoch: 2 | Step: 80000 | Train loss: 1.119 | Eval loss: 1.109 | Eval accuracy: 0.436 | ɛ: 0.07
2024-06-17 22:51:13,038 - INFO - Epoch: 2 | Step: 85000 | Train loss: 1.120 | Eval loss: 1.095 | Eval accuracy: 0.438 | ɛ: 0.07
2024-06-17 22:51:20,171 - INFO - Epoch: 2 | Step: 90000 | Train loss: 1.120 | Eval loss: 1.109 | Eval accuracy: 0.438 | ɛ: 0.08
2024-06-17 22:51:28,310 - INFO - Epoch: 2 | Step: 95000 | Train loss: 1.120 | Eval loss: 1.102 | Eval accuracy: 0.438 | ɛ: 0.08
2024-06-17 22:51:35,219 - INFO - Epoch: 2 | Step: 100000 | Train loss: 1.120 | Eval loss: 1.109 | Eval accuracy: 0.438 | ɛ: 0.08
2024-06-17 22:51:42,269 - INFO - Epoch: 2 | Step: 105000 | Train loss: 1.120 | Eval loss: 1.098 | Eval accuracy: 0.438 | ɛ: 0.08
2024-06-17 22:51:49,220 - INFO - Epoch: 2 | Step: 110000 | Train loss: 1.120 | Eval loss: 1.105 | Eval accuracy: 0.437 | ɛ: 0.08
2024-06-17 22:51:56,348 - INFO - Epoch: 2 | Step: 115000 | Train loss: 1.119 | Eval loss: 1.104 | Eval accuracy: 0.438 | ɛ: 0.08
2024-06-17 22:52:03,351 - INFO - Epoch: 2 | Step: 120000 | Train loss: 1.119 | Eval loss: 1.094 | Eval accuracy: 0.442 | ɛ: 0.08
2024-06-17 22:52:11,276 - INFO - Epoch: 2 | Step: 125000 | Train loss: 1.119 | Eval loss: 1.095 | Eval accuracy: 0.442 | ɛ: 0.08
2024-06-17 22:52:18,955 - INFO - Epoch: 2 | Step: 130000 | Train loss: 1.119 | Eval loss: 1.099 | Eval accuracy: 0.440 | ɛ: 0.08
2024-06-17 22:52:26,518 - INFO - Epoch: 2 | Step: 135000 | Train loss: 1.119 | Eval loss: 1.101 | Eval accuracy: 0.439 | ɛ: 0.08
2024-06-17 22:52:33,923 - INFO - Epoch: 2 | Step: 140000 | Train loss: 1.119 | Eval loss: 1.101 | Eval accuracy: 0.441 | ɛ: 0.08
2024-06-17 22:52:40,971 - INFO - Epoch: 2 | Step: 145000 | Train loss: 1.119 | Eval loss: 1.097 | Eval accuracy: 0.442 | ɛ: 0.08
2024-06-17 22:52:49,168 - INFO - Epoch: 2 | Step: 150000 | Train loss: 1.119 | Eval loss: 1.095 | Eval accuracy: 0.442 | ɛ: 0.08
2024-06-17 22:52:56,320 - INFO - Epoch: 2 | Step: 155000 | Train loss: 1.119 | Eval loss: 1.097 | Eval accuracy: 0.443 | ɛ: 0.08
2024-06-17 22:53:05,987 - INFO - Epoch: 2 | Step: 160000 | Train loss: 1.119 | Eval loss: 1.100 | Eval accuracy: 0.440 | ɛ: 0.08
2024-06-17 22:53:13,756 - INFO - Epoch: 2 | Step: 165000 | Train loss: 1.119 | Eval loss: 1.111 | Eval accuracy: 0.439 | ɛ: 0.08
2024-06-17 22:53:21,777 - INFO - Epoch: 2 | Step: 170000 | Train loss: 1.119 | Eval loss: 1.111 | Eval accuracy: 0.438 | ɛ: 0.08
2024-06-17 22:53:29,163 - INFO - Epoch: 2 | Step: 175000 | Train loss: 1.119 | Eval loss: 1.104 | Eval accuracy: 0.441 | ɛ: 0.08
2024-06-17 22:53:36,482 - INFO - Epoch: 2 | Step: 180000 | Train loss: 1.120 | Eval loss: 1.101 | Eval accuracy: 0.443 | ɛ: 0.08
2024-06-17 22:53:43,676 - INFO - Epoch: 2 | Step: 185000 | Train loss: 1.120 | Eval loss: 1.100 | Eval accuracy: 0.442 | ɛ: 0.08
2024-06-17 22:53:51,054 - INFO - Epoch: 2 | Step: 190000 | Train loss: 1.120 | Eval loss: 1.105 | Eval accuracy: 0.439 | ɛ: 0.08
2024-06-17 22:53:58,801 - INFO - Epoch: 2 | Step: 195000 | Train loss: 1.120 | Eval loss: 1.107 | Eval accuracy: 0.440 | ɛ: 0.08
2024-06-17 22:54:05,966 - INFO - Epoch: 2 | Step: 200000 | Train loss: 1.120 | Eval loss: 1.112 | Eval accuracy: 0.440 | ɛ: 0.08
2024-06-17 22:54:14,398 - INFO - Epoch: 2 | Step: 205000 | Train loss: 1.120 | Eval loss: 1.122 | Eval accuracy: 0.438 | ɛ: 0.08
2024-06-17 22:54:23,343 - INFO - Epoch: 2 | Step: 210000 | Train loss: 1.120 | Eval loss: 1.127 | Eval accuracy: 0.437 | ɛ: 0.08
2024-06-17 22:54:31,223 - INFO - Epoch: 2 | Step: 215000 | Train loss: 1.120 | Eval loss: 1.110 | Eval accuracy: 0.440 | ɛ: 0.08
2024-06-17 22:54:40,097 - INFO - Epoch: 2 | Step: 220000 | Train loss: 1.120 | Eval loss: 1.110 | Eval accuracy: 0.438 | ɛ: 0.08
2024-06-17 22:54:47,321 - INFO - Epoch: 2 | Step: 225000 | Train loss: 1.121 | Eval loss: 1.113 | Eval accuracy: 0.440 | ɛ: 0.08
2024-06-17 22:54:54,467 - INFO - Epoch: 2 | Step: 230000 | Train loss: 1.121 | Eval loss: 1.110 | Eval accuracy: 0.442 | ɛ: 0.08
2024-06-17 22:55:01,439 - INFO - Epoch: 2 | Step: 235000 | Train loss: 1.121 | Eval loss: 1.109 | Eval accuracy: 0.441 | ɛ: 0.08
2024-06-17 22:55:08,962 - INFO - Epoch: 2 | Step: 240000 | Train loss: 1.121 | Eval loss: 1.108 | Eval accuracy: 0.441 | ɛ: 0.08
2024-06-17 22:55:16,581 - INFO - Epoch: 2 | Step: 245000 | Train loss: 1.121 | Eval loss: 1.099 | Eval accuracy: 0.443 | ɛ: 0.08
2024-06-17 22:55:24,397 - INFO - Epoch: 2 | Step: 250000 | Train loss: 1.121 | Eval loss: 1.100 | Eval accuracy: 0.443 | ɛ: 0.09
2024-06-17 22:55:32,249 - INFO - Epoch: 2 | Step: 255000 | Train loss: 1.122 | Eval loss: 1.097 | Eval accuracy: 0.444 | ɛ: 0.09
2024-06-17 22:55:40,909 - INFO - Epoch: 2 | Step: 260000 | Train loss: 1.122 | Eval loss: 1.102 | Eval accuracy: 0.441 | ɛ: 0.09
2024-06-17 22:55:50,228 - INFO - Epoch: 2 | Step: 265000 | Train loss: 1.121 | Eval loss: 1.103 | Eval accuracy: 0.441 | ɛ: 0.09
2024-06-17 22:55:58,310 - INFO - Epoch: 2 | Step: 270000 | Train loss: 1.122 | Eval loss: 1.114 | Eval accuracy: 0.441 | ɛ: 0.09
2024-06-17 22:56:07,008 - INFO - Epoch: 2 | Step: 275000 | Train loss: 1.122 | Eval loss: 1.109 | Eval accuracy: 0.441 | ɛ: 0.09
2024-06-17 22:56:14,855 - INFO - Epoch: 2 | Step: 280000 | Train loss: 1.122 | Eval loss: 1.112 | Eval accuracy: 0.441 | ɛ: 0.09
2024-06-17 22:56:22,472 - INFO - Epoch: 2 | Step: 285000 | Train loss: 1.122 | Eval loss: 1.108 | Eval accuracy: 0.441 | ɛ: 0.09
2024-06-17 22:56:30,414 - INFO - Epoch: 2 | Step: 290000 | Train loss: 1.123 | Eval loss: 1.101 | Eval accuracy: 0.444 | ɛ: 0.09
2024-06-17 22:56:38,660 - INFO - Epoch: 2 | Step: 295000 | Train loss: 1.123 | Eval loss: 1.101 | Eval accuracy: 0.444 | ɛ: 0.09
2024-06-17 22:56:46,473 - INFO - Epoch: 2 | Step: 300000 | Train loss: 1.123 | Eval loss: 1.098 | Eval accuracy: 0.445 | ɛ: 0.09
2024-06-17 22:56:54,240 - INFO - Epoch: 2 | Step: 305000 | Train loss: 1.123 | Eval loss: 1.100 | Eval accuracy: 0.444 | ɛ: 0.09
2024-06-17 22:57:02,043 - INFO - Epoch: 2 | Step: 310000 | Train loss: 1.122 | Eval loss: 1.104 | Eval accuracy: 0.443 | ɛ: 0.09
2024-06-17 22:57:09,781 - INFO - Epoch: 2 | Step: 315000 | Train loss: 1.122 | Eval loss: 1.108 | Eval accuracy: 0.442 | ɛ: 0.09
2024-06-17 22:57:18,723 - INFO - Epoch: 2 | Step: 320000 | Train loss: 1.122 | Eval loss: 1.104 | Eval accuracy: 0.444 | ɛ: 0.09
2024-06-17 22:57:26,573 - INFO - Epoch: 2 | Step: 325000 | Train loss: 1.123 | Eval loss: 1.103 | Eval accuracy: 0.443 | ɛ: 0.09
2024-06-17 22:57:34,440 - INFO - Epoch: 2 | Step: 330000 | Train loss: 1.123 | Eval loss: 1.104 | Eval accuracy: 0.444 | ɛ: 0.09
2024-06-17 22:57:42,246 - INFO - Epoch: 2 | Step: 335000 | Train loss: 1.123 | Eval loss: 1.102 | Eval accuracy: 0.444 | ɛ: 0.09
2024-06-17 22:57:49,836 - INFO - Epoch: 2 | Step: 340000 | Train loss: 1.122 | Eval loss: 1.104 | Eval accuracy: 0.444 | ɛ: 0.09
2024-06-17 22:57:57,161 - INFO - Epoch: 2 | Step: 345000 | Train loss: 1.122 | Eval loss: 1.094 | Eval accuracy: 0.447 | ɛ: 0.09
2024-06-17 22:58:04,814 - INFO - Epoch: 2 | Step: 350000 | Train loss: 1.122 | Eval loss: 1.101 | Eval accuracy: 0.446 | ɛ: 0.09
2024-06-17 22:58:12,379 - INFO - Epoch: 2 | Step: 355000 | Train loss: 1.123 | Eval loss: 1.102 | Eval accuracy: 0.446 | ɛ: 0.09
2024-06-17 22:58:19,968 - INFO - Epoch: 2 | Step: 360000 | Train loss: 1.123 | Eval loss: 1.098 | Eval accuracy: 0.447 | ɛ: 0.09
2024-06-17 22:58:27,631 - INFO - Epoch: 2 | Step: 365000 | Train loss: 1.123 | Eval loss: 1.104 | Eval accuracy: 0.445 | ɛ: 0.09
2024-06-17 22:58:37,888 - INFO - Epoch: 2 | Step: 370000 | Train loss: 1.123 | Eval loss: 1.107 | Eval accuracy: 0.443 | ɛ: 0.09
2024-06-17 22:58:46,238 - INFO - Epoch: 2 | Step: 375000 | Train loss: 1.123 | Eval loss: 1.102 | Eval accuracy: 0.444 | ɛ: 0.09
2024-06-17 22:58:54,341 - INFO - Epoch: 2 | Step: 380000 | Train loss: 1.123 | Eval loss: 1.111 | Eval accuracy: 0.444 | ɛ: 0.09
2024-06-17 22:59:02,478 - INFO - Epoch: 2 | Step: 385000 | Train loss: 1.123 | Eval loss: 1.101 | Eval accuracy: 0.446 | ɛ: 0.09
2024-06-17 22:59:09,985 - INFO - Epoch: 2 | Step: 390000 | Train loss: 1.123 | Eval loss: 1.094 | Eval accuracy: 0.448 | ɛ: 0.09
2024-06-17 22:59:57,649 - INFO - Results saved to results/evaluation_results_dp.json
2024-06-17 23:00:04,404 - INFO - Total parameters: 4386178 || Trainable parameters: 258 (0.005882114223362572%)
2024-06-17 23:00:12,734 - INFO - Epoch: 1 | Step: 5000 | Train loss: 0.894 | Eval loss: 0.951 | Eval accuracy: 0.500 | ɛ: 0.02
2024-06-17 23:00:20,796 - INFO - Epoch: 1 | Step: 10000 | Train loss: 0.895 | Eval loss: 1.032 | Eval accuracy: 0.499 | ɛ: 0.03
2024-06-17 23:00:29,148 - INFO - Epoch: 1 | Step: 15000 | Train loss: 0.917 | Eval loss: 1.069 | Eval accuracy: 0.500 | ɛ: 0.03
2024-06-17 23:00:37,973 - INFO - Epoch: 1 | Step: 20000 | Train loss: 0.926 | Eval loss: 1.053 | Eval accuracy: 0.504 | ɛ: 0.04
2024-06-17 23:00:45,661 - INFO - Epoch: 1 | Step: 25000 | Train loss: 0.931 | Eval loss: 1.157 | Eval accuracy: 0.502 | ɛ: 0.04
2024-06-17 23:00:53,497 - INFO - Epoch: 1 | Step: 30000 | Train loss: 0.941 | Eval loss: 1.207 | Eval accuracy: 0.504 | ɛ: 0.04
2024-06-17 23:01:01,032 - INFO - Epoch: 1 | Step: 35000 | Train loss: 0.957 | Eval loss: 1.325 | Eval accuracy: 0.503 | ɛ: 0.04
2024-06-17 23:01:08,859 - INFO - Epoch: 1 | Step: 40000 | Train loss: 0.979 | Eval loss: 1.297 | Eval accuracy: 0.508 | ɛ: 0.05
2024-06-17 23:01:17,106 - INFO - Epoch: 1 | Step: 45000 | Train loss: 0.993 | Eval loss: 1.335 | Eval accuracy: 0.510 | ɛ: 0.05
2024-06-17 23:01:25,121 - INFO - Epoch: 1 | Step: 50000 | Train loss: 1.004 | Eval loss: 1.352 | Eval accuracy: 0.513 | ɛ: 0.05
2024-06-17 23:01:33,105 - INFO - Epoch: 1 | Step: 55000 | Train loss: 1.013 | Eval loss: 1.385 | Eval accuracy: 0.516 | ɛ: 0.05
2024-06-17 23:01:40,593 - INFO - Epoch: 1 | Step: 60000 | Train loss: 1.022 | Eval loss: 1.388 | Eval accuracy: 0.520 | ɛ: 0.06
2024-06-17 23:01:48,985 - INFO - Epoch: 1 | Step: 65000 | Train loss: 1.029 | Eval loss: 1.401 | Eval accuracy: 0.526 | ɛ: 0.06
2024-06-17 23:01:56,406 - INFO - Epoch: 1 | Step: 70000 | Train loss: 1.035 | Eval loss: 1.302 | Eval accuracy: 0.542 | ɛ: 0.06
2024-06-17 23:02:04,094 - INFO - Epoch: 1 | Step: 75000 | Train loss: 1.035 | Eval loss: 1.345 | Eval accuracy: 0.542 | ɛ: 0.06
2024-06-17 23:02:12,298 - INFO - Epoch: 1 | Step: 80000 | Train loss: 1.037 | Eval loss: 1.322 | Eval accuracy: 0.550 | ɛ: 0.06
2024-06-17 23:02:19,589 - INFO - Epoch: 1 | Step: 85000 | Train loss: 1.036 | Eval loss: 1.311 | Eval accuracy: 0.558 | ɛ: 0.07
2024-06-17 23:02:27,003 - INFO - Epoch: 1 | Step: 90000 | Train loss: 1.037 | Eval loss: 1.233 | Eval accuracy: 0.568 | ɛ: 0.07
2024-06-17 23:02:34,919 - INFO - Epoch: 1 | Step: 95000 | Train loss: 1.036 | Eval loss: 1.270 | Eval accuracy: 0.570 | ɛ: 0.07
2024-06-17 23:02:42,638 - INFO - Epoch: 1 | Step: 100000 | Train loss: 1.034 | Eval loss: 1.215 | Eval accuracy: 0.582 | ɛ: 0.07
2024-06-17 23:03:01,176 - INFO - Epoch: 2 | Step: 5000 | Train loss: 1.002 | Eval loss: 1.229 | Eval accuracy: 0.592 | ɛ: 0.07
2024-06-17 23:03:09,191 - INFO - Epoch: 2 | Step: 10000 | Train loss: 0.999 | Eval loss: 1.244 | Eval accuracy: 0.592 | ɛ: 0.07
2024-06-17 23:03:16,503 - INFO - Epoch: 2 | Step: 15000 | Train loss: 1.016 | Eval loss: 1.241 | Eval accuracy: 0.597 | ɛ: 0.08
2024-06-17 23:03:24,681 - INFO - Epoch: 2 | Step: 20000 | Train loss: 1.031 | Eval loss: 1.211 | Eval accuracy: 0.604 | ɛ: 0.08
2024-06-17 23:03:33,708 - INFO - Epoch: 2 | Step: 25000 | Train loss: 1.045 | Eval loss: 1.215 | Eval accuracy: 0.606 | ɛ: 0.08
2024-06-17 23:03:42,045 - INFO - Epoch: 2 | Step: 30000 | Train loss: 1.051 | Eval loss: 1.197 | Eval accuracy: 0.609 | ɛ: 0.08
2024-06-17 23:03:49,343 - INFO - Epoch: 2 | Step: 35000 | Train loss: 1.052 | Eval loss: 1.205 | Eval accuracy: 0.611 | ɛ: 0.08
2024-06-17 23:03:57,853 - INFO - Epoch: 2 | Step: 40000 | Train loss: 1.059 | Eval loss: 1.215 | Eval accuracy: 0.611 | ɛ: 0.08
2024-06-17 23:04:05,892 - INFO - Epoch: 2 | Step: 45000 | Train loss: 1.068 | Eval loss: 1.220 | Eval accuracy: 0.612 | ɛ: 0.08
2024-06-17 23:04:15,446 - INFO - Epoch: 2 | Step: 50000 | Train loss: 1.068 | Eval loss: 1.216 | Eval accuracy: 0.616 | ɛ: 0.09
2024-06-17 23:04:23,765 - INFO - Epoch: 2 | Step: 55000 | Train loss: 1.071 | Eval loss: 1.202 | Eval accuracy: 0.621 | ɛ: 0.09
2024-06-17 23:04:31,949 - INFO - Epoch: 2 | Step: 60000 | Train loss: 1.077 | Eval loss: 1.196 | Eval accuracy: 0.623 | ɛ: 0.09
2024-06-17 23:04:40,339 - INFO - Epoch: 2 | Step: 65000 | Train loss: 1.081 | Eval loss: 1.172 | Eval accuracy: 0.630 | ɛ: 0.09
2024-06-17 23:04:49,976 - INFO - Epoch: 2 | Step: 70000 | Train loss: 1.089 | Eval loss: 1.190 | Eval accuracy: 0.630 | ɛ: 0.09
2024-06-17 23:04:59,185 - INFO - Epoch: 2 | Step: 75000 | Train loss: 1.091 | Eval loss: 1.203 | Eval accuracy: 0.630 | ɛ: 0.09
2024-06-17 23:05:07,409 - INFO - Epoch: 2 | Step: 80000 | Train loss: 1.096 | Eval loss: 1.238 | Eval accuracy: 0.624 | ɛ: 0.09
2024-06-17 23:05:16,567 - INFO - Epoch: 2 | Step: 85000 | Train loss: 1.104 | Eval loss: 1.236 | Eval accuracy: 0.627 | ɛ: 0.09
2024-06-17 23:05:26,381 - INFO - Epoch: 2 | Step: 90000 | Train loss: 1.108 | Eval loss: 1.247 | Eval accuracy: 0.627 | ɛ: 0.10
2024-06-17 23:05:34,172 - INFO - Epoch: 2 | Step: 95000 | Train loss: 1.114 | Eval loss: 1.233 | Eval accuracy: 0.629 | ɛ: 0.10
2024-06-17 23:05:41,980 - INFO - Epoch: 2 | Step: 100000 | Train loss: 1.120 | Eval loss: 1.242 | Eval accuracy: 0.630 | ɛ: 0.10
2024-06-17 23:05:50,096 - INFO - Epoch: 2 | Step: 105000 | Train loss: 1.125 | Eval loss: 1.272 | Eval accuracy: 0.626 | ɛ: 0.10
2024-06-17 23:05:54,586 - INFO - Results saved to results/evaluation_results_dp.json
2024-06-17 23:09:04,178 - INFO - Total parameters: 4394886 || Trainable parameters: 8579 (0.19520415319077672%)
2024-06-17 23:09:22,425 - INFO - Epoch: 1 | Step: 5000 | Train loss: 1.104 | Eval loss: 1.102 | Eval accuracy: 0.322 | ɛ: 0.02
2024-06-17 23:09:41,484 - INFO - Epoch: 1 | Step: 10000 | Train loss: 1.100 | Eval loss: 1.096 | Eval accuracy: 0.377 | ɛ: 0.02
2024-06-17 23:10:02,411 - INFO - Epoch: 1 | Step: 15000 | Train loss: 1.098 | Eval loss: 1.091 | Eval accuracy: 0.385 | ɛ: 0.02
2024-06-17 23:10:22,044 - INFO - Epoch: 1 | Step: 20000 | Train loss: 1.096 | Eval loss: 1.089 | Eval accuracy: 0.385 | ɛ: 0.02
2024-06-17 23:10:41,564 - INFO - Epoch: 1 | Step: 25000 | Train loss: 1.095 | Eval loss: 1.087 | Eval accuracy: 0.388 | ɛ: 0.03
2024-06-17 23:11:01,193 - INFO - Epoch: 1 | Step: 30000 | Train loss: 1.093 | Eval loss: 1.086 | Eval accuracy: 0.387 | ɛ: 0.03
2024-06-17 23:11:21,406 - INFO - Epoch: 1 | Step: 35000 | Train loss: 1.093 | Eval loss: 1.084 | Eval accuracy: 0.395 | ɛ: 0.03
2024-06-17 23:11:41,217 - INFO - Epoch: 1 | Step: 40000 | Train loss: 1.091 | Eval loss: 1.084 | Eval accuracy: 0.396 | ɛ: 0.03
2024-06-17 23:12:02,554 - INFO - Epoch: 1 | Step: 45000 | Train loss: 1.090 | Eval loss: 1.083 | Eval accuracy: 0.402 | ɛ: 0.03
2024-06-17 23:12:22,908 - INFO - Epoch: 1 | Step: 50000 | Train loss: 1.090 | Eval loss: 1.081 | Eval accuracy: 0.404 | ɛ: 0.03
2024-06-17 23:12:42,533 - INFO - Epoch: 1 | Step: 55000 | Train loss: 1.090 | Eval loss: 1.081 | Eval accuracy: 0.406 | ɛ: 0.03
2024-06-17 23:13:02,164 - INFO - Epoch: 1 | Step: 60000 | Train loss: 1.090 | Eval loss: 1.080 | Eval accuracy: 0.408 | ɛ: 0.03
2024-06-17 23:13:21,364 - INFO - Epoch: 1 | Step: 65000 | Train loss: 1.089 | Eval loss: 1.081 | Eval accuracy: 0.401 | ɛ: 0.03
2024-06-17 23:13:40,744 - INFO - Epoch: 1 | Step: 70000 | Train loss: 1.089 | Eval loss: 1.080 | Eval accuracy: 0.404 | ɛ: 0.04
2024-06-17 23:14:00,087 - INFO - Epoch: 1 | Step: 75000 | Train loss: 1.088 | Eval loss: 1.080 | Eval accuracy: 0.404 | ɛ: 0.04
2024-06-17 23:14:20,641 - INFO - Epoch: 1 | Step: 80000 | Train loss: 1.088 | Eval loss: 1.080 | Eval accuracy: 0.408 | ɛ: 0.04
2024-06-17 23:14:41,201 - INFO - Epoch: 1 | Step: 85000 | Train loss: 1.088 | Eval loss: 1.079 | Eval accuracy: 0.408 | ɛ: 0.04
2024-06-17 23:15:01,134 - INFO - Epoch: 1 | Step: 90000 | Train loss: 1.088 | Eval loss: 1.078 | Eval accuracy: 0.408 | ɛ: 0.04
2024-06-17 23:15:21,382 - INFO - Epoch: 1 | Step: 95000 | Train loss: 1.087 | Eval loss: 1.079 | Eval accuracy: 0.416 | ɛ: 0.04
2024-06-17 23:15:41,749 - INFO - Epoch: 1 | Step: 100000 | Train loss: 1.086 | Eval loss: 1.080 | Eval accuracy: 0.416 | ɛ: 0.04
2024-06-17 23:16:01,917 - INFO - Epoch: 1 | Step: 105000 | Train loss: 1.087 | Eval loss: 1.081 | Eval accuracy: 0.415 | ɛ: 0.04
2024-06-17 23:16:22,517 - INFO - Epoch: 1 | Step: 110000 | Train loss: 1.086 | Eval loss: 1.081 | Eval accuracy: 0.415 | ɛ: 0.04
2024-06-17 23:16:43,588 - INFO - Epoch: 1 | Step: 115000 | Train loss: 1.086 | Eval loss: 1.083 | Eval accuracy: 0.417 | ɛ: 0.04
2024-06-17 23:17:04,311 - INFO - Epoch: 1 | Step: 120000 | Train loss: 1.086 | Eval loss: 1.082 | Eval accuracy: 0.420 | ɛ: 0.04
2024-06-17 23:17:25,660 - INFO - Epoch: 1 | Step: 125000 | Train loss: 1.086 | Eval loss: 1.081 | Eval accuracy: 0.415 | ɛ: 0.04
2024-06-17 23:17:46,856 - INFO - Epoch: 1 | Step: 130000 | Train loss: 1.086 | Eval loss: 1.082 | Eval accuracy: 0.416 | ɛ: 0.04
2024-06-17 23:18:08,087 - INFO - Epoch: 1 | Step: 135000 | Train loss: 1.086 | Eval loss: 1.082 | Eval accuracy: 0.418 | ɛ: 0.04
2024-06-17 23:18:27,488 - INFO - Epoch: 1 | Step: 140000 | Train loss: 1.086 | Eval loss: 1.083 | Eval accuracy: 0.420 | ɛ: 0.05
2024-06-17 23:18:48,126 - INFO - Epoch: 1 | Step: 145000 | Train loss: 1.086 | Eval loss: 1.086 | Eval accuracy: 0.417 | ɛ: 0.05
2024-06-17 23:19:08,631 - INFO - Epoch: 1 | Step: 150000 | Train loss: 1.086 | Eval loss: 1.087 | Eval accuracy: 0.418 | ɛ: 0.05
2024-06-17 23:19:30,260 - INFO - Epoch: 1 | Step: 155000 | Train loss: 1.086 | Eval loss: 1.083 | Eval accuracy: 0.421 | ɛ: 0.05
2024-06-17 23:19:51,238 - INFO - Epoch: 1 | Step: 160000 | Train loss: 1.086 | Eval loss: 1.080 | Eval accuracy: 0.419 | ɛ: 0.05
2024-06-17 23:20:13,166 - INFO - Epoch: 1 | Step: 165000 | Train loss: 1.086 | Eval loss: 1.081 | Eval accuracy: 0.418 | ɛ: 0.05
2024-06-17 23:20:35,121 - INFO - Epoch: 1 | Step: 170000 | Train loss: 1.086 | Eval loss: 1.080 | Eval accuracy: 0.419 | ɛ: 0.05
2024-06-17 23:20:56,282 - INFO - Epoch: 1 | Step: 175000 | Train loss: 1.086 | Eval loss: 1.080 | Eval accuracy: 0.420 | ɛ: 0.05
2024-06-17 23:21:17,532 - INFO - Epoch: 1 | Step: 180000 | Train loss: 1.086 | Eval loss: 1.078 | Eval accuracy: 0.422 | ɛ: 0.05
2024-06-17 23:21:38,431 - INFO - Epoch: 1 | Step: 185000 | Train loss: 1.086 | Eval loss: 1.079 | Eval accuracy: 0.422 | ɛ: 0.05
2024-06-17 23:21:58,933 - INFO - Epoch: 1 | Step: 190000 | Train loss: 1.086 | Eval loss: 1.079 | Eval accuracy: 0.423 | ɛ: 0.05
2024-06-17 23:22:19,607 - INFO - Epoch: 1 | Step: 195000 | Train loss: 1.086 | Eval loss: 1.079 | Eval accuracy: 0.422 | ɛ: 0.05
2024-06-17 23:22:40,366 - INFO - Epoch: 1 | Step: 200000 | Train loss: 1.086 | Eval loss: 1.079 | Eval accuracy: 0.421 | ɛ: 0.05
2024-06-18 00:01:40,085 - INFO - Total parameters: 4388486 || Trainable parameters: 2179 (0.04965265925423939%)
2024-06-18 00:29:32,984 - INFO - Train results: {'train_runtime': 1672.7886, 'train_samples_per_second': 4929.937, 'train_steps_per_second': 154.061, 'train_loss': 1.007150628894314, 'epoch': 21.0}
2024-06-18 00:29:34,315 - INFO - Evaluation results: {'eval_loss': 0.9688162803649902, 'eval_accuracy': 0.5337748344370861, 'eval_runtime': 1.1214, 'eval_samples_per_second': 8752.729, 'eval_steps_per_second': 273.774, 'epoch': 21.0}
2024-06-18 00:29:34,315 - INFO - Results saved to results/evaluation_results.json
2024-06-18 00:30:09,900 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-18 00:37:23,931 - INFO - Train results: {'train_runtime': 433.915, 'train_samples_per_second': 5069.203, 'train_steps_per_second': 158.45, 'train_loss': 0.586443315671821, 'epoch': 21.0}
2024-06-18 00:37:24,612 - INFO - Evaluation results: {'eval_loss': 0.534360408782959, 'eval_accuracy': 0.7378729635731284, 'eval_runtime': 0.6249, 'eval_samples_per_second': 8741.754, 'eval_steps_per_second': 273.63, 'epoch': 21.0}
2024-06-18 00:37:24,612 - INFO - Results saved to results/evaluation_results.json
2024-06-18 00:39:29,818 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-18 01:05:03,949 - INFO - Train results: {'train_runtime': 1534.0292, 'train_samples_per_second': 4980.848, 'train_steps_per_second': 155.663, 'train_loss': 0.5518882374842395, 'epoch': 21.0}
2024-06-18 01:05:08,706 - INFO - Evaluation results: {'eval_loss': 0.5404354929924011, 'eval_accuracy': 0.697303982191442, 'eval_f1': 0.6423937817778037, 'eval_runtime': 4.5626, 'eval_samples_per_second': 8861.162, 'eval_steps_per_second': 277.035, 'epoch': 21.0}
2024-06-18 01:05:08,706 - INFO - Results saved to results/evaluation_results.json
2024-06-18 01:05:23,208 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-18 01:10:01,325 - INFO - Train results: {'train_runtime': 278.0162, 'train_samples_per_second': 5087.218, 'train_steps_per_second': 159.002, 'train_loss': 0.5820772775683033, 'epoch': 21.0}
2024-06-18 01:10:01,465 - INFO - Evaluation results: {'eval_loss': 0.5594936609268188, 'eval_accuracy': 0.6915137614678899, 'eval_runtime': 0.1039, 'eval_samples_per_second': 8391.688, 'eval_steps_per_second': 269.458, 'epoch': 21.0}
2024-06-18 01:10:01,465 - INFO - Results saved to results/evaluation_results.json
2024-06-18 01:11:31,859 - INFO - Total parameters count: 4386307
2024-06-18 01:11:31,859 - INFO - Trainable parameters count: 4386307 (100.0%)
2024-06-18 01:39:32,342 - INFO - Train results: {'train_runtime': 1680.3679, 'train_samples_per_second': 4907.7, 'train_steps_per_second': 153.366, 'train_loss': 0.6167380450817396, 'epoch': 21.0}
2024-06-18 01:39:33,618 - INFO - Evaluation results: {'eval_loss': 0.8576476573944092, 'eval_accuracy': 0.6811003565970454, 'eval_runtime': 1.0654, 'eval_samples_per_second': 9212.454, 'eval_steps_per_second': 288.153, 'epoch': 21.0}
2024-06-18 01:39:33,618 - INFO - Results saved to results/evaluation_results.json
2024-06-18 01:40:08,938 - INFO - Total parameters count: 4386178
2024-06-18 01:40:08,938 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-18 01:47:26,464 - INFO - Train results: {'train_runtime': 437.4114, 'train_samples_per_second': 5028.682, 'train_steps_per_second': 157.184, 'train_loss': 0.31532317685717515, 'epoch': 21.0}
2024-06-18 01:47:27,113 - INFO - Evaluation results: {'eval_loss': 0.7283658385276794, 'eval_accuracy': 0.7596558667398865, 'eval_runtime': 0.5924, 'eval_samples_per_second': 9221.597, 'eval_steps_per_second': 288.65, 'epoch': 21.0}
2024-06-18 01:47:27,113 - INFO - Results saved to results/evaluation_results.json
2024-06-18 01:49:32,894 - INFO - Total parameters count: 4386178
2024-06-18 01:49:32,894 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-18 02:15:23,434 - INFO - Train results: {'train_runtime': 1550.4289, 'train_samples_per_second': 4928.163, 'train_steps_per_second': 154.016, 'train_loss': 0.2865173934520763, 'epoch': 21.0}
2024-06-18 02:15:27,980 - INFO - Evaluation results: {'eval_loss': 0.48104530572891235, 'eval_accuracy': 0.8302250803858521, 'eval_f1': 0.7816377171215881, 'eval_runtime': 4.3508, 'eval_samples_per_second': 9292.497, 'eval_steps_per_second': 290.52, 'epoch': 21.0}
2024-06-18 02:15:27,980 - INFO - Results saved to results/evaluation_results.json
2024-06-18 02:15:42,415 - INFO - Total parameters count: 4386178
2024-06-18 02:15:42,415 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-18 02:20:22,434 - INFO - Train results: {'train_runtime': 279.9102, 'train_samples_per_second': 5052.796, 'train_steps_per_second': 157.926, 'train_loss': 0.13983999780536088, 'epoch': 21.0}
2024-06-18 02:20:22,567 - INFO - Evaluation results: {'eval_loss': 0.9951156973838806, 'eval_accuracy': 0.7958715596330275, 'eval_runtime': 0.0958, 'eval_samples_per_second': 9103.912, 'eval_steps_per_second': 292.327, 'epoch': 21.0}
2024-06-18 02:20:22,568 - INFO - Results saved to results/evaluation_results.json
2024-06-18 02:21:52,611 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight'}
2024-06-18 02:21:52,611 - INFO - Total parameters: 4394886 || Trainable parameters: 8579 (0.19520415319077672%)
2024-06-18 02:49:32,233 - INFO - Train results: {'train_runtime': 1659.5206, 'train_samples_per_second': 4969.352, 'train_steps_per_second': 155.293, 'train_loss': 0.9254868892561814, 'epoch': 21.0}
2024-06-18 02:49:33,556 - INFO - Evaluation results: {'eval_loss': 0.8802981972694397, 'eval_accuracy': 0.6007131940906776, 'eval_runtime': 1.1135, 'eval_samples_per_second': 8814.472, 'eval_steps_per_second': 275.705, 'epoch': 21.0}
2024-06-18 02:49:33,557 - INFO - Results saved to results/evaluation_results.json
2024-06-18 02:50:08,951 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias'}
2024-06-18 02:50:08,951 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-18 02:57:21,156 - INFO - Train results: {'train_runtime': 432.0989, 'train_samples_per_second': 5090.508, 'train_steps_per_second': 159.116, 'train_loss': 0.5397143703267556, 'epoch': 21.0}
2024-06-18 02:57:21,832 - INFO - Evaluation results: {'eval_loss': 0.4884827136993408, 'eval_accuracy': 0.7711879919458173, 'eval_runtime': 0.6179, 'eval_samples_per_second': 8840.661, 'eval_steps_per_second': 276.726, 'epoch': 21.0}
2024-06-18 02:57:21,832 - INFO - Results saved to results/evaluation_results.json
2024-06-18 02:59:28,990 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight'}
2024-06-18 02:59:28,990 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-18 03:25:03,845 - INFO - Train results: {'train_runtime': 1534.7509, 'train_samples_per_second': 4978.506, 'train_steps_per_second': 155.589, 'train_loss': 0.5164134705743274, 'epoch': 21.0}
2024-06-18 03:25:08,617 - INFO - Evaluation results: {'eval_loss': 0.49589622020721436, 'eval_accuracy': 0.7323769478110315, 'eval_f1': 0.6761254789272031, 'eval_runtime': 4.5785, 'eval_samples_per_second': 8830.428, 'eval_steps_per_second': 276.074, 'epoch': 21.0}
2024-06-18 03:25:08,618 - INFO - Results saved to results/evaluation_results.json
2024-06-18 03:25:22,816 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight'}
2024-06-18 03:25:22,817 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-18 03:30:00,250 - INFO - Train results: {'train_runtime': 277.3289, 'train_samples_per_second': 5099.825, 'train_steps_per_second': 159.396, 'train_loss': 0.4866330280116262, 'epoch': 21.0}
2024-06-18 03:30:00,389 - INFO - Evaluation results: {'eval_loss': 0.48064568638801575, 'eval_accuracy': 0.7763761467889908, 'eval_runtime': 0.1027, 'eval_samples_per_second': 8494.083, 'eval_steps_per_second': 272.746, 'epoch': 21.0}
2024-06-18 03:30:00,390 - INFO - Results saved to results/evaluation_results.json
2024-06-18 03:31:29,981 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-18 03:31:30,126 - ERROR - Something went wrong while running Prefix+LoRA
2024-06-18 03:31:30,126 - ERROR - Error: BertForSequenceClassification.__init__() got an unexpected keyword argument 'num_labels'
2024-06-18 03:32:05,213 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-18 03:32:05,363 - ERROR - Something went wrong while running Prefix+LoRA
2024-06-18 03:32:05,363 - ERROR - Error: BertForSequenceClassification.__init__() got an unexpected keyword argument 'num_labels'
2024-06-18 03:34:10,039 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-18 03:34:10,247 - ERROR - Something went wrong while running Prefix+LoRA
2024-06-18 03:34:10,247 - ERROR - Error: BertForSequenceClassification.__init__() got an unexpected keyword argument 'num_labels'
2024-06-18 03:34:24,236 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-18 03:34:24,390 - ERROR - Something went wrong while running Prefix+LoRA
2024-06-18 03:34:24,391 - ERROR - Error: BertForSequenceClassification.__init__() got an unexpected keyword argument 'num_labels'
2024-06-18 03:34:26,290 - ERROR - Something went wrong while running Additional Layer
2024-06-18 03:34:26,290 - ERROR - Error: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: no
- Save strategy: steps
2024-06-18 03:34:28,133 - ERROR - Something went wrong while running Additional Layer
2024-06-18 03:34:28,133 - ERROR - Error: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: no
- Save strategy: steps
2024-06-18 03:34:29,979 - ERROR - Something went wrong while running Additional Layer
2024-06-18 03:34:29,979 - ERROR - Error: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: no
- Save strategy: steps
2024-06-18 03:34:31,841 - ERROR - Something went wrong while running Additional Layer
2024-06-18 03:34:31,841 - ERROR - Error: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: no
- Save strategy: steps
2024-06-18 08:01:31,992 - INFO - Total parameters: 4392578 || Trainable parameters: 6400 (0.14570031539565148%)
2024-06-18 08:35:07,746 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-18 08:37:02,071 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-18 08:38:21,987 - INFO - New Parameters Added by Adapters: {'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'base_model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.classifier.modules_to_save.default.weight', 'word_embeddings.weight', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.pooler.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.embeddings.token_type_embeddings.weight', 'base_model.classifier.original_module.bias', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.query.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.bert.pooler.dense.weight', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.classifier.original_module.weight', 'base_model.bert.embeddings.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.self.value.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.classifier.modules_to_save.default.bias', 'base_model.bert.encoder.layer.0.attention.self.query.bias', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'prompt_encoder.default.embedding.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.weight'}
2024-06-18 08:38:21,987 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-18 08:41:20,890 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-18 08:41:21,163 - INFO - Total parameters: 4726914 || Trainable parameters: 332544 (7.035118472644097 %)
2024-06-18 12:50:11,195 - INFO - Total parameters: 4388486 || Trainable parameters: 2179 (0.04965265925423939%)
2024-06-18 13:03:42,168 - INFO - Train results: {'train_runtime': 810.8409, 'train_samples_per_second': 4843.145, 'train_steps_per_second': 151.349, 'train_loss': 1.0283935929292807, 'epoch': 10.0}
2024-06-18 13:03:43,543 - INFO - Evaluation results: {'eval_loss': 1.0019688606262207, 'eval_accuracy': 0.5033112582781457, 'eval_runtime': 1.2096, 'eval_samples_per_second': 8114.007, 'eval_steps_per_second': 253.795, 'epoch': 10.0}
2024-06-18 13:03:43,543 - INFO - Results saved to results/evaluation_results.json
2024-06-18 13:04:19,632 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-18 13:07:53,438 - INFO - Train results: {'train_runtime': 213.6745, 'train_samples_per_second': 4901.988, 'train_steps_per_second': 153.224, 'train_loss': 0.604650314333385, 'epoch': 10.0}
2024-06-18 13:07:54,140 - INFO - Evaluation results: {'eval_loss': 0.5569403171539307, 'eval_accuracy': 0.7232289950576606, 'eval_runtime': 0.6741, 'eval_samples_per_second': 8104.157, 'eval_steps_per_second': 253.672, 'epoch': 10.0}
2024-06-18 13:07:54,140 - INFO - Results saved to results/evaluation_results.json
2024-06-18 13:10:00,839 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-18 13:21:54,453 - INFO - Train results: {'train_runtime': 713.4958, 'train_samples_per_second': 5099.483, 'train_steps_per_second': 159.37, 'train_loss': 0.5626000895872142, 'epoch': 10.0}
2024-06-18 13:21:59,098 - INFO - Evaluation results: {'eval_loss': 0.547855019569397, 'eval_accuracy': 0.6924066287410339, 'eval_f1': 0.6338260408692068, 'eval_runtime': 4.5533, 'eval_samples_per_second': 8879.239, 'eval_steps_per_second': 277.6, 'epoch': 10.0}
2024-06-18 13:21:59,099 - INFO - Results saved to results/evaluation_results.json
2024-06-18 13:22:13,298 - INFO - Total parameters: 4388228 || Trainable parameters: 2050 (0.046715895345456074%)
2024-06-18 13:24:25,771 - INFO - Train results: {'train_runtime': 132.3675, 'train_samples_per_second': 5088.032, 'train_steps_per_second': 159.027, 'train_loss': 0.6066830699177649, 'epoch': 10.0}
2024-06-18 13:24:25,893 - INFO - Evaluation results: {'eval_loss': 0.5753700733184814, 'eval_accuracy': 0.6892201834862385, 'eval_runtime': 0.1031, 'eval_samples_per_second': 8459.407, 'eval_steps_per_second': 271.632, 'epoch': 10.0}
2024-06-18 13:24:25,893 - INFO - Results saved to results/evaluation_results.json
2024-06-18 13:25:56,892 - INFO - Total parameters count: 4386307
2024-06-18 13:25:56,892 - INFO - Trainable parameters count: 4386307 (100.0%)
2024-06-18 13:38:42,631 - INFO - Train results: {'train_runtime': 765.6387, 'train_samples_per_second': 5129.077, 'train_steps_per_second': 160.284, 'train_loss': 0.7175415371201029, 'epoch': 10.0}
2024-06-18 13:38:43,793 - INFO - Evaluation results: {'eval_loss': 0.7558501958847046, 'eval_accuracy': 0.6882322975038206, 'eval_runtime': 1.0629, 'eval_samples_per_second': 9234.537, 'eval_steps_per_second': 288.844, 'epoch': 10.0}
2024-06-18 13:38:43,794 - INFO - Results saved to results/evaluation_results.json
2024-06-18 13:39:19,190 - INFO - Total parameters count: 4386178
2024-06-18 13:39:19,190 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-18 13:42:46,203 - INFO - Train results: {'train_runtime': 206.8967, 'train_samples_per_second': 5062.575, 'train_steps_per_second': 158.243, 'train_loss': 0.41769370360569613, 'epoch': 10.0}
2024-06-18 13:42:46,812 - INFO - Evaluation results: {'eval_loss': 0.5181419253349304, 'eval_accuracy': 0.7741167856489108, 'eval_runtime': 0.582, 'eval_samples_per_second': 9387.105, 'eval_steps_per_second': 293.83, 'epoch': 10.0}
2024-06-18 13:42:46,813 - INFO - Results saved to results/evaluation_results.json
2024-06-18 13:44:52,944 - INFO - Total parameters count: 4386178
2024-06-18 13:44:52,944 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-18 13:56:59,945 - INFO - Train results: {'train_runtime': 726.885, 'train_samples_per_second': 5005.551, 'train_steps_per_second': 156.435, 'train_loss': 0.3560644319971427, 'epoch': 10.0}
2024-06-18 13:57:04,576 - INFO - Evaluation results: {'eval_loss': 0.4050644040107727, 'eval_accuracy': 0.8208013851100667, 'eval_f1': 0.7717607031471505, 'eval_runtime': 4.5384, 'eval_samples_per_second': 8908.34, 'eval_steps_per_second': 278.51, 'epoch': 10.0}
2024-06-18 13:57:04,576 - INFO - Results saved to results/evaluation_results.json
2024-06-18 13:57:18,834 - INFO - Total parameters count: 4386178
2024-06-18 13:57:18,834 - INFO - Trainable parameters count: 4386178 (100.0%)
2024-06-18 13:59:33,319 - INFO - Train results: {'train_runtime': 134.3695, 'train_samples_per_second': 5012.223, 'train_steps_per_second': 156.658, 'train_loss': 0.20479774414218804, 'epoch': 10.0}
2024-06-18 13:59:33,437 - INFO - Evaluation results: {'eval_loss': 0.6750989556312561, 'eval_accuracy': 0.823394495412844, 'eval_runtime': 0.0994, 'eval_samples_per_second': 8771.433, 'eval_steps_per_second': 281.652, 'epoch': 10.0}
2024-06-18 13:59:33,437 - INFO - Results saved to results/evaluation_results.json
2024-06-18 14:01:04,169 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight'}
2024-06-18 14:01:04,169 - INFO - Total parameters: 4394886 || Trainable parameters: 8579 (0.19520415319077672%)
2024-06-18 14:14:00,542 - INFO - Train results: {'train_runtime': 776.2649, 'train_samples_per_second': 5058.866, 'train_steps_per_second': 158.09, 'train_loss': 0.9378113804382637, 'epoch': 10.0}
2024-06-18 14:14:01,752 - INFO - Evaluation results: {'eval_loss': 0.892228364944458, 'eval_accuracy': 0.5922567498726439, 'eval_runtime': 1.1103, 'eval_samples_per_second': 8840.222, 'eval_steps_per_second': 276.51, 'epoch': 10.0}
2024-06-18 14:14:01,753 - INFO - Results saved to results/evaluation_results.json
2024-06-18 14:14:37,724 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.embeddings.LayerNorm.weight'}
2024-06-18 14:14:37,724 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-18 14:17:58,269 - INFO - Train results: {'train_runtime': 200.3942, 'train_samples_per_second': 5226.849, 'train_steps_per_second': 163.378, 'train_loss': 0.5494717018591171, 'epoch': 10.0}
2024-06-18 14:17:58,954 - INFO - Evaluation results: {'eval_loss': 0.4978960454463959, 'eval_accuracy': 0.7614863628043199, 'eval_runtime': 0.6577, 'eval_samples_per_second': 8305.802, 'eval_steps_per_second': 259.984, 'epoch': 10.0}
2024-06-18 14:17:58,955 - INFO - Results saved to results/evaluation_results.json
2024-06-18 14:20:04,616 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.classifier.original_module.bias', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight'}
2024-06-18 14:20:04,616 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-18 14:31:43,989 - INFO - Train results: {'train_runtime': 699.2638, 'train_samples_per_second': 5203.272, 'train_steps_per_second': 162.614, 'train_loss': 0.5279605703569425, 'epoch': 10.0}
2024-06-18 14:31:48,605 - INFO - Evaluation results: {'eval_loss': 0.5115140080451965, 'eval_accuracy': 0.7202819688350235, 'eval_f1': 0.6653845015829808, 'eval_runtime': 4.524, 'eval_samples_per_second': 8936.868, 'eval_steps_per_second': 279.401, 'epoch': 10.0}
2024-06-18 14:31:48,606 - INFO - Results saved to results/evaluation_results.json
2024-06-18 14:32:03,012 - INFO - New Parameters Added by Adapters: {'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias'}
2024-06-18 14:32:03,012 - INFO - Total parameters: 4394628 || Trainable parameters: 8450 (0.1922802112033146%)
2024-06-18 14:34:09,727 - INFO - Train results: {'train_runtime': 126.6008, 'train_samples_per_second': 5319.794, 'train_steps_per_second': 166.271, 'train_loss': 0.5052224590319636, 'epoch': 10.0}
2024-06-18 14:34:09,847 - INFO - Evaluation results: {'eval_loss': 0.49291303753852844, 'eval_accuracy': 0.7545871559633027, 'eval_runtime': 0.102, 'eval_samples_per_second': 8550.839, 'eval_steps_per_second': 274.568, 'epoch': 10.0}
2024-06-18 14:34:09,848 - INFO - Results saved to results/evaluation_results.json
2024-06-18 14:34:44,576 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-18 14:34:44,870 - INFO - Total parameters: 4726914 || Trainable parameters: 332544 (7.035118472644097 %)
2024-06-18 14:38:02,390 - INFO - Train results: {'train_runtime': 197.4086, 'train_samples_per_second': 5305.899, 'train_steps_per_second': 165.849, 'train_loss': 0.6269367396139706, 'epoch': 10.0}
2024-06-18 14:38:03,129 - INFO - Evaluation results: {'eval_loss': 0.5786020755767822, 'eval_accuracy': 0.7138934651290499, 'eval_runtime': 0.7119, 'eval_samples_per_second': 7673.494, 'eval_steps_per_second': 240.192, 'epoch': 10.0}
2024-06-18 14:38:03,130 - INFO - Results saved to results/evaluation_results.json
2024-06-18 14:39:30,459 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-18 14:39:30,730 - INFO - Total parameters: 4727043 || Trainable parameters: 332544 (7.034926485754414 %)
2024-06-18 14:52:09,525 - INFO - Train results: {'train_runtime': 758.7175, 'train_samples_per_second': 5175.866, 'train_steps_per_second': 161.747, 'train_loss': 1.0147753652724325, 'epoch': 10.0}
2024-06-18 14:52:10,839 - INFO - Evaluation results: {'eval_loss': 0.9788184762001038, 'eval_accuracy': 0.5622007131940907, 'eval_runtime': 1.212, 'eval_samples_per_second': 8098.332, 'eval_steps_per_second': 253.305, 'epoch': 10.0}
2024-06-18 14:52:10,840 - INFO - Results saved to results/evaluation_results.json
2024-06-18 14:52:19,165 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-18 14:52:19,441 - INFO - Total parameters: 4726914 || Trainable parameters: 332544 (7.035118472644097 %)
2024-06-18 14:55:36,461 - INFO - Train results: {'train_runtime': 196.9166, 'train_samples_per_second': 5319.157, 'train_steps_per_second': 166.263, 'train_loss': 0.6222268588672561, 'epoch': 10.0}
2024-06-18 14:55:37,168 - INFO - Evaluation results: {'eval_loss': 0.5785854458808899, 'eval_accuracy': 0.7257916895478674, 'eval_runtime': 0.6783, 'eval_samples_per_second': 8054.175, 'eval_steps_per_second': 252.108, 'epoch': 10.0}
2024-06-18 14:55:37,168 - INFO - Results saved to results/evaluation_results.json
2024-06-18 14:55:42,029 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-18 14:55:42,297 - INFO - Total parameters: 4726914 || Trainable parameters: 332544 (7.035118472644097 %)
2024-06-18 14:59:02,330 - INFO - Train results: {'train_runtime': 199.9693, 'train_samples_per_second': 5237.955, 'train_steps_per_second': 163.725, 'train_loss': 0.6290298282918073, 'epoch': 10.0}
2024-06-18 14:59:03,033 - INFO - Evaluation results: {'eval_loss': 0.5835501551628113, 'eval_accuracy': 0.7181036060772469, 'eval_runtime': 0.6748, 'eval_samples_per_second': 8095.744, 'eval_steps_per_second': 253.409, 'epoch': 10.0}
2024-06-18 14:59:03,034 - INFO - Results saved to results/evaluation_results.json
2024-06-18 14:59:11,095 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-18 14:59:11,397 - INFO - Total parameters: 4726914 || Trainable parameters: 332544 (7.035118472644097 %)
2024-06-18 15:02:29,397 - INFO - Train results: {'train_runtime': 197.8983, 'train_samples_per_second': 5292.769, 'train_steps_per_second': 165.439, 'train_loss': 0.6255393309060145, 'epoch': 10.0}
2024-06-18 15:02:30,108 - INFO - Evaluation results: {'eval_loss': 0.5726837515830994, 'eval_accuracy': 0.7201171517481237, 'eval_runtime': 0.6835, 'eval_samples_per_second': 7992.414, 'eval_steps_per_second': 250.174, 'epoch': 10.0}
2024-06-18 15:02:30,109 - INFO - Results saved to results/evaluation_results.json
2024-06-18 15:04:32,386 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-18 15:04:32,669 - INFO - Total parameters: 4726914 || Trainable parameters: 332544 (7.035118472644097 %)
2024-06-18 15:16:12,514 - INFO - Train results: {'train_runtime': 699.7747, 'train_samples_per_second': 5199.474, 'train_steps_per_second': 162.495, 'train_loss': 0.5946219645225009, 'epoch': 10.0}
2024-06-18 15:16:17,752 - INFO - Evaluation results: {'eval_loss': 0.5718391537666321, 'eval_accuracy': 0.7044768736087064, 'eval_f1': 0.6153994720916758, 'eval_runtime': 5.1435, 'eval_samples_per_second': 7860.432, 'eval_steps_per_second': 245.748, 'epoch': 10.0}
2024-06-18 15:16:17,753 - INFO - Results saved to results/evaluation_results.json
2024-06-18 15:16:26,170 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-18 15:16:26,464 - INFO - Total parameters: 4726914 || Trainable parameters: 332544 (7.035118472644097 %)
2024-06-18 15:19:48,828 - INFO - Train results: {'train_runtime': 202.2537, 'train_samples_per_second': 5178.792, 'train_steps_per_second': 161.876, 'train_loss': 0.6119224697770181, 'epoch': 10.0}
2024-06-18 15:19:49,550 - INFO - Evaluation results: {'eval_loss': 0.5661758184432983, 'eval_accuracy': 0.7446457990115322, 'eval_runtime': 0.6942, 'eval_samples_per_second': 7869.946, 'eval_steps_per_second': 246.341, 'epoch': 10.0}
2024-06-18 15:19:49,551 - INFO - Results saved to results/evaluation_results.json
2024-06-18 15:20:00,570 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-18 15:20:00,874 - INFO - Total parameters: 4726914 || Trainable parameters: 332544 (7.035118472644097 %)
2024-06-18 15:22:11,561 - INFO - Train results: {'train_runtime': 130.6222, 'train_samples_per_second': 5156.016, 'train_steps_per_second': 161.152, 'train_loss': 0.5977252288102537, 'epoch': 10.0}
2024-06-18 15:22:11,694 - INFO - Evaluation results: {'eval_loss': 0.5817530155181885, 'eval_accuracy': 0.7098623853211009, 'eval_runtime': 0.1134, 'eval_samples_per_second': 7692.393, 'eval_steps_per_second': 247.003, 'epoch': 10.0}
2024-06-18 15:22:11,694 - INFO - Results saved to results/evaluation_results.json
2024-06-18 15:22:13,645 - ERROR - Something went wrong while running Additional Layer
2024-06-18 15:22:13,645 - ERROR - Error: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: no
- Save strategy: steps
2024-06-18 15:22:15,547 - ERROR - Something went wrong while running Additional Layer
2024-06-18 15:22:15,547 - ERROR - Error: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: no
- Save strategy: steps
2024-06-18 15:22:17,444 - ERROR - Something went wrong while running Additional Layer
2024-06-18 15:22:17,444 - ERROR - Error: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: no
- Save strategy: steps
2024-06-18 15:22:19,353 - ERROR - Something went wrong while running Additional Layer
2024-06-18 15:22:19,353 - ERROR - Error: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: no
- Save strategy: steps
2024-06-18 15:23:51,604 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.classifier.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.classifier.bias', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight'}
2024-06-18 15:23:51,604 - INFO - Total parameters count: 4392707
2024-06-18 15:23:51,604 - INFO - Trainable parameters count: 6400 (0.1456960366352684%)
2024-06-18 15:23:51,659 - INFO - Unique labels: {0, 1, 2}
2024-06-18 15:35:47,152 - INFO - Train results: {'train_runtime': 715.3811, 'train_samples_per_second': 5489.41, 'train_steps_per_second': 171.545, 'train_loss': 1.1012963732886656, 'epoch': 10.0}
2024-06-18 15:35:48,449 - INFO - Evaluation results: {'eval_loss': 1.0983190536499023, 'eval_accuracy': 0.33306164034640856, 'eval_runtime': 1.1965, 'eval_samples_per_second': 8203.383, 'eval_steps_per_second': 256.591, 'epoch': 10.0}
2024-06-18 15:35:48,450 - INFO - Results saved to results/evaluation_results.json
2024-06-18 15:36:24,680 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.classifier.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.classifier.weight', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'soft_prompt.soft_prompts', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias'}
2024-06-18 15:36:24,680 - INFO - Total parameters count: 4392578
2024-06-18 15:36:24,680 - INFO - Trainable parameters count: 6400 (0.14570031539565148%)
2024-06-18 15:36:24,695 - INFO - Unique labels: {0, 1}
2024-06-18 15:39:32,172 - INFO - Train results: {'train_runtime': 187.3772, 'train_samples_per_second': 5589.953, 'train_steps_per_second': 174.728, 'train_loss': 0.6951195806865658, 'epoch': 10.0}
2024-06-18 15:39:32,848 - INFO - Evaluation results: {'eval_loss': 0.6941783428192139, 'eval_accuracy': 0.4693391909207395, 'eval_runtime': 0.647, 'eval_samples_per_second': 8443.015, 'eval_steps_per_second': 264.279, 'epoch': 10.0}
2024-06-18 15:39:32,848 - INFO - Results saved to results/evaluation_results.json
2024-06-18 15:41:38,263 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.pooler.dense.weight', 'bert.classifier.weight', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.classifier.bias', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'soft_prompt.soft_prompts'}
2024-06-18 15:41:38,263 - INFO - Total parameters count: 4392578
2024-06-18 15:41:38,263 - INFO - Trainable parameters count: 6400 (0.14570031539565148%)
2024-06-18 15:41:38,314 - INFO - Unique labels: {0, 1}
2024-06-18 15:52:34,627 - INFO - Train results: {'train_runtime': 656.2012, 'train_samples_per_second': 5544.732, 'train_steps_per_second': 173.285, 'train_loss': 0.660270503890115, 'epoch': 10.0}
2024-06-18 15:52:39,569 - INFO - Evaluation results: {'eval_loss': 0.6570406556129456, 'eval_accuracy': 0.6318327974276527, 'eval_f1': 0.0, 'eval_runtime': 4.851, 'eval_samples_per_second': 8334.322, 'eval_steps_per_second': 260.564, 'epoch': 10.0}
2024-06-18 15:52:39,570 - INFO - Results saved to results/evaluation_results.json
2024-06-18 15:52:54,862 - INFO - New Parameters Added by Adapters: {'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.classifier.bias', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.output.dense.bias', 'soft_prompt.soft_prompts', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.classifier.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight'}
2024-06-18 15:52:54,862 - INFO - Total parameters count: 4392578
2024-06-18 15:52:54,862 - INFO - Trainable parameters count: 6400 (0.14570031539565148%)
2024-06-18 15:52:54,874 - INFO - Unique labels: {0, 1}
2024-06-18 15:55:09,962 - INFO - Train results: {'train_runtime': 134.9756, 'train_samples_per_second': 4989.716, 'train_steps_per_second': 155.954, 'train_loss': 0.6877538723843681, 'epoch': 10.0}
2024-06-18 15:55:10,324 - INFO - Evaluation results: {'eval_loss': 0.6982190608978271, 'eval_accuracy': 0.5091743119266054, 'eval_runtime': 0.3094, 'eval_samples_per_second': 2818.67, 'eval_steps_per_second': 90.508, 'epoch': 10.0}
2024-06-18 15:55:10,326 - INFO - Results saved to results/evaluation_results.json
2024-06-18 15:56:46,063 - INFO - Total parameters count: 4401286
2024-06-18 15:56:46,063 - INFO - Trainable parameters count: 14979 (0.3403323483181961%)
2024-06-18 15:56:46,116 - INFO - Unique labels: {0, 1, 2}
2024-06-18 16:07:57,658 - INFO - Train results: {'train_runtime': 671.4434, 'train_samples_per_second': 5848.624, 'train_steps_per_second': 182.77, 'train_loss': 1.0461094868773744, 'epoch': 10.0}
2024-06-18 16:07:58,673 - INFO - Evaluation results: {'eval_loss': 1.0232281684875488, 'eval_accuracy': 0.4761079979623026, 'eval_runtime': 0.914, 'eval_samples_per_second': 10739.047, 'eval_steps_per_second': 335.903, 'epoch': 10.0}
2024-06-18 16:07:58,674 - INFO - Results saved to results/evaluation_results.json
2024-06-18 16:08:34,721 - INFO - Total parameters count: 4401028
2024-06-18 16:08:34,721 - INFO - Trainable parameters count: 14850 (0.33742116614572776%)
2024-06-18 16:08:34,736 - INFO - Unique labels: {0, 1}
2024-06-18 16:11:29,927 - INFO - Train results: {'train_runtime': 175.0721, 'train_samples_per_second': 5982.849, 'train_steps_per_second': 187.009, 'train_loss': 0.6686732955727551, 'epoch': 10.0}
2024-06-18 16:11:30,466 - INFO - Evaluation results: {'eval_loss': 0.6511533260345459, 'eval_accuracy': 0.6203551162365001, 'eval_runtime': 0.5124, 'eval_samples_per_second': 10660.895, 'eval_steps_per_second': 333.702, 'epoch': 10.0}
2024-06-18 16:11:30,467 - INFO - Results saved to results/evaluation_results.json
2024-06-18 16:13:34,685 - INFO - Total parameters count: 4401028
2024-06-18 16:13:34,685 - INFO - Trainable parameters count: 14850 (0.33742116614572776%)
2024-06-18 16:13:34,735 - INFO - Unique labels: {0, 1}
2024-06-18 16:23:54,006 - INFO - Train results: {'train_runtime': 619.1588, 'train_samples_per_second': 5876.457, 'train_steps_per_second': 183.652, 'train_loss': 0.5673989644519006, 'epoch': 10.0}
2024-06-18 16:23:57,920 - INFO - Evaluation results: {'eval_loss': 0.534538209438324, 'eval_accuracy': 0.7134306208261192, 'eval_f1': 0.6430244022676855, 'eval_runtime': 3.8218, 'eval_samples_per_second': 10578.824, 'eval_steps_per_second': 330.735, 'epoch': 10.0}
2024-06-18 16:23:57,921 - INFO - Results saved to results/evaluation_results.json
2024-06-18 16:24:12,532 - INFO - Total parameters count: 4401028
2024-06-18 16:24:12,532 - INFO - Trainable parameters count: 14850 (0.33742116614572776%)
2024-06-18 16:24:12,542 - INFO - Unique labels: {0, 1}
2024-06-18 16:26:04,990 - INFO - Train results: {'train_runtime': 112.3293, 'train_samples_per_second': 5995.673, 'train_steps_per_second': 187.395, 'train_loss': 0.5385496367411489, 'epoch': 10.0}
2024-06-18 16:26:05,093 - INFO - Evaluation results: {'eval_loss': 0.514143705368042, 'eval_accuracy': 0.7431192660550459, 'eval_runtime': 0.0851, 'eval_samples_per_second': 10246.575, 'eval_steps_per_second': 329.018, 'epoch': 10.0}
2024-06-18 16:26:05,094 - INFO - Results saved to results/evaluation_results.json
2024-06-18 16:35:36,786 - INFO - Total parameters: 4392707 || Trainable parameters: 6400 (0.1456960366352684%)
2024-06-18 16:40:35,405 - INFO - Training finished
2024-06-18 16:40:35,405 - INFO - epsilon: inf, delta: 1e-05
2024-06-18 16:40:35,790 - INFO - Evaluation finished
2024-06-18 16:40:35,790 - INFO - Evaluation results: {'loss': nan, 'accuracy': 0.3544513879053959}.
2024-06-18 16:40:35,790 - INFO - Results saved to results/evaluation_results_dp.json
2024-06-18 16:41:12,142 - INFO - Total parameters: 4392578 || Trainable parameters: 6400 (0.14570031539565148%)
2024-06-18 16:42:31,782 - INFO - Training finished
2024-06-18 16:42:31,782 - INFO - epsilon: inf, delta: 1e-05
2024-06-18 16:42:31,997 - INFO - Evaluation finished
2024-06-18 16:42:31,997 - INFO - Evaluation results: {'loss': nan, 'accuracy': 0.49471618357487923}.
2024-06-18 16:42:31,997 - INFO - Results saved to results/evaluation_results_dp.json
2024-06-18 16:44:37,961 - INFO - Total parameters: 4392578 || Trainable parameters: 6400 (0.14570031539565148%)
2024-06-18 16:49:14,533 - INFO - Training finished
2024-06-18 16:49:14,533 - INFO - epsilon: inf, delta: 1e-05
2024-06-18 16:49:16,150 - INFO - Evaluation finished
2024-06-18 16:49:16,150 - INFO - Evaluation results: {'loss': nan, 'accuracy': 0.6316469823688969}.
2024-06-18 16:49:16,150 - INFO - Results saved to results/evaluation_results_dp.json
2024-06-18 16:49:30,498 - INFO - Total parameters: 4392578 || Trainable parameters: 6400 (0.14570031539565148%)
2024-06-18 16:50:21,958 - INFO - Training finished
2024-06-18 16:50:21,959 - INFO - epsilon: inf, delta: 1e-05
2024-06-18 16:50:21,995 - INFO - Evaluation finished
2024-06-18 16:50:21,995 - INFO - Evaluation results: {'loss': nan, 'accuracy': 0.49107142857142855}.
2024-06-18 16:50:21,995 - INFO - Results saved to results/evaluation_results_dp.json
2024-06-18 16:55:35,309 - INFO - Total parameters: 4392707 || Trainable parameters: 6400 (0.1456960366352684%)
2024-06-18 17:00:33,451 - INFO - Training finished
2024-06-18 17:00:33,451 - INFO - epsilon: 8.0, delta: 1e-05
2024-06-18 17:00:33,839 - INFO - Evaluation finished
2024-06-18 17:00:33,839 - INFO - Evaluation results: {'loss': 1.1134303070434919, 'accuracy': 0.3181737714204787}.
2024-06-18 17:00:33,839 - INFO - Results saved to results/evaluation_results_dp.json
2024-06-18 17:00:43,025 - INFO - Total parameters: 4392578 || Trainable parameters: 6400 (0.14570031539565148%)
2024-06-18 17:02:02,637 - INFO - Training finished
2024-06-18 17:02:02,637 - INFO - epsilon: 8.0, delta: 1e-05
2024-06-18 17:02:02,853 - INFO - Evaluation finished
2024-06-18 17:02:02,853 - INFO - Evaluation results: {'loss': 0.7035641370461001, 'accuracy': 0.49471618357487923}.
2024-06-18 17:02:02,853 - INFO - Results saved to results/evaluation_results_dp.json
2024-06-18 17:02:12,450 - INFO - Total parameters: 4392578 || Trainable parameters: 6400 (0.14570031539565148%)
2024-06-18 17:06:49,183 - INFO - Training finished
2024-06-18 17:06:49,183 - INFO - epsilon: 8.0, delta: 1e-05
2024-06-18 17:06:50,780 - INFO - Evaluation finished
2024-06-18 17:06:50,780 - INFO - Evaluation results: {'loss': 0.6834821456312379, 'accuracy': 0.6314491975587704}.
2024-06-18 17:06:50,780 - INFO - Results saved to results/evaluation_results_dp.json
2024-06-18 17:06:59,768 - INFO - Total parameters: 4392578 || Trainable parameters: 6400 (0.14570031539565148%)
2024-06-18 17:07:51,005 - INFO - Training finished
2024-06-18 17:07:51,005 - INFO - epsilon: 8.0, delta: 1e-05
2024-06-18 17:07:51,041 - INFO - Evaluation finished
2024-06-18 17:07:51,041 - INFO - Evaluation results: {'loss': 0.7009874113968441, 'accuracy': 0.5089285714285714}.
2024-06-18 17:07:51,041 - INFO - Results saved to results/evaluation_results_dp.json
2024-06-18 21:19:50,633 - INFO - New Parameters Added by Adapters: {'base_model.classifier.original_module.bias', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.classifier.modules_to_save.default.bias', 'base_model.classifier.original_module.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'base_model.bert.embeddings.token_type_embeddings.weight', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.0.attention.self.query.weight', 'base_model.bert.pooler.dense.bias', 'base_model.bert.embeddings.LayerNorm.bias', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.classifier.modules_to_save.default.weight', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.pooler.dense.weight', 'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'prompt_encoder.default.embedding.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.query.bias', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'word_embeddings.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.value.weight'}
2024-06-18 21:19:50,633 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-18 21:23:23,178 - INFO - Train results: {'train_runtime': 212.4213, 'train_samples_per_second': 4930.908, 'train_steps_per_second': 154.128, 'train_loss': 0.6894862884126499, 'epoch': 10.0}
2024-06-18 21:23:23,861 - INFO - Evaluation results: {'eval_loss': 0.6815396547317505, 'eval_accuracy': 0.5654402343034962, 'eval_runtime': 0.6548, 'eval_samples_per_second': 8343.599, 'eval_steps_per_second': 261.167, 'epoch': 10.0}
2024-06-18 21:23:23,862 - INFO - Results saved to results/evaluation_results.json
2024-06-18 21:24:52,263 - INFO - New Parameters Added by Adapters: {'base_model.classifier.original_module.bias', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.classifier.modules_to_save.default.bias', 'base_model.classifier.original_module.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'base_model.bert.embeddings.token_type_embeddings.weight', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.0.attention.self.query.weight', 'base_model.bert.pooler.dense.bias', 'base_model.bert.embeddings.LayerNorm.bias', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.classifier.modules_to_save.default.weight', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.pooler.dense.weight', 'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'prompt_encoder.default.embedding.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.query.bias', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'word_embeddings.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.value.weight'}
2024-06-18 21:24:52,264 - INFO - Total parameters: 4390534 || Trainable parameters: 4227 (0.09627530500845684%)
2024-06-18 21:38:17,430 - INFO - Train results: {'train_runtime': 805.091, 'train_samples_per_second': 4877.734, 'train_steps_per_second': 152.43, 'train_loss': 1.0910373782396627, 'epoch': 10.0}
2024-06-18 21:38:18,711 - INFO - Evaluation results: {'eval_loss': 1.0819060802459717, 'eval_accuracy': 0.400509424350484, 'eval_runtime': 1.1815, 'eval_samples_per_second': 8306.965, 'eval_steps_per_second': 259.831, 'epoch': 10.0}
2024-06-18 21:38:18,712 - INFO - Results saved to results/evaluation_results.json
2024-06-18 21:38:27,777 - INFO - New Parameters Added by Adapters: {'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.classifier.modules_to_save.default.weight', 'base_model.bert.embeddings.token_type_embeddings.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.weight', 'word_embeddings.weight', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.classifier.modules_to_save.default.bias', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'base_model.bert.encoder.layer.0.attention.self.value.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'base_model.bert.pooler.dense.bias', 'base_model.classifier.original_module.bias', 'base_model.bert.pooler.dense.weight', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.0.attention.self.query.weight', 'base_model.bert.encoder.layer.0.attention.self.query.bias', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.classifier.original_module.weight', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.bert.embeddings.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'prompt_encoder.default.embedding.weight', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias'}
2024-06-18 21:38:27,778 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-18 21:41:52,529 - INFO - Train results: {'train_runtime': 204.6378, 'train_samples_per_second': 5118.457, 'train_steps_per_second': 159.99, 'train_loss': 0.6895873735208808, 'epoch': 10.0}
2024-06-18 21:41:53,216 - INFO - Evaluation results: {'eval_loss': 0.6825734972953796, 'eval_accuracy': 0.5636097382390628, 'eval_runtime': 0.6597, 'eval_samples_per_second': 8280.807, 'eval_steps_per_second': 259.202, 'epoch': 10.0}
2024-06-18 21:41:53,217 - INFO - Results saved to results/evaluation_results.json
2024-06-18 21:41:58,719 - INFO - New Parameters Added by Adapters: {'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.classifier.modules_to_save.default.weight', 'base_model.bert.embeddings.token_type_embeddings.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.weight', 'word_embeddings.weight', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.classifier.modules_to_save.default.bias', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'base_model.bert.encoder.layer.0.attention.self.value.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'base_model.bert.pooler.dense.bias', 'base_model.classifier.original_module.bias', 'base_model.bert.pooler.dense.weight', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.0.attention.self.query.weight', 'base_model.bert.encoder.layer.0.attention.self.query.bias', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.classifier.original_module.weight', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.bert.embeddings.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'prompt_encoder.default.embedding.weight', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias'}
2024-06-18 21:41:58,719 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-18 21:45:20,965 - INFO - Train results: {'train_runtime': 202.1816, 'train_samples_per_second': 5180.64, 'train_steps_per_second': 161.934, 'train_loss': 0.6814905510490524, 'epoch': 10.0}
2024-06-18 21:45:21,638 - INFO - Evaluation results: {'eval_loss': 0.6638860106468201, 'eval_accuracy': 0.6015010067728355, 'eval_runtime': 0.6452, 'eval_samples_per_second': 8466.979, 'eval_steps_per_second': 265.029, 'epoch': 10.0}
2024-06-18 21:45:21,639 - INFO - Results saved to results/evaluation_results.json
2024-06-18 21:45:30,190 - INFO - New Parameters Added by Adapters: {'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.pooler.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.query.bias', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.bert.embeddings.LayerNorm.bias', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.encoder.layer.0.attention.self.query.weight', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.classifier.original_module.bias', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.classifier.modules_to_save.default.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.bert.embeddings.token_type_embeddings.weight', 'prompt_encoder.default.embedding.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'word_embeddings.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.classifier.original_module.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.classifier.modules_to_save.default.bias', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.self.value.weight', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.bert.pooler.dense.weight'}
2024-06-18 21:45:30,191 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-18 21:48:52,942 - INFO - Train results: {'train_runtime': 202.639, 'train_samples_per_second': 5168.945, 'train_steps_per_second': 161.568, 'train_loss': 0.685033675498473, 'epoch': 10.0}
2024-06-18 21:48:53,613 - INFO - Evaluation results: {'eval_loss': 0.6754259467124939, 'eval_accuracy': 0.5828299469156142, 'eval_runtime': 0.6439, 'eval_samples_per_second': 8484.382, 'eval_steps_per_second': 265.574, 'epoch': 10.0}
2024-06-18 21:48:53,614 - INFO - Results saved to results/evaluation_results.json
2024-06-18 21:50:56,242 - INFO - New Parameters Added by Adapters: {'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.pooler.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.bert.encoder.layer.0.attention.self.query.bias', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.bert.embeddings.LayerNorm.bias', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.encoder.layer.0.attention.self.query.weight', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.classifier.original_module.bias', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.classifier.modules_to_save.default.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.bert.embeddings.token_type_embeddings.weight', 'prompt_encoder.default.embedding.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'word_embeddings.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.classifier.original_module.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.classifier.modules_to_save.default.bias', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.encoder.layer.0.attention.self.value.weight', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.bert.pooler.dense.weight'}
2024-06-18 21:50:56,242 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-18 22:03:14,287 - INFO - Train results: {'train_runtime': 737.9689, 'train_samples_per_second': 4930.371, 'train_steps_per_second': 154.085, 'train_loss': 0.6335265600673966, 'epoch': 10.0}
2024-06-18 22:03:19,378 - INFO - Evaluation results: {'eval_loss': 0.6222567558288574, 'eval_accuracy': 0.6390304229532525, 'eval_f1': 0.4978667767685109, 'eval_runtime': 4.9958, 'eval_samples_per_second': 8092.843, 'eval_steps_per_second': 253.014, 'epoch': 10.0}
2024-06-18 22:03:19,379 - INFO - Results saved to results/evaluation_results.json
2024-06-18 22:03:28,199 - INFO - New Parameters Added by Adapters: {'base_model.bert.encoder.layer.0.attention.self.query.weight', 'prompt_encoder.default.embedding.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'base_model.bert.embeddings.token_type_embeddings.weight', 'base_model.bert.pooler.dense.bias', 'base_model.classifier.original_module.weight', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.classifier.modules_to_save.default.weight', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.value.weight', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'word_embeddings.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.classifier.original_module.bias', 'base_model.bert.encoder.layer.0.attention.self.query.bias', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.classifier.modules_to_save.default.bias', 'base_model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.bert.pooler.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'base_model.bert.embeddings.LayerNorm.bias', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight'}
2024-06-18 22:03:28,199 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-18 22:06:54,733 - INFO - Train results: {'train_runtime': 206.4238, 'train_samples_per_second': 5074.173, 'train_steps_per_second': 158.606, 'train_loss': 0.6807988398382412, 'epoch': 10.0}
2024-06-18 22:06:55,395 - INFO - Evaluation results: {'eval_loss': 0.66751629114151, 'eval_accuracy': 0.5945451217279882, 'eval_runtime': 0.6341, 'eval_samples_per_second': 8615.795, 'eval_steps_per_second': 269.687, 'epoch': 10.0}
2024-06-18 22:06:55,396 - INFO - Results saved to results/evaluation_results.json
2024-06-18 22:07:06,786 - INFO - New Parameters Added by Adapters: {'base_model.bert.encoder.layer.0.attention.self.query.weight', 'prompt_encoder.default.embedding.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.self.key.weight', 'base_model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.self.query.bias', 'base_model.bert.embeddings.token_type_embeddings.weight', 'base_model.bert.pooler.dense.bias', 'base_model.classifier.original_module.weight', 'base_model.bert.encoder.layer.1.output.dense.weight', 'base_model.classifier.modules_to_save.default.weight', 'base_model.bert.encoder.layer.0.output.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.value.weight', 'base_model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.bert.embeddings.word_embeddings.weight', 'base_model.bert.encoder.layer.0.output.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.value.weight', 'base_model.bert.embeddings.position_embeddings.weight', 'base_model.bert.encoder.layer.1.attention.self.value.bias', 'base_model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.bert.embeddings.LayerNorm.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.bert.encoder.layer.0.intermediate.dense.bias', 'word_embeddings.weight', 'base_model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.bert.encoder.layer.1.attention.self.query.weight', 'base_model.bert.encoder.layer.1.attention.self.key.bias', 'base_model.bert.encoder.layer.1.output.dense.bias', 'base_model.bert.encoder.layer.1.attention.self.key.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.classifier.original_module.bias', 'base_model.bert.encoder.layer.0.attention.self.query.bias', 'base_model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.classifier.modules_to_save.default.bias', 'base_model.bert.encoder.layer.0.attention.self.key.bias', 'base_model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.bert.pooler.dense.weight', 'base_model.bert.encoder.layer.0.attention.self.value.bias', 'base_model.bert.embeddings.LayerNorm.bias', 'base_model.bert.encoder.layer.1.output.LayerNorm.weight'}
2024-06-18 22:07:06,787 - INFO - Total parameters: 4390276 || Trainable parameters: 4098 (0.09334265089484124%)
2024-06-18 22:09:17,764 - INFO - Train results: {'train_runtime': 130.9161, 'train_samples_per_second': 5144.44, 'train_steps_per_second': 160.79, 'train_loss': 0.6678007353513088, 'epoch': 10.0}
2024-06-18 22:09:17,884 - INFO - Evaluation results: {'eval_loss': 0.6474547982215881, 'eval_accuracy': 0.6708715596330275, 'eval_runtime': 0.1019, 'eval_samples_per_second': 8553.838, 'eval_steps_per_second': 274.665, 'epoch': 10.0}
2024-06-18 22:09:17,885 - INFO - Results saved to results/evaluation_results.json
2024-06-18 22:09:19,791 - ERROR - Something went wrong while running Additional Layer
2024-06-18 22:09:19,791 - ERROR - Error: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: no
- Save strategy: steps
2024-06-18 22:09:21,643 - ERROR - Something went wrong while running Additional Layer
2024-06-18 22:09:21,643 - ERROR - Error: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: no
- Save strategy: steps
2024-06-18 22:09:23,509 - ERROR - Something went wrong while running Additional Layer
2024-06-18 22:09:23,509 - ERROR - Error: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: no
- Save strategy: steps
2024-06-18 22:09:25,373 - ERROR - Something went wrong while running Additional Layer
2024-06-18 22:09:25,374 - ERROR - Error: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: no
- Save strategy: steps
2024-06-18 22:47:41,386 - INFO - Total parameters: 4386307 || Trainable parameters: 387 (0.00882291184816749%)
2024-06-18 22:57:23,259 - INFO - Train results: {'train_runtime': 581.7646, 'train_samples_per_second': 6750.187, 'train_steps_per_second': 210.944, 'train_loss': 1.0614502135435948, 'epoch': 10.0}
2024-06-18 22:57:35,893 - INFO - Evaluation results: {'eval_loss': 1.0432934761047363, 'eval_accuracy': 0.45827814569536424, 'eval_runtime': 12.4192, 'eval_samples_per_second': 790.311, 'eval_steps_per_second': 24.72, 'epoch': 10.0}
2024-06-18 22:57:35,895 - INFO - Results saved to results/evaluation_results.json
2024-06-18 22:58:09,401 - INFO - Total parameters: 4386178 || Trainable parameters: 258 (0.005882114223362572%)
2024-06-18 23:00:39,180 - INFO - Train results: {'train_runtime': 149.6805, 'train_samples_per_second': 6997.77, 'train_steps_per_second': 218.733, 'train_loss': 0.6349573813055664, 'epoch': 10.0}
2024-06-18 23:00:42,919 - INFO - Evaluation results: {'eval_loss': 0.595863938331604, 'eval_accuracy': 0.6857038257367747, 'eval_runtime': 3.6572, 'eval_samples_per_second': 1493.78, 'eval_steps_per_second': 46.758, 'epoch': 10.0}
2024-06-18 23:00:42,920 - INFO - Results saved to results/evaluation_results.json
2024-06-18 23:02:46,554 - INFO - Total parameters: 4386178 || Trainable parameters: 258 (0.005882114223362572%)
2024-06-18 23:11:22,650 - INFO - Train results: {'train_runtime': 515.9913, 'train_samples_per_second': 7051.397, 'train_steps_per_second': 220.372, 'train_loss': 0.5921029085723405, 'epoch': 10.0}
2024-06-18 23:21:01,535 - INFO - Total parameters: 4386178 || Trainable parameters: 258 (0.005882114223362572%)
2024-06-18 23:29:37,943 - INFO - Train results: {'train_runtime': 516.3005, 'train_samples_per_second': 7047.175, 'train_steps_per_second': 220.24, 'train_loss': 0.5906541530086459, 'epoch': 10.0}
2024-06-18 23:38:20,490 - INFO - Total parameters: 4386178 || Trainable parameters: 258 (0.005882114223362572%)
2024-06-18 23:46:46,679 - INFO - Train results: {'train_runtime': 506.0718, 'train_samples_per_second': 7189.612, 'train_steps_per_second': 224.691, 'train_loss': 0.5912481914230412, 'epoch': 10.0}
2024-06-18 23:59:40,842 - INFO - Total parameters: 4386178 || Trainable parameters: 258 (0.005882114223362572%)
2024-06-19 00:01:10,616 - INFO - Train results: {'train_runtime': 89.6724, 'train_samples_per_second': 7510.565, 'train_steps_per_second': 234.743, 'train_loss': 0.5991609445377087, 'epoch': 10.0}
2024-06-19 00:01:10,868 - INFO - Evaluation results: {'eval_loss': 0.5722342133522034, 'eval_accuracy': 0.698394495412844, 'eval_runtime': 0.2286, 'eval_samples_per_second': 3815.163, 'eval_steps_per_second': 122.505, 'epoch': 10.0}
2024-06-19 00:01:10,870 - INFO - Results saved to results/evaluation_results.json
2024-06-19 00:02:15,799 - INFO - Total parameters: 4386178 || Trainable parameters: 258 (0.005882114223362572%)
2024-06-19 21:04:09,203 - INFO - Metric: EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
2024-06-19 21:04:09,532 - INFO - Total parameters: 4726914 || Trainable parameters: 332544 (7.035118472644097 %)
