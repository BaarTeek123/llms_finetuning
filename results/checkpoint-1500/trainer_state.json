{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.17816842855446016,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001187789523696401,
      "grad_norm": 1.571284294128418,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.691,
      "step": 10
    },
    {
      "epoch": 0.002375579047392802,
      "grad_norm": 2.3712053298950195,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.7104,
      "step": 20
    },
    {
      "epoch": 0.003563368571089203,
      "grad_norm": 2.394946813583374,
      "learning_rate": 3e-06,
      "loss": 0.7076,
      "step": 30
    },
    {
      "epoch": 0.004751158094785604,
      "grad_norm": 3.6025919914245605,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.714,
      "step": 40
    },
    {
      "epoch": 0.005938947618482005,
      "grad_norm": 1.937501311302185,
      "learning_rate": 5e-06,
      "loss": 0.6987,
      "step": 50
    },
    {
      "epoch": 0.007126737142178406,
      "grad_norm": 3.3844287395477295,
      "learning_rate": 6e-06,
      "loss": 0.7021,
      "step": 60
    },
    {
      "epoch": 0.008314526665874808,
      "grad_norm": 3.468790054321289,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.699,
      "step": 70
    },
    {
      "epoch": 0.009502316189571208,
      "grad_norm": 1.6214921474456787,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.6835,
      "step": 80
    },
    {
      "epoch": 0.010690105713267608,
      "grad_norm": 1.8596107959747314,
      "learning_rate": 9e-06,
      "loss": 0.6895,
      "step": 90
    },
    {
      "epoch": 0.01187789523696401,
      "grad_norm": 2.3269872665405273,
      "learning_rate": 1e-05,
      "loss": 0.6835,
      "step": 100
    },
    {
      "epoch": 0.01306568476066041,
      "grad_norm": 4.275064945220947,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.6999,
      "step": 110
    },
    {
      "epoch": 0.014253474284356813,
      "grad_norm": 1.7505264282226562,
      "learning_rate": 1.2e-05,
      "loss": 0.6941,
      "step": 120
    },
    {
      "epoch": 0.015441263808053213,
      "grad_norm": 1.5032916069030762,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.6868,
      "step": 130
    },
    {
      "epoch": 0.016629053331749615,
      "grad_norm": 3.410565137863159,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.703,
      "step": 140
    },
    {
      "epoch": 0.017816842855446016,
      "grad_norm": 2.50521183013916,
      "learning_rate": 1.5e-05,
      "loss": 0.7025,
      "step": 150
    },
    {
      "epoch": 0.019004632379142416,
      "grad_norm": 3.892134428024292,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.6794,
      "step": 160
    },
    {
      "epoch": 0.020192421902838816,
      "grad_norm": 1.7042125463485718,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.6917,
      "step": 170
    },
    {
      "epoch": 0.021380211426535217,
      "grad_norm": 4.2205915451049805,
      "learning_rate": 1.8e-05,
      "loss": 0.6971,
      "step": 180
    },
    {
      "epoch": 0.02256800095023162,
      "grad_norm": 1.5046566724777222,
      "learning_rate": 1.9e-05,
      "loss": 0.6874,
      "step": 190
    },
    {
      "epoch": 0.02375579047392802,
      "grad_norm": 1.69389009475708,
      "learning_rate": 2e-05,
      "loss": 0.6692,
      "step": 200
    },
    {
      "epoch": 0.02494357999762442,
      "grad_norm": 1.7059226036071777,
      "learning_rate": 2.1e-05,
      "loss": 0.6717,
      "step": 210
    },
    {
      "epoch": 0.02613136952132082,
      "grad_norm": 1.818788766860962,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.6601,
      "step": 220
    },
    {
      "epoch": 0.02731915904501722,
      "grad_norm": 2.0799472332000732,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.6629,
      "step": 230
    },
    {
      "epoch": 0.028506948568713626,
      "grad_norm": 1.823838472366333,
      "learning_rate": 2.4e-05,
      "loss": 0.6681,
      "step": 240
    },
    {
      "epoch": 0.029694738092410026,
      "grad_norm": 2.8223283290863037,
      "learning_rate": 2.5e-05,
      "loss": 0.6993,
      "step": 250
    },
    {
      "epoch": 0.030882527616106426,
      "grad_norm": 2.522125244140625,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.7037,
      "step": 260
    },
    {
      "epoch": 0.03207031713980283,
      "grad_norm": 1.8430627584457397,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.6947,
      "step": 270
    },
    {
      "epoch": 0.03325810666349923,
      "grad_norm": 3.059164524078369,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.6869,
      "step": 280
    },
    {
      "epoch": 0.03444589618719563,
      "grad_norm": 1.9441590309143066,
      "learning_rate": 2.9e-05,
      "loss": 0.6886,
      "step": 290
    },
    {
      "epoch": 0.03563368571089203,
      "grad_norm": 1.770355224609375,
      "learning_rate": 3e-05,
      "loss": 0.6928,
      "step": 300
    },
    {
      "epoch": 0.03682147523458843,
      "grad_norm": 1.7763140201568604,
      "learning_rate": 3.1e-05,
      "loss": 0.6743,
      "step": 310
    },
    {
      "epoch": 0.03800926475828483,
      "grad_norm": 1.8267428874969482,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.6672,
      "step": 320
    },
    {
      "epoch": 0.03919705428198123,
      "grad_norm": 1.6273711919784546,
      "learning_rate": 3.3e-05,
      "loss": 0.6543,
      "step": 330
    },
    {
      "epoch": 0.04038484380567763,
      "grad_norm": 4.313201904296875,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.6706,
      "step": 340
    },
    {
      "epoch": 0.04157263332937403,
      "grad_norm": 4.140109062194824,
      "learning_rate": 3.5e-05,
      "loss": 0.6651,
      "step": 350
    },
    {
      "epoch": 0.04276042285307043,
      "grad_norm": 2.572695255279541,
      "learning_rate": 3.6e-05,
      "loss": 0.67,
      "step": 360
    },
    {
      "epoch": 0.04394821237676684,
      "grad_norm": 2.187011957168579,
      "learning_rate": 3.7e-05,
      "loss": 0.6432,
      "step": 370
    },
    {
      "epoch": 0.04513600190046324,
      "grad_norm": 4.77373743057251,
      "learning_rate": 3.8e-05,
      "loss": 0.671,
      "step": 380
    },
    {
      "epoch": 0.04632379142415964,
      "grad_norm": 2.0730113983154297,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.6803,
      "step": 390
    },
    {
      "epoch": 0.04751158094785604,
      "grad_norm": 3.850599765777588,
      "learning_rate": 4e-05,
      "loss": 0.6695,
      "step": 400
    },
    {
      "epoch": 0.04869937047155244,
      "grad_norm": 3.2260897159576416,
      "learning_rate": 4.1e-05,
      "loss": 0.6611,
      "step": 410
    },
    {
      "epoch": 0.04988715999524884,
      "grad_norm": 2.1476058959960938,
      "learning_rate": 4.2e-05,
      "loss": 0.6543,
      "step": 420
    },
    {
      "epoch": 0.05107494951894524,
      "grad_norm": 5.54133415222168,
      "learning_rate": 4.3e-05,
      "loss": 0.6637,
      "step": 430
    },
    {
      "epoch": 0.05226273904264164,
      "grad_norm": 3.140399932861328,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.6532,
      "step": 440
    },
    {
      "epoch": 0.05345052856633804,
      "grad_norm": 3.0184481143951416,
      "learning_rate": 4.5e-05,
      "loss": 0.613,
      "step": 450
    },
    {
      "epoch": 0.05463831809003444,
      "grad_norm": 3.482469320297241,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.6442,
      "step": 460
    },
    {
      "epoch": 0.055826107613730844,
      "grad_norm": 4.6692423820495605,
      "learning_rate": 4.7e-05,
      "loss": 0.6508,
      "step": 470
    },
    {
      "epoch": 0.05701389713742725,
      "grad_norm": 5.267462253570557,
      "learning_rate": 4.8e-05,
      "loss": 0.6357,
      "step": 480
    },
    {
      "epoch": 0.05820168666112365,
      "grad_norm": 5.792267322540283,
      "learning_rate": 4.9e-05,
      "loss": 0.6523,
      "step": 490
    },
    {
      "epoch": 0.05938947618482005,
      "grad_norm": 3.983430862426758,
      "learning_rate": 5e-05,
      "loss": 0.639,
      "step": 500
    },
    {
      "epoch": 0.06057726570851645,
      "grad_norm": 5.588817596435547,
      "learning_rate": 4.997980369188512e-05,
      "loss": 0.64,
      "step": 510
    },
    {
      "epoch": 0.06176505523221285,
      "grad_norm": 7.067869186401367,
      "learning_rate": 4.995960738377025e-05,
      "loss": 0.633,
      "step": 520
    },
    {
      "epoch": 0.06295284475590926,
      "grad_norm": 3.3713228702545166,
      "learning_rate": 4.993941107565537e-05,
      "loss": 0.6247,
      "step": 530
    },
    {
      "epoch": 0.06414063427960566,
      "grad_norm": 6.371031761169434,
      "learning_rate": 4.99192147675405e-05,
      "loss": 0.6461,
      "step": 540
    },
    {
      "epoch": 0.06532842380330206,
      "grad_norm": 2.8868744373321533,
      "learning_rate": 4.9899018459425616e-05,
      "loss": 0.5931,
      "step": 550
    },
    {
      "epoch": 0.06651621332699846,
      "grad_norm": 4.016470909118652,
      "learning_rate": 4.987882215131074e-05,
      "loss": 0.6165,
      "step": 560
    },
    {
      "epoch": 0.06770400285069486,
      "grad_norm": 4.354010105133057,
      "learning_rate": 4.985862584319587e-05,
      "loss": 0.6168,
      "step": 570
    },
    {
      "epoch": 0.06889179237439126,
      "grad_norm": 4.343535423278809,
      "learning_rate": 4.9838429535080985e-05,
      "loss": 0.6168,
      "step": 580
    },
    {
      "epoch": 0.07007958189808766,
      "grad_norm": 5.812617301940918,
      "learning_rate": 4.981823322696612e-05,
      "loss": 0.6248,
      "step": 590
    },
    {
      "epoch": 0.07126737142178406,
      "grad_norm": 5.959688663482666,
      "learning_rate": 4.9798036918851236e-05,
      "loss": 0.6154,
      "step": 600
    },
    {
      "epoch": 0.07245516094548046,
      "grad_norm": 4.963962554931641,
      "learning_rate": 4.977784061073636e-05,
      "loss": 0.6111,
      "step": 610
    },
    {
      "epoch": 0.07364295046917686,
      "grad_norm": 6.3539605140686035,
      "learning_rate": 4.975764430262148e-05,
      "loss": 0.6219,
      "step": 620
    },
    {
      "epoch": 0.07483073999287326,
      "grad_norm": 4.269138813018799,
      "learning_rate": 4.9737447994506606e-05,
      "loss": 0.5873,
      "step": 630
    },
    {
      "epoch": 0.07601852951656966,
      "grad_norm": 5.565948963165283,
      "learning_rate": 4.971725168639173e-05,
      "loss": 0.5671,
      "step": 640
    },
    {
      "epoch": 0.07720631904026606,
      "grad_norm": 9.216350555419922,
      "learning_rate": 4.969705537827685e-05,
      "loss": 0.5477,
      "step": 650
    },
    {
      "epoch": 0.07839410856396246,
      "grad_norm": 2.9129035472869873,
      "learning_rate": 4.9676859070161975e-05,
      "loss": 0.5838,
      "step": 660
    },
    {
      "epoch": 0.07958189808765886,
      "grad_norm": 13.75711727142334,
      "learning_rate": 4.96566627620471e-05,
      "loss": 0.5386,
      "step": 670
    },
    {
      "epoch": 0.08076968761135526,
      "grad_norm": 4.186140537261963,
      "learning_rate": 4.9636466453932226e-05,
      "loss": 0.6116,
      "step": 680
    },
    {
      "epoch": 0.08195747713505167,
      "grad_norm": 2.5830237865448,
      "learning_rate": 4.9616270145817344e-05,
      "loss": 0.54,
      "step": 690
    },
    {
      "epoch": 0.08314526665874807,
      "grad_norm": 8.397902488708496,
      "learning_rate": 4.959607383770247e-05,
      "loss": 0.6059,
      "step": 700
    },
    {
      "epoch": 0.08433305618244447,
      "grad_norm": 8.332036972045898,
      "learning_rate": 4.9575877529587595e-05,
      "loss": 0.5963,
      "step": 710
    },
    {
      "epoch": 0.08552084570614087,
      "grad_norm": 4.008006572723389,
      "learning_rate": 4.9555681221472714e-05,
      "loss": 0.4987,
      "step": 720
    },
    {
      "epoch": 0.08670863522983727,
      "grad_norm": 8.097256660461426,
      "learning_rate": 4.953548491335784e-05,
      "loss": 0.5773,
      "step": 730
    },
    {
      "epoch": 0.08789642475353368,
      "grad_norm": 6.3633551597595215,
      "learning_rate": 4.9515288605242965e-05,
      "loss": 0.5049,
      "step": 740
    },
    {
      "epoch": 0.08908421427723008,
      "grad_norm": 6.642234802246094,
      "learning_rate": 4.949509229712809e-05,
      "loss": 0.4989,
      "step": 750
    },
    {
      "epoch": 0.09027200380092648,
      "grad_norm": 6.825439453125,
      "learning_rate": 4.947489598901321e-05,
      "loss": 0.551,
      "step": 760
    },
    {
      "epoch": 0.09145979332462288,
      "grad_norm": 4.613224029541016,
      "learning_rate": 4.9454699680898334e-05,
      "loss": 0.4616,
      "step": 770
    },
    {
      "epoch": 0.09264758284831928,
      "grad_norm": 11.66037368774414,
      "learning_rate": 4.943450337278346e-05,
      "loss": 0.6176,
      "step": 780
    },
    {
      "epoch": 0.09383537237201568,
      "grad_norm": 8.434765815734863,
      "learning_rate": 4.941430706466858e-05,
      "loss": 0.4825,
      "step": 790
    },
    {
      "epoch": 0.09502316189571208,
      "grad_norm": 12.122628211975098,
      "learning_rate": 4.9394110756553704e-05,
      "loss": 0.5341,
      "step": 800
    },
    {
      "epoch": 0.09621095141940848,
      "grad_norm": 8.43961238861084,
      "learning_rate": 4.937391444843883e-05,
      "loss": 0.5171,
      "step": 810
    },
    {
      "epoch": 0.09739874094310488,
      "grad_norm": 7.21361780166626,
      "learning_rate": 4.9353718140323954e-05,
      "loss": 0.496,
      "step": 820
    },
    {
      "epoch": 0.09858653046680128,
      "grad_norm": 9.11520004272461,
      "learning_rate": 4.933352183220907e-05,
      "loss": 0.5466,
      "step": 830
    },
    {
      "epoch": 0.09977431999049768,
      "grad_norm": 6.771602153778076,
      "learning_rate": 4.93133255240942e-05,
      "loss": 0.4673,
      "step": 840
    },
    {
      "epoch": 0.10096210951419408,
      "grad_norm": 9.030261993408203,
      "learning_rate": 4.9293129215979324e-05,
      "loss": 0.5149,
      "step": 850
    },
    {
      "epoch": 0.10214989903789048,
      "grad_norm": 6.3041462898254395,
      "learning_rate": 4.927293290786444e-05,
      "loss": 0.446,
      "step": 860
    },
    {
      "epoch": 0.10333768856158689,
      "grad_norm": 8.052353858947754,
      "learning_rate": 4.925273659974957e-05,
      "loss": 0.4965,
      "step": 870
    },
    {
      "epoch": 0.10452547808528329,
      "grad_norm": 6.866906642913818,
      "learning_rate": 4.923254029163469e-05,
      "loss": 0.526,
      "step": 880
    },
    {
      "epoch": 0.10571326760897969,
      "grad_norm": 5.992532730102539,
      "learning_rate": 4.921234398351981e-05,
      "loss": 0.5709,
      "step": 890
    },
    {
      "epoch": 0.10690105713267609,
      "grad_norm": 9.69338607788086,
      "learning_rate": 4.919214767540494e-05,
      "loss": 0.4764,
      "step": 900
    },
    {
      "epoch": 0.10808884665637249,
      "grad_norm": 6.739308834075928,
      "learning_rate": 4.917195136729006e-05,
      "loss": 0.5039,
      "step": 910
    },
    {
      "epoch": 0.10927663618006889,
      "grad_norm": 6.0438408851623535,
      "learning_rate": 4.915175505917519e-05,
      "loss": 0.4738,
      "step": 920
    },
    {
      "epoch": 0.11046442570376529,
      "grad_norm": 14.130400657653809,
      "learning_rate": 4.913155875106031e-05,
      "loss": 0.4798,
      "step": 930
    },
    {
      "epoch": 0.11165221522746169,
      "grad_norm": 5.511341571807861,
      "learning_rate": 4.911136244294543e-05,
      "loss": 0.5008,
      "step": 940
    },
    {
      "epoch": 0.1128400047511581,
      "grad_norm": 13.602300643920898,
      "learning_rate": 4.909116613483056e-05,
      "loss": 0.5073,
      "step": 950
    },
    {
      "epoch": 0.1140277942748545,
      "grad_norm": 5.304278373718262,
      "learning_rate": 4.9070969826715676e-05,
      "loss": 0.4298,
      "step": 960
    },
    {
      "epoch": 0.1152155837985509,
      "grad_norm": 3.1956429481506348,
      "learning_rate": 4.90507735186008e-05,
      "loss": 0.442,
      "step": 970
    },
    {
      "epoch": 0.1164033733222473,
      "grad_norm": 14.201797485351562,
      "learning_rate": 4.903057721048592e-05,
      "loss": 0.5348,
      "step": 980
    },
    {
      "epoch": 0.1175911628459437,
      "grad_norm": 11.228254318237305,
      "learning_rate": 4.901038090237105e-05,
      "loss": 0.5108,
      "step": 990
    },
    {
      "epoch": 0.1187789523696401,
      "grad_norm": 11.285070419311523,
      "learning_rate": 4.899018459425617e-05,
      "loss": 0.5736,
      "step": 1000
    },
    {
      "epoch": 0.1199667418933365,
      "grad_norm": 4.430886745452881,
      "learning_rate": 4.8969988286141297e-05,
      "loss": 0.4655,
      "step": 1010
    },
    {
      "epoch": 0.1211545314170329,
      "grad_norm": 7.314514636993408,
      "learning_rate": 4.894979197802642e-05,
      "loss": 0.4665,
      "step": 1020
    },
    {
      "epoch": 0.1223423209407293,
      "grad_norm": 4.75882625579834,
      "learning_rate": 4.892959566991154e-05,
      "loss": 0.4779,
      "step": 1030
    },
    {
      "epoch": 0.1235301104644257,
      "grad_norm": 8.033220291137695,
      "learning_rate": 4.8909399361796666e-05,
      "loss": 0.5434,
      "step": 1040
    },
    {
      "epoch": 0.1247178999881221,
      "grad_norm": 12.560453414916992,
      "learning_rate": 4.8889203053681785e-05,
      "loss": 0.4601,
      "step": 1050
    },
    {
      "epoch": 0.12590568951181852,
      "grad_norm": 7.138411045074463,
      "learning_rate": 4.886900674556692e-05,
      "loss": 0.4404,
      "step": 1060
    },
    {
      "epoch": 0.1270934790355149,
      "grad_norm": 9.074617385864258,
      "learning_rate": 4.8848810437452035e-05,
      "loss": 0.5196,
      "step": 1070
    },
    {
      "epoch": 0.12828126855921132,
      "grad_norm": 9.30325984954834,
      "learning_rate": 4.882861412933716e-05,
      "loss": 0.3809,
      "step": 1080
    },
    {
      "epoch": 0.1294690580829077,
      "grad_norm": 9.668411254882812,
      "learning_rate": 4.8808417821222286e-05,
      "loss": 0.5434,
      "step": 1090
    },
    {
      "epoch": 0.13065684760660412,
      "grad_norm": 6.513296604156494,
      "learning_rate": 4.8788221513107405e-05,
      "loss": 0.4667,
      "step": 1100
    },
    {
      "epoch": 0.1318446371303005,
      "grad_norm": 12.994779586791992,
      "learning_rate": 4.876802520499253e-05,
      "loss": 0.5508,
      "step": 1110
    },
    {
      "epoch": 0.13303242665399692,
      "grad_norm": 6.9217848777771,
      "learning_rate": 4.874782889687765e-05,
      "loss": 0.4355,
      "step": 1120
    },
    {
      "epoch": 0.1342202161776933,
      "grad_norm": 6.177170753479004,
      "learning_rate": 4.872763258876278e-05,
      "loss": 0.4651,
      "step": 1130
    },
    {
      "epoch": 0.13540800570138972,
      "grad_norm": 9.289709091186523,
      "learning_rate": 4.87074362806479e-05,
      "loss": 0.528,
      "step": 1140
    },
    {
      "epoch": 0.1365957952250861,
      "grad_norm": 2.8235373497009277,
      "learning_rate": 4.8687239972533025e-05,
      "loss": 0.4242,
      "step": 1150
    },
    {
      "epoch": 0.13778358474878252,
      "grad_norm": 4.679508209228516,
      "learning_rate": 4.866704366441815e-05,
      "loss": 0.4134,
      "step": 1160
    },
    {
      "epoch": 0.1389713742724789,
      "grad_norm": 13.73006820678711,
      "learning_rate": 4.864684735630327e-05,
      "loss": 0.5172,
      "step": 1170
    },
    {
      "epoch": 0.14015916379617532,
      "grad_norm": 3.132460594177246,
      "learning_rate": 4.8626651048188395e-05,
      "loss": 0.49,
      "step": 1180
    },
    {
      "epoch": 0.1413469533198717,
      "grad_norm": 10.425761222839355,
      "learning_rate": 4.860645474007351e-05,
      "loss": 0.4173,
      "step": 1190
    },
    {
      "epoch": 0.14253474284356812,
      "grad_norm": 7.193592548370361,
      "learning_rate": 4.8586258431958645e-05,
      "loss": 0.4992,
      "step": 1200
    },
    {
      "epoch": 0.1437225323672645,
      "grad_norm": 2.1664464473724365,
      "learning_rate": 4.8566062123843764e-05,
      "loss": 0.4033,
      "step": 1210
    },
    {
      "epoch": 0.14491032189096092,
      "grad_norm": 13.453564643859863,
      "learning_rate": 4.854586581572888e-05,
      "loss": 0.5312,
      "step": 1220
    },
    {
      "epoch": 0.1460981114146573,
      "grad_norm": 13.485347747802734,
      "learning_rate": 4.852566950761401e-05,
      "loss": 0.4753,
      "step": 1230
    },
    {
      "epoch": 0.14728590093835373,
      "grad_norm": 13.25900936126709,
      "learning_rate": 4.8505473199499134e-05,
      "loss": 0.4944,
      "step": 1240
    },
    {
      "epoch": 0.1484736904620501,
      "grad_norm": 15.741289138793945,
      "learning_rate": 4.848527689138426e-05,
      "loss": 0.448,
      "step": 1250
    },
    {
      "epoch": 0.14966147998574653,
      "grad_norm": 5.608346462249756,
      "learning_rate": 4.846508058326938e-05,
      "loss": 0.4952,
      "step": 1260
    },
    {
      "epoch": 0.15084926950944294,
      "grad_norm": 7.130008697509766,
      "learning_rate": 4.84448842751545e-05,
      "loss": 0.4956,
      "step": 1270
    },
    {
      "epoch": 0.15203705903313933,
      "grad_norm": 9.327939987182617,
      "learning_rate": 4.842468796703963e-05,
      "loss": 0.4349,
      "step": 1280
    },
    {
      "epoch": 0.15322484855683574,
      "grad_norm": 3.7363126277923584,
      "learning_rate": 4.840449165892475e-05,
      "loss": 0.5283,
      "step": 1290
    },
    {
      "epoch": 0.15441263808053213,
      "grad_norm": 14.569894790649414,
      "learning_rate": 4.838429535080987e-05,
      "loss": 0.3612,
      "step": 1300
    },
    {
      "epoch": 0.15560042760422854,
      "grad_norm": 8.083725929260254,
      "learning_rate": 4.8364099042695e-05,
      "loss": 0.5067,
      "step": 1310
    },
    {
      "epoch": 0.15678821712792493,
      "grad_norm": 12.746700286865234,
      "learning_rate": 4.834390273458012e-05,
      "loss": 0.4559,
      "step": 1320
    },
    {
      "epoch": 0.15797600665162134,
      "grad_norm": 5.609346389770508,
      "learning_rate": 4.832370642646524e-05,
      "loss": 0.4844,
      "step": 1330
    },
    {
      "epoch": 0.15916379617531773,
      "grad_norm": 2.569969892501831,
      "learning_rate": 4.830351011835037e-05,
      "loss": 0.3494,
      "step": 1340
    },
    {
      "epoch": 0.16035158569901414,
      "grad_norm": 15.027949333190918,
      "learning_rate": 4.828331381023549e-05,
      "loss": 0.4443,
      "step": 1350
    },
    {
      "epoch": 0.16153937522271053,
      "grad_norm": 7.381288051605225,
      "learning_rate": 4.826311750212061e-05,
      "loss": 0.3741,
      "step": 1360
    },
    {
      "epoch": 0.16272716474640694,
      "grad_norm": 8.2087984085083,
      "learning_rate": 4.824292119400574e-05,
      "loss": 0.4046,
      "step": 1370
    },
    {
      "epoch": 0.16391495427010333,
      "grad_norm": 9.716196060180664,
      "learning_rate": 4.822272488589086e-05,
      "loss": 0.4667,
      "step": 1380
    },
    {
      "epoch": 0.16510274379379974,
      "grad_norm": 11.40641975402832,
      "learning_rate": 4.820252857777599e-05,
      "loss": 0.6322,
      "step": 1390
    },
    {
      "epoch": 0.16629053331749613,
      "grad_norm": 7.021626949310303,
      "learning_rate": 4.8182332269661106e-05,
      "loss": 0.4983,
      "step": 1400
    },
    {
      "epoch": 0.16747832284119255,
      "grad_norm": 9.936989784240723,
      "learning_rate": 4.816213596154623e-05,
      "loss": 0.3805,
      "step": 1410
    },
    {
      "epoch": 0.16866611236488893,
      "grad_norm": 7.261778831481934,
      "learning_rate": 4.814193965343136e-05,
      "loss": 0.3919,
      "step": 1420
    },
    {
      "epoch": 0.16985390188858535,
      "grad_norm": 7.500991344451904,
      "learning_rate": 4.8121743345316476e-05,
      "loss": 0.6191,
      "step": 1430
    },
    {
      "epoch": 0.17104169141228173,
      "grad_norm": 6.671547889709473,
      "learning_rate": 4.81015470372016e-05,
      "loss": 0.4216,
      "step": 1440
    },
    {
      "epoch": 0.17222948093597815,
      "grad_norm": 8.298477172851562,
      "learning_rate": 4.8081350729086726e-05,
      "loss": 0.5507,
      "step": 1450
    },
    {
      "epoch": 0.17341727045967453,
      "grad_norm": 3.7865095138549805,
      "learning_rate": 4.806115442097185e-05,
      "loss": 0.4875,
      "step": 1460
    },
    {
      "epoch": 0.17460505998337095,
      "grad_norm": 10.071208000183105,
      "learning_rate": 4.804095811285697e-05,
      "loss": 0.5597,
      "step": 1470
    },
    {
      "epoch": 0.17579284950706736,
      "grad_norm": 3.9404659271240234,
      "learning_rate": 4.8020761804742096e-05,
      "loss": 0.4932,
      "step": 1480
    },
    {
      "epoch": 0.17698063903076375,
      "grad_norm": 14.825315475463867,
      "learning_rate": 4.800056549662722e-05,
      "loss": 0.3966,
      "step": 1490
    },
    {
      "epoch": 0.17816842855446016,
      "grad_norm": 3.219564914703369,
      "learning_rate": 4.798036918851234e-05,
      "loss": 0.3166,
      "step": 1500
    }
  ],
  "logging_steps": 10,
  "max_steps": 25257,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3811461120000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
