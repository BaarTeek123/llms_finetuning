{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.4751158094785604,
  "eval_steps": 500,
  "global_step": 4000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001187789523696401,
      "grad_norm": 1.571284294128418,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.691,
      "step": 10
    },
    {
      "epoch": 0.002375579047392802,
      "grad_norm": 2.3712053298950195,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.7104,
      "step": 20
    },
    {
      "epoch": 0.003563368571089203,
      "grad_norm": 2.394946813583374,
      "learning_rate": 3e-06,
      "loss": 0.7076,
      "step": 30
    },
    {
      "epoch": 0.004751158094785604,
      "grad_norm": 3.6025919914245605,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.714,
      "step": 40
    },
    {
      "epoch": 0.005938947618482005,
      "grad_norm": 1.937501311302185,
      "learning_rate": 5e-06,
      "loss": 0.6987,
      "step": 50
    },
    {
      "epoch": 0.007126737142178406,
      "grad_norm": 3.3844287395477295,
      "learning_rate": 6e-06,
      "loss": 0.7021,
      "step": 60
    },
    {
      "epoch": 0.008314526665874808,
      "grad_norm": 3.468790054321289,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.699,
      "step": 70
    },
    {
      "epoch": 0.009502316189571208,
      "grad_norm": 1.6214921474456787,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.6835,
      "step": 80
    },
    {
      "epoch": 0.010690105713267608,
      "grad_norm": 1.8596107959747314,
      "learning_rate": 9e-06,
      "loss": 0.6895,
      "step": 90
    },
    {
      "epoch": 0.01187789523696401,
      "grad_norm": 2.3269872665405273,
      "learning_rate": 1e-05,
      "loss": 0.6835,
      "step": 100
    },
    {
      "epoch": 0.01306568476066041,
      "grad_norm": 4.275064945220947,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.6999,
      "step": 110
    },
    {
      "epoch": 0.014253474284356813,
      "grad_norm": 1.7505264282226562,
      "learning_rate": 1.2e-05,
      "loss": 0.6941,
      "step": 120
    },
    {
      "epoch": 0.015441263808053213,
      "grad_norm": 1.5032916069030762,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.6868,
      "step": 130
    },
    {
      "epoch": 0.016629053331749615,
      "grad_norm": 3.410565137863159,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.703,
      "step": 140
    },
    {
      "epoch": 0.017816842855446016,
      "grad_norm": 2.50521183013916,
      "learning_rate": 1.5e-05,
      "loss": 0.7025,
      "step": 150
    },
    {
      "epoch": 0.019004632379142416,
      "grad_norm": 3.892134428024292,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.6794,
      "step": 160
    },
    {
      "epoch": 0.020192421902838816,
      "grad_norm": 1.7042125463485718,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.6917,
      "step": 170
    },
    {
      "epoch": 0.021380211426535217,
      "grad_norm": 4.2205915451049805,
      "learning_rate": 1.8e-05,
      "loss": 0.6971,
      "step": 180
    },
    {
      "epoch": 0.02256800095023162,
      "grad_norm": 1.5046566724777222,
      "learning_rate": 1.9e-05,
      "loss": 0.6874,
      "step": 190
    },
    {
      "epoch": 0.02375579047392802,
      "grad_norm": 1.69389009475708,
      "learning_rate": 2e-05,
      "loss": 0.6692,
      "step": 200
    },
    {
      "epoch": 0.02494357999762442,
      "grad_norm": 1.7059226036071777,
      "learning_rate": 2.1e-05,
      "loss": 0.6717,
      "step": 210
    },
    {
      "epoch": 0.02613136952132082,
      "grad_norm": 1.818788766860962,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.6601,
      "step": 220
    },
    {
      "epoch": 0.02731915904501722,
      "grad_norm": 2.0799472332000732,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.6629,
      "step": 230
    },
    {
      "epoch": 0.028506948568713626,
      "grad_norm": 1.823838472366333,
      "learning_rate": 2.4e-05,
      "loss": 0.6681,
      "step": 240
    },
    {
      "epoch": 0.029694738092410026,
      "grad_norm": 2.8223283290863037,
      "learning_rate": 2.5e-05,
      "loss": 0.6993,
      "step": 250
    },
    {
      "epoch": 0.030882527616106426,
      "grad_norm": 2.522125244140625,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.7037,
      "step": 260
    },
    {
      "epoch": 0.03207031713980283,
      "grad_norm": 1.8430627584457397,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.6947,
      "step": 270
    },
    {
      "epoch": 0.03325810666349923,
      "grad_norm": 3.059164524078369,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.6869,
      "step": 280
    },
    {
      "epoch": 0.03444589618719563,
      "grad_norm": 1.9441590309143066,
      "learning_rate": 2.9e-05,
      "loss": 0.6886,
      "step": 290
    },
    {
      "epoch": 0.03563368571089203,
      "grad_norm": 1.770355224609375,
      "learning_rate": 3e-05,
      "loss": 0.6928,
      "step": 300
    },
    {
      "epoch": 0.03682147523458843,
      "grad_norm": 1.7763140201568604,
      "learning_rate": 3.1e-05,
      "loss": 0.6743,
      "step": 310
    },
    {
      "epoch": 0.03800926475828483,
      "grad_norm": 1.8267428874969482,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.6672,
      "step": 320
    },
    {
      "epoch": 0.03919705428198123,
      "grad_norm": 1.6273711919784546,
      "learning_rate": 3.3e-05,
      "loss": 0.6543,
      "step": 330
    },
    {
      "epoch": 0.04038484380567763,
      "grad_norm": 4.313201904296875,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.6706,
      "step": 340
    },
    {
      "epoch": 0.04157263332937403,
      "grad_norm": 4.140109062194824,
      "learning_rate": 3.5e-05,
      "loss": 0.6651,
      "step": 350
    },
    {
      "epoch": 0.04276042285307043,
      "grad_norm": 2.572695255279541,
      "learning_rate": 3.6e-05,
      "loss": 0.67,
      "step": 360
    },
    {
      "epoch": 0.04394821237676684,
      "grad_norm": 2.187011957168579,
      "learning_rate": 3.7e-05,
      "loss": 0.6432,
      "step": 370
    },
    {
      "epoch": 0.04513600190046324,
      "grad_norm": 4.77373743057251,
      "learning_rate": 3.8e-05,
      "loss": 0.671,
      "step": 380
    },
    {
      "epoch": 0.04632379142415964,
      "grad_norm": 2.0730113983154297,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.6803,
      "step": 390
    },
    {
      "epoch": 0.04751158094785604,
      "grad_norm": 3.850599765777588,
      "learning_rate": 4e-05,
      "loss": 0.6695,
      "step": 400
    },
    {
      "epoch": 0.04869937047155244,
      "grad_norm": 3.2260897159576416,
      "learning_rate": 4.1e-05,
      "loss": 0.6611,
      "step": 410
    },
    {
      "epoch": 0.04988715999524884,
      "grad_norm": 2.1476058959960938,
      "learning_rate": 4.2e-05,
      "loss": 0.6543,
      "step": 420
    },
    {
      "epoch": 0.05107494951894524,
      "grad_norm": 5.54133415222168,
      "learning_rate": 4.3e-05,
      "loss": 0.6637,
      "step": 430
    },
    {
      "epoch": 0.05226273904264164,
      "grad_norm": 3.140399932861328,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.6532,
      "step": 440
    },
    {
      "epoch": 0.05345052856633804,
      "grad_norm": 3.0184481143951416,
      "learning_rate": 4.5e-05,
      "loss": 0.613,
      "step": 450
    },
    {
      "epoch": 0.05463831809003444,
      "grad_norm": 3.482469320297241,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.6442,
      "step": 460
    },
    {
      "epoch": 0.055826107613730844,
      "grad_norm": 4.6692423820495605,
      "learning_rate": 4.7e-05,
      "loss": 0.6508,
      "step": 470
    },
    {
      "epoch": 0.05701389713742725,
      "grad_norm": 5.267462253570557,
      "learning_rate": 4.8e-05,
      "loss": 0.6357,
      "step": 480
    },
    {
      "epoch": 0.05820168666112365,
      "grad_norm": 5.792267322540283,
      "learning_rate": 4.9e-05,
      "loss": 0.6523,
      "step": 490
    },
    {
      "epoch": 0.05938947618482005,
      "grad_norm": 3.983430862426758,
      "learning_rate": 5e-05,
      "loss": 0.639,
      "step": 500
    },
    {
      "epoch": 0.06057726570851645,
      "grad_norm": 5.588817596435547,
      "learning_rate": 4.997980369188512e-05,
      "loss": 0.64,
      "step": 510
    },
    {
      "epoch": 0.06176505523221285,
      "grad_norm": 7.067869186401367,
      "learning_rate": 4.995960738377025e-05,
      "loss": 0.633,
      "step": 520
    },
    {
      "epoch": 0.06295284475590926,
      "grad_norm": 3.3713228702545166,
      "learning_rate": 4.993941107565537e-05,
      "loss": 0.6247,
      "step": 530
    },
    {
      "epoch": 0.06414063427960566,
      "grad_norm": 6.371031761169434,
      "learning_rate": 4.99192147675405e-05,
      "loss": 0.6461,
      "step": 540
    },
    {
      "epoch": 0.06532842380330206,
      "grad_norm": 2.8868744373321533,
      "learning_rate": 4.9899018459425616e-05,
      "loss": 0.5931,
      "step": 550
    },
    {
      "epoch": 0.06651621332699846,
      "grad_norm": 4.016470909118652,
      "learning_rate": 4.987882215131074e-05,
      "loss": 0.6165,
      "step": 560
    },
    {
      "epoch": 0.06770400285069486,
      "grad_norm": 4.354010105133057,
      "learning_rate": 4.985862584319587e-05,
      "loss": 0.6168,
      "step": 570
    },
    {
      "epoch": 0.06889179237439126,
      "grad_norm": 4.343535423278809,
      "learning_rate": 4.9838429535080985e-05,
      "loss": 0.6168,
      "step": 580
    },
    {
      "epoch": 0.07007958189808766,
      "grad_norm": 5.812617301940918,
      "learning_rate": 4.981823322696612e-05,
      "loss": 0.6248,
      "step": 590
    },
    {
      "epoch": 0.07126737142178406,
      "grad_norm": 5.959688663482666,
      "learning_rate": 4.9798036918851236e-05,
      "loss": 0.6154,
      "step": 600
    },
    {
      "epoch": 0.07245516094548046,
      "grad_norm": 4.963962554931641,
      "learning_rate": 4.977784061073636e-05,
      "loss": 0.6111,
      "step": 610
    },
    {
      "epoch": 0.07364295046917686,
      "grad_norm": 6.3539605140686035,
      "learning_rate": 4.975764430262148e-05,
      "loss": 0.6219,
      "step": 620
    },
    {
      "epoch": 0.07483073999287326,
      "grad_norm": 4.269138813018799,
      "learning_rate": 4.9737447994506606e-05,
      "loss": 0.5873,
      "step": 630
    },
    {
      "epoch": 0.07601852951656966,
      "grad_norm": 5.565948963165283,
      "learning_rate": 4.971725168639173e-05,
      "loss": 0.5671,
      "step": 640
    },
    {
      "epoch": 0.07720631904026606,
      "grad_norm": 9.216350555419922,
      "learning_rate": 4.969705537827685e-05,
      "loss": 0.5477,
      "step": 650
    },
    {
      "epoch": 0.07839410856396246,
      "grad_norm": 2.9129035472869873,
      "learning_rate": 4.9676859070161975e-05,
      "loss": 0.5838,
      "step": 660
    },
    {
      "epoch": 0.07958189808765886,
      "grad_norm": 13.75711727142334,
      "learning_rate": 4.96566627620471e-05,
      "loss": 0.5386,
      "step": 670
    },
    {
      "epoch": 0.08076968761135526,
      "grad_norm": 4.186140537261963,
      "learning_rate": 4.9636466453932226e-05,
      "loss": 0.6116,
      "step": 680
    },
    {
      "epoch": 0.08195747713505167,
      "grad_norm": 2.5830237865448,
      "learning_rate": 4.9616270145817344e-05,
      "loss": 0.54,
      "step": 690
    },
    {
      "epoch": 0.08314526665874807,
      "grad_norm": 8.397902488708496,
      "learning_rate": 4.959607383770247e-05,
      "loss": 0.6059,
      "step": 700
    },
    {
      "epoch": 0.08433305618244447,
      "grad_norm": 8.332036972045898,
      "learning_rate": 4.9575877529587595e-05,
      "loss": 0.5963,
      "step": 710
    },
    {
      "epoch": 0.08552084570614087,
      "grad_norm": 4.008006572723389,
      "learning_rate": 4.9555681221472714e-05,
      "loss": 0.4987,
      "step": 720
    },
    {
      "epoch": 0.08670863522983727,
      "grad_norm": 8.097256660461426,
      "learning_rate": 4.953548491335784e-05,
      "loss": 0.5773,
      "step": 730
    },
    {
      "epoch": 0.08789642475353368,
      "grad_norm": 6.3633551597595215,
      "learning_rate": 4.9515288605242965e-05,
      "loss": 0.5049,
      "step": 740
    },
    {
      "epoch": 0.08908421427723008,
      "grad_norm": 6.642234802246094,
      "learning_rate": 4.949509229712809e-05,
      "loss": 0.4989,
      "step": 750
    },
    {
      "epoch": 0.09027200380092648,
      "grad_norm": 6.825439453125,
      "learning_rate": 4.947489598901321e-05,
      "loss": 0.551,
      "step": 760
    },
    {
      "epoch": 0.09145979332462288,
      "grad_norm": 4.613224029541016,
      "learning_rate": 4.9454699680898334e-05,
      "loss": 0.4616,
      "step": 770
    },
    {
      "epoch": 0.09264758284831928,
      "grad_norm": 11.66037368774414,
      "learning_rate": 4.943450337278346e-05,
      "loss": 0.6176,
      "step": 780
    },
    {
      "epoch": 0.09383537237201568,
      "grad_norm": 8.434765815734863,
      "learning_rate": 4.941430706466858e-05,
      "loss": 0.4825,
      "step": 790
    },
    {
      "epoch": 0.09502316189571208,
      "grad_norm": 12.122628211975098,
      "learning_rate": 4.9394110756553704e-05,
      "loss": 0.5341,
      "step": 800
    },
    {
      "epoch": 0.09621095141940848,
      "grad_norm": 8.43961238861084,
      "learning_rate": 4.937391444843883e-05,
      "loss": 0.5171,
      "step": 810
    },
    {
      "epoch": 0.09739874094310488,
      "grad_norm": 7.21361780166626,
      "learning_rate": 4.9353718140323954e-05,
      "loss": 0.496,
      "step": 820
    },
    {
      "epoch": 0.09858653046680128,
      "grad_norm": 9.11520004272461,
      "learning_rate": 4.933352183220907e-05,
      "loss": 0.5466,
      "step": 830
    },
    {
      "epoch": 0.09977431999049768,
      "grad_norm": 6.771602153778076,
      "learning_rate": 4.93133255240942e-05,
      "loss": 0.4673,
      "step": 840
    },
    {
      "epoch": 0.10096210951419408,
      "grad_norm": 9.030261993408203,
      "learning_rate": 4.9293129215979324e-05,
      "loss": 0.5149,
      "step": 850
    },
    {
      "epoch": 0.10214989903789048,
      "grad_norm": 6.3041462898254395,
      "learning_rate": 4.927293290786444e-05,
      "loss": 0.446,
      "step": 860
    },
    {
      "epoch": 0.10333768856158689,
      "grad_norm": 8.052353858947754,
      "learning_rate": 4.925273659974957e-05,
      "loss": 0.4965,
      "step": 870
    },
    {
      "epoch": 0.10452547808528329,
      "grad_norm": 6.866906642913818,
      "learning_rate": 4.923254029163469e-05,
      "loss": 0.526,
      "step": 880
    },
    {
      "epoch": 0.10571326760897969,
      "grad_norm": 5.992532730102539,
      "learning_rate": 4.921234398351981e-05,
      "loss": 0.5709,
      "step": 890
    },
    {
      "epoch": 0.10690105713267609,
      "grad_norm": 9.69338607788086,
      "learning_rate": 4.919214767540494e-05,
      "loss": 0.4764,
      "step": 900
    },
    {
      "epoch": 0.10808884665637249,
      "grad_norm": 6.739308834075928,
      "learning_rate": 4.917195136729006e-05,
      "loss": 0.5039,
      "step": 910
    },
    {
      "epoch": 0.10927663618006889,
      "grad_norm": 6.0438408851623535,
      "learning_rate": 4.915175505917519e-05,
      "loss": 0.4738,
      "step": 920
    },
    {
      "epoch": 0.11046442570376529,
      "grad_norm": 14.130400657653809,
      "learning_rate": 4.913155875106031e-05,
      "loss": 0.4798,
      "step": 930
    },
    {
      "epoch": 0.11165221522746169,
      "grad_norm": 5.511341571807861,
      "learning_rate": 4.911136244294543e-05,
      "loss": 0.5008,
      "step": 940
    },
    {
      "epoch": 0.1128400047511581,
      "grad_norm": 13.602300643920898,
      "learning_rate": 4.909116613483056e-05,
      "loss": 0.5073,
      "step": 950
    },
    {
      "epoch": 0.1140277942748545,
      "grad_norm": 5.304278373718262,
      "learning_rate": 4.9070969826715676e-05,
      "loss": 0.4298,
      "step": 960
    },
    {
      "epoch": 0.1152155837985509,
      "grad_norm": 3.1956429481506348,
      "learning_rate": 4.90507735186008e-05,
      "loss": 0.442,
      "step": 970
    },
    {
      "epoch": 0.1164033733222473,
      "grad_norm": 14.201797485351562,
      "learning_rate": 4.903057721048592e-05,
      "loss": 0.5348,
      "step": 980
    },
    {
      "epoch": 0.1175911628459437,
      "grad_norm": 11.228254318237305,
      "learning_rate": 4.901038090237105e-05,
      "loss": 0.5108,
      "step": 990
    },
    {
      "epoch": 0.1187789523696401,
      "grad_norm": 11.285070419311523,
      "learning_rate": 4.899018459425617e-05,
      "loss": 0.5736,
      "step": 1000
    },
    {
      "epoch": 0.1199667418933365,
      "grad_norm": 4.430886745452881,
      "learning_rate": 4.8969988286141297e-05,
      "loss": 0.4655,
      "step": 1010
    },
    {
      "epoch": 0.1211545314170329,
      "grad_norm": 7.314514636993408,
      "learning_rate": 4.894979197802642e-05,
      "loss": 0.4665,
      "step": 1020
    },
    {
      "epoch": 0.1223423209407293,
      "grad_norm": 4.75882625579834,
      "learning_rate": 4.892959566991154e-05,
      "loss": 0.4779,
      "step": 1030
    },
    {
      "epoch": 0.1235301104644257,
      "grad_norm": 8.033220291137695,
      "learning_rate": 4.8909399361796666e-05,
      "loss": 0.5434,
      "step": 1040
    },
    {
      "epoch": 0.1247178999881221,
      "grad_norm": 12.560453414916992,
      "learning_rate": 4.8889203053681785e-05,
      "loss": 0.4601,
      "step": 1050
    },
    {
      "epoch": 0.12590568951181852,
      "grad_norm": 7.138411045074463,
      "learning_rate": 4.886900674556692e-05,
      "loss": 0.4404,
      "step": 1060
    },
    {
      "epoch": 0.1270934790355149,
      "grad_norm": 9.074617385864258,
      "learning_rate": 4.8848810437452035e-05,
      "loss": 0.5196,
      "step": 1070
    },
    {
      "epoch": 0.12828126855921132,
      "grad_norm": 9.30325984954834,
      "learning_rate": 4.882861412933716e-05,
      "loss": 0.3809,
      "step": 1080
    },
    {
      "epoch": 0.1294690580829077,
      "grad_norm": 9.668411254882812,
      "learning_rate": 4.8808417821222286e-05,
      "loss": 0.5434,
      "step": 1090
    },
    {
      "epoch": 0.13065684760660412,
      "grad_norm": 6.513296604156494,
      "learning_rate": 4.8788221513107405e-05,
      "loss": 0.4667,
      "step": 1100
    },
    {
      "epoch": 0.1318446371303005,
      "grad_norm": 12.994779586791992,
      "learning_rate": 4.876802520499253e-05,
      "loss": 0.5508,
      "step": 1110
    },
    {
      "epoch": 0.13303242665399692,
      "grad_norm": 6.9217848777771,
      "learning_rate": 4.874782889687765e-05,
      "loss": 0.4355,
      "step": 1120
    },
    {
      "epoch": 0.1342202161776933,
      "grad_norm": 6.177170753479004,
      "learning_rate": 4.872763258876278e-05,
      "loss": 0.4651,
      "step": 1130
    },
    {
      "epoch": 0.13540800570138972,
      "grad_norm": 9.289709091186523,
      "learning_rate": 4.87074362806479e-05,
      "loss": 0.528,
      "step": 1140
    },
    {
      "epoch": 0.1365957952250861,
      "grad_norm": 2.8235373497009277,
      "learning_rate": 4.8687239972533025e-05,
      "loss": 0.4242,
      "step": 1150
    },
    {
      "epoch": 0.13778358474878252,
      "grad_norm": 4.679508209228516,
      "learning_rate": 4.866704366441815e-05,
      "loss": 0.4134,
      "step": 1160
    },
    {
      "epoch": 0.1389713742724789,
      "grad_norm": 13.73006820678711,
      "learning_rate": 4.864684735630327e-05,
      "loss": 0.5172,
      "step": 1170
    },
    {
      "epoch": 0.14015916379617532,
      "grad_norm": 3.132460594177246,
      "learning_rate": 4.8626651048188395e-05,
      "loss": 0.49,
      "step": 1180
    },
    {
      "epoch": 0.1413469533198717,
      "grad_norm": 10.425761222839355,
      "learning_rate": 4.860645474007351e-05,
      "loss": 0.4173,
      "step": 1190
    },
    {
      "epoch": 0.14253474284356812,
      "grad_norm": 7.193592548370361,
      "learning_rate": 4.8586258431958645e-05,
      "loss": 0.4992,
      "step": 1200
    },
    {
      "epoch": 0.1437225323672645,
      "grad_norm": 2.1664464473724365,
      "learning_rate": 4.8566062123843764e-05,
      "loss": 0.4033,
      "step": 1210
    },
    {
      "epoch": 0.14491032189096092,
      "grad_norm": 13.453564643859863,
      "learning_rate": 4.854586581572888e-05,
      "loss": 0.5312,
      "step": 1220
    },
    {
      "epoch": 0.1460981114146573,
      "grad_norm": 13.485347747802734,
      "learning_rate": 4.852566950761401e-05,
      "loss": 0.4753,
      "step": 1230
    },
    {
      "epoch": 0.14728590093835373,
      "grad_norm": 13.25900936126709,
      "learning_rate": 4.8505473199499134e-05,
      "loss": 0.4944,
      "step": 1240
    },
    {
      "epoch": 0.1484736904620501,
      "grad_norm": 15.741289138793945,
      "learning_rate": 4.848527689138426e-05,
      "loss": 0.448,
      "step": 1250
    },
    {
      "epoch": 0.14966147998574653,
      "grad_norm": 5.608346462249756,
      "learning_rate": 4.846508058326938e-05,
      "loss": 0.4952,
      "step": 1260
    },
    {
      "epoch": 0.15084926950944294,
      "grad_norm": 7.130008697509766,
      "learning_rate": 4.84448842751545e-05,
      "loss": 0.4956,
      "step": 1270
    },
    {
      "epoch": 0.15203705903313933,
      "grad_norm": 9.327939987182617,
      "learning_rate": 4.842468796703963e-05,
      "loss": 0.4349,
      "step": 1280
    },
    {
      "epoch": 0.15322484855683574,
      "grad_norm": 3.7363126277923584,
      "learning_rate": 4.840449165892475e-05,
      "loss": 0.5283,
      "step": 1290
    },
    {
      "epoch": 0.15441263808053213,
      "grad_norm": 14.569894790649414,
      "learning_rate": 4.838429535080987e-05,
      "loss": 0.3612,
      "step": 1300
    },
    {
      "epoch": 0.15560042760422854,
      "grad_norm": 8.083725929260254,
      "learning_rate": 4.8364099042695e-05,
      "loss": 0.5067,
      "step": 1310
    },
    {
      "epoch": 0.15678821712792493,
      "grad_norm": 12.746700286865234,
      "learning_rate": 4.834390273458012e-05,
      "loss": 0.4559,
      "step": 1320
    },
    {
      "epoch": 0.15797600665162134,
      "grad_norm": 5.609346389770508,
      "learning_rate": 4.832370642646524e-05,
      "loss": 0.4844,
      "step": 1330
    },
    {
      "epoch": 0.15916379617531773,
      "grad_norm": 2.569969892501831,
      "learning_rate": 4.830351011835037e-05,
      "loss": 0.3494,
      "step": 1340
    },
    {
      "epoch": 0.16035158569901414,
      "grad_norm": 15.027949333190918,
      "learning_rate": 4.828331381023549e-05,
      "loss": 0.4443,
      "step": 1350
    },
    {
      "epoch": 0.16153937522271053,
      "grad_norm": 7.381288051605225,
      "learning_rate": 4.826311750212061e-05,
      "loss": 0.3741,
      "step": 1360
    },
    {
      "epoch": 0.16272716474640694,
      "grad_norm": 8.2087984085083,
      "learning_rate": 4.824292119400574e-05,
      "loss": 0.4046,
      "step": 1370
    },
    {
      "epoch": 0.16391495427010333,
      "grad_norm": 9.716196060180664,
      "learning_rate": 4.822272488589086e-05,
      "loss": 0.4667,
      "step": 1380
    },
    {
      "epoch": 0.16510274379379974,
      "grad_norm": 11.40641975402832,
      "learning_rate": 4.820252857777599e-05,
      "loss": 0.6322,
      "step": 1390
    },
    {
      "epoch": 0.16629053331749613,
      "grad_norm": 7.021626949310303,
      "learning_rate": 4.8182332269661106e-05,
      "loss": 0.4983,
      "step": 1400
    },
    {
      "epoch": 0.16747832284119255,
      "grad_norm": 9.936989784240723,
      "learning_rate": 4.816213596154623e-05,
      "loss": 0.3805,
      "step": 1410
    },
    {
      "epoch": 0.16866611236488893,
      "grad_norm": 7.261778831481934,
      "learning_rate": 4.814193965343136e-05,
      "loss": 0.3919,
      "step": 1420
    },
    {
      "epoch": 0.16985390188858535,
      "grad_norm": 7.500991344451904,
      "learning_rate": 4.8121743345316476e-05,
      "loss": 0.6191,
      "step": 1430
    },
    {
      "epoch": 0.17104169141228173,
      "grad_norm": 6.671547889709473,
      "learning_rate": 4.81015470372016e-05,
      "loss": 0.4216,
      "step": 1440
    },
    {
      "epoch": 0.17222948093597815,
      "grad_norm": 8.298477172851562,
      "learning_rate": 4.8081350729086726e-05,
      "loss": 0.5507,
      "step": 1450
    },
    {
      "epoch": 0.17341727045967453,
      "grad_norm": 3.7865095138549805,
      "learning_rate": 4.806115442097185e-05,
      "loss": 0.4875,
      "step": 1460
    },
    {
      "epoch": 0.17460505998337095,
      "grad_norm": 10.071208000183105,
      "learning_rate": 4.804095811285697e-05,
      "loss": 0.5597,
      "step": 1470
    },
    {
      "epoch": 0.17579284950706736,
      "grad_norm": 3.9404659271240234,
      "learning_rate": 4.8020761804742096e-05,
      "loss": 0.4932,
      "step": 1480
    },
    {
      "epoch": 0.17698063903076375,
      "grad_norm": 14.825315475463867,
      "learning_rate": 4.800056549662722e-05,
      "loss": 0.3966,
      "step": 1490
    },
    {
      "epoch": 0.17816842855446016,
      "grad_norm": 3.219564914703369,
      "learning_rate": 4.798036918851234e-05,
      "loss": 0.3166,
      "step": 1500
    },
    {
      "epoch": 0.17935621807815655,
      "grad_norm": 8.211339950561523,
      "learning_rate": 4.7960172880397465e-05,
      "loss": 0.4569,
      "step": 1510
    },
    {
      "epoch": 0.18054400760185296,
      "grad_norm": 28.574251174926758,
      "learning_rate": 4.793997657228259e-05,
      "loss": 0.4082,
      "step": 1520
    },
    {
      "epoch": 0.18173179712554935,
      "grad_norm": 9.549306869506836,
      "learning_rate": 4.7919780264167716e-05,
      "loss": 0.4462,
      "step": 1530
    },
    {
      "epoch": 0.18291958664924576,
      "grad_norm": 10.74986743927002,
      "learning_rate": 4.7899583956052835e-05,
      "loss": 0.5763,
      "step": 1540
    },
    {
      "epoch": 0.18410737617294215,
      "grad_norm": 18.05826187133789,
      "learning_rate": 4.7879387647937953e-05,
      "loss": 0.3571,
      "step": 1550
    },
    {
      "epoch": 0.18529516569663856,
      "grad_norm": 16.856159210205078,
      "learning_rate": 4.7859191339823086e-05,
      "loss": 0.5069,
      "step": 1560
    },
    {
      "epoch": 0.18648295522033495,
      "grad_norm": 9.720256805419922,
      "learning_rate": 4.7838995031708204e-05,
      "loss": 0.4871,
      "step": 1570
    },
    {
      "epoch": 0.18767074474403136,
      "grad_norm": 9.31482982635498,
      "learning_rate": 4.781879872359333e-05,
      "loss": 0.4513,
      "step": 1580
    },
    {
      "epoch": 0.18885853426772775,
      "grad_norm": 15.629776954650879,
      "learning_rate": 4.7798602415478455e-05,
      "loss": 0.4253,
      "step": 1590
    },
    {
      "epoch": 0.19004632379142417,
      "grad_norm": 13.192623138427734,
      "learning_rate": 4.7778406107363574e-05,
      "loss": 0.4643,
      "step": 1600
    },
    {
      "epoch": 0.19123411331512055,
      "grad_norm": 6.096563339233398,
      "learning_rate": 4.77582097992487e-05,
      "loss": 0.4526,
      "step": 1610
    },
    {
      "epoch": 0.19242190283881697,
      "grad_norm": 4.842116355895996,
      "learning_rate": 4.773801349113382e-05,
      "loss": 0.3882,
      "step": 1620
    },
    {
      "epoch": 0.19360969236251335,
      "grad_norm": 12.17288875579834,
      "learning_rate": 4.771781718301895e-05,
      "loss": 0.4722,
      "step": 1630
    },
    {
      "epoch": 0.19479748188620977,
      "grad_norm": 10.484006881713867,
      "learning_rate": 4.769762087490407e-05,
      "loss": 0.4494,
      "step": 1640
    },
    {
      "epoch": 0.19598527140990615,
      "grad_norm": 18.098337173461914,
      "learning_rate": 4.7677424566789194e-05,
      "loss": 0.3344,
      "step": 1650
    },
    {
      "epoch": 0.19717306093360257,
      "grad_norm": 3.5647261142730713,
      "learning_rate": 4.765722825867432e-05,
      "loss": 0.4355,
      "step": 1660
    },
    {
      "epoch": 0.19836085045729895,
      "grad_norm": 8.591007232666016,
      "learning_rate": 4.763703195055944e-05,
      "loss": 0.4566,
      "step": 1670
    },
    {
      "epoch": 0.19954863998099537,
      "grad_norm": 17.269392013549805,
      "learning_rate": 4.7616835642444563e-05,
      "loss": 0.3974,
      "step": 1680
    },
    {
      "epoch": 0.20073642950469178,
      "grad_norm": 15.745661735534668,
      "learning_rate": 4.759663933432968e-05,
      "loss": 0.4178,
      "step": 1690
    },
    {
      "epoch": 0.20192421902838817,
      "grad_norm": 11.832619667053223,
      "learning_rate": 4.7576443026214814e-05,
      "loss": 0.4294,
      "step": 1700
    },
    {
      "epoch": 0.20311200855208458,
      "grad_norm": 9.936813354492188,
      "learning_rate": 4.755624671809993e-05,
      "loss": 0.4479,
      "step": 1710
    },
    {
      "epoch": 0.20429979807578097,
      "grad_norm": 18.938295364379883,
      "learning_rate": 4.753605040998506e-05,
      "loss": 0.4331,
      "step": 1720
    },
    {
      "epoch": 0.20548758759947738,
      "grad_norm": 1.5995380878448486,
      "learning_rate": 4.7515854101870184e-05,
      "loss": 0.3973,
      "step": 1730
    },
    {
      "epoch": 0.20667537712317377,
      "grad_norm": 18.9797420501709,
      "learning_rate": 4.74956577937553e-05,
      "loss": 0.611,
      "step": 1740
    },
    {
      "epoch": 0.20786316664687018,
      "grad_norm": 8.75420093536377,
      "learning_rate": 4.747546148564043e-05,
      "loss": 0.3798,
      "step": 1750
    },
    {
      "epoch": 0.20905095617056657,
      "grad_norm": 6.259380340576172,
      "learning_rate": 4.7455265177525546e-05,
      "loss": 0.5,
      "step": 1760
    },
    {
      "epoch": 0.21023874569426299,
      "grad_norm": 10.076289176940918,
      "learning_rate": 4.743506886941068e-05,
      "loss": 0.493,
      "step": 1770
    },
    {
      "epoch": 0.21142653521795937,
      "grad_norm": 24.3170166015625,
      "learning_rate": 4.74148725612958e-05,
      "loss": 0.5387,
      "step": 1780
    },
    {
      "epoch": 0.21261432474165579,
      "grad_norm": 8.668848991394043,
      "learning_rate": 4.739467625318092e-05,
      "loss": 0.5339,
      "step": 1790
    },
    {
      "epoch": 0.21380211426535217,
      "grad_norm": 31.958791732788086,
      "learning_rate": 4.737447994506604e-05,
      "loss": 0.4081,
      "step": 1800
    },
    {
      "epoch": 0.2149899037890486,
      "grad_norm": 12.881916999816895,
      "learning_rate": 4.735428363695117e-05,
      "loss": 0.3624,
      "step": 1810
    },
    {
      "epoch": 0.21617769331274497,
      "grad_norm": 10.048140525817871,
      "learning_rate": 4.733408732883629e-05,
      "loss": 0.3082,
      "step": 1820
    },
    {
      "epoch": 0.2173654828364414,
      "grad_norm": 9.223479270935059,
      "learning_rate": 4.731389102072141e-05,
      "loss": 0.4794,
      "step": 1830
    },
    {
      "epoch": 0.21855327236013777,
      "grad_norm": 6.066252708435059,
      "learning_rate": 4.729369471260654e-05,
      "loss": 0.5793,
      "step": 1840
    },
    {
      "epoch": 0.2197410618838342,
      "grad_norm": 18.52729606628418,
      "learning_rate": 4.727349840449166e-05,
      "loss": 0.5252,
      "step": 1850
    },
    {
      "epoch": 0.22092885140753057,
      "grad_norm": 1.8465696573257446,
      "learning_rate": 4.725330209637679e-05,
      "loss": 0.3994,
      "step": 1860
    },
    {
      "epoch": 0.222116640931227,
      "grad_norm": 6.187740802764893,
      "learning_rate": 4.7233105788261906e-05,
      "loss": 0.4273,
      "step": 1870
    },
    {
      "epoch": 0.22330443045492337,
      "grad_norm": 10.340858459472656,
      "learning_rate": 4.721290948014703e-05,
      "loss": 0.322,
      "step": 1880
    },
    {
      "epoch": 0.2244922199786198,
      "grad_norm": 30.44989776611328,
      "learning_rate": 4.7192713172032156e-05,
      "loss": 0.4524,
      "step": 1890
    },
    {
      "epoch": 0.2256800095023162,
      "grad_norm": 19.412546157836914,
      "learning_rate": 4.7172516863917275e-05,
      "loss": 0.4389,
      "step": 1900
    },
    {
      "epoch": 0.2268677990260126,
      "grad_norm": 10.495121002197266,
      "learning_rate": 4.715232055580241e-05,
      "loss": 0.4439,
      "step": 1910
    },
    {
      "epoch": 0.228055588549709,
      "grad_norm": 6.538065433502197,
      "learning_rate": 4.7132124247687526e-05,
      "loss": 0.4181,
      "step": 1920
    },
    {
      "epoch": 0.2292433780734054,
      "grad_norm": 11.317536354064941,
      "learning_rate": 4.7111927939572644e-05,
      "loss": 0.3044,
      "step": 1930
    },
    {
      "epoch": 0.2304311675971018,
      "grad_norm": 12.072446823120117,
      "learning_rate": 4.709173163145777e-05,
      "loss": 0.3031,
      "step": 1940
    },
    {
      "epoch": 0.2316189571207982,
      "grad_norm": 7.378643035888672,
      "learning_rate": 4.7071535323342895e-05,
      "loss": 0.3866,
      "step": 1950
    },
    {
      "epoch": 0.2328067466444946,
      "grad_norm": 4.614828109741211,
      "learning_rate": 4.705133901522802e-05,
      "loss": 0.3214,
      "step": 1960
    },
    {
      "epoch": 0.233994536168191,
      "grad_norm": 18.714794158935547,
      "learning_rate": 4.703114270711314e-05,
      "loss": 0.3879,
      "step": 1970
    },
    {
      "epoch": 0.2351823256918874,
      "grad_norm": 15.207596778869629,
      "learning_rate": 4.7010946398998265e-05,
      "loss": 0.5226,
      "step": 1980
    },
    {
      "epoch": 0.2363701152155838,
      "grad_norm": 17.063119888305664,
      "learning_rate": 4.699075009088339e-05,
      "loss": 0.377,
      "step": 1990
    },
    {
      "epoch": 0.2375579047392802,
      "grad_norm": 15.8844575881958,
      "learning_rate": 4.697055378276851e-05,
      "loss": 0.4126,
      "step": 2000
    },
    {
      "epoch": 0.2387456942629766,
      "grad_norm": 12.60599136352539,
      "learning_rate": 4.6950357474653634e-05,
      "loss": 0.3159,
      "step": 2010
    },
    {
      "epoch": 0.239933483786673,
      "grad_norm": 1.9277938604354858,
      "learning_rate": 4.693016116653876e-05,
      "loss": 0.5723,
      "step": 2020
    },
    {
      "epoch": 0.2411212733103694,
      "grad_norm": 13.465182304382324,
      "learning_rate": 4.6909964858423885e-05,
      "loss": 0.4973,
      "step": 2030
    },
    {
      "epoch": 0.2423090628340658,
      "grad_norm": 12.744091987609863,
      "learning_rate": 4.6889768550309004e-05,
      "loss": 0.2987,
      "step": 2040
    },
    {
      "epoch": 0.2434968523577622,
      "grad_norm": 21.032520294189453,
      "learning_rate": 4.686957224219413e-05,
      "loss": 0.3695,
      "step": 2050
    },
    {
      "epoch": 0.2446846418814586,
      "grad_norm": 8.545729637145996,
      "learning_rate": 4.6849375934079254e-05,
      "loss": 0.4652,
      "step": 2060
    },
    {
      "epoch": 0.245872431405155,
      "grad_norm": 11.345853805541992,
      "learning_rate": 4.682917962596437e-05,
      "loss": 0.478,
      "step": 2070
    },
    {
      "epoch": 0.2470602209288514,
      "grad_norm": 4.2132086753845215,
      "learning_rate": 4.68089833178495e-05,
      "loss": 0.5556,
      "step": 2080
    },
    {
      "epoch": 0.2482480104525478,
      "grad_norm": 12.806997299194336,
      "learning_rate": 4.6788787009734624e-05,
      "loss": 0.4915,
      "step": 2090
    },
    {
      "epoch": 0.2494357999762442,
      "grad_norm": 12.168424606323242,
      "learning_rate": 4.676859070161975e-05,
      "loss": 0.3425,
      "step": 2100
    },
    {
      "epoch": 0.2506235894999406,
      "grad_norm": 12.227484703063965,
      "learning_rate": 4.674839439350487e-05,
      "loss": 0.4878,
      "step": 2110
    },
    {
      "epoch": 0.25181137902363704,
      "grad_norm": 6.092287063598633,
      "learning_rate": 4.672819808538999e-05,
      "loss": 0.3697,
      "step": 2120
    },
    {
      "epoch": 0.2529991685473334,
      "grad_norm": 19.45308494567871,
      "learning_rate": 4.670800177727512e-05,
      "loss": 0.3939,
      "step": 2130
    },
    {
      "epoch": 0.2541869580710298,
      "grad_norm": 13.067036628723145,
      "learning_rate": 4.668780546916024e-05,
      "loss": 0.4764,
      "step": 2140
    },
    {
      "epoch": 0.2553747475947262,
      "grad_norm": 10.887768745422363,
      "learning_rate": 4.666760916104536e-05,
      "loss": 0.4823,
      "step": 2150
    },
    {
      "epoch": 0.25656253711842264,
      "grad_norm": 11.950045585632324,
      "learning_rate": 4.664741285293049e-05,
      "loss": 0.3819,
      "step": 2160
    },
    {
      "epoch": 0.257750326642119,
      "grad_norm": 5.315244674682617,
      "learning_rate": 4.6627216544815614e-05,
      "loss": 0.4088,
      "step": 2170
    },
    {
      "epoch": 0.2589381161658154,
      "grad_norm": 16.66663360595703,
      "learning_rate": 4.660702023670073e-05,
      "loss": 0.393,
      "step": 2180
    },
    {
      "epoch": 0.2601259056895118,
      "grad_norm": 7.909355163574219,
      "learning_rate": 4.658682392858586e-05,
      "loss": 0.3492,
      "step": 2190
    },
    {
      "epoch": 0.26131369521320824,
      "grad_norm": 7.2842302322387695,
      "learning_rate": 4.656662762047098e-05,
      "loss": 0.4222,
      "step": 2200
    },
    {
      "epoch": 0.2625014847369046,
      "grad_norm": 18.798355102539062,
      "learning_rate": 4.65464313123561e-05,
      "loss": 0.4438,
      "step": 2210
    },
    {
      "epoch": 0.263689274260601,
      "grad_norm": 12.2824068069458,
      "learning_rate": 4.652623500424123e-05,
      "loss": 0.3608,
      "step": 2220
    },
    {
      "epoch": 0.26487706378429743,
      "grad_norm": 5.844780921936035,
      "learning_rate": 4.650603869612635e-05,
      "loss": 0.4291,
      "step": 2230
    },
    {
      "epoch": 0.26606485330799384,
      "grad_norm": 11.465861320495605,
      "learning_rate": 4.648584238801148e-05,
      "loss": 0.5435,
      "step": 2240
    },
    {
      "epoch": 0.2672526428316902,
      "grad_norm": 9.61861801147461,
      "learning_rate": 4.6465646079896596e-05,
      "loss": 0.4031,
      "step": 2250
    },
    {
      "epoch": 0.2684404323553866,
      "grad_norm": 7.932845115661621,
      "learning_rate": 4.6445449771781715e-05,
      "loss": 0.3118,
      "step": 2260
    },
    {
      "epoch": 0.26962822187908303,
      "grad_norm": 18.786752700805664,
      "learning_rate": 4.642525346366685e-05,
      "loss": 0.5005,
      "step": 2270
    },
    {
      "epoch": 0.27081601140277944,
      "grad_norm": 15.974261283874512,
      "learning_rate": 4.6405057155551966e-05,
      "loss": 0.5509,
      "step": 2280
    },
    {
      "epoch": 0.2720038009264758,
      "grad_norm": 11.837593078613281,
      "learning_rate": 4.638486084743709e-05,
      "loss": 0.3788,
      "step": 2290
    },
    {
      "epoch": 0.2731915904501722,
      "grad_norm": 4.74309778213501,
      "learning_rate": 4.636466453932222e-05,
      "loss": 0.3716,
      "step": 2300
    },
    {
      "epoch": 0.27437937997386863,
      "grad_norm": 14.604912757873535,
      "learning_rate": 4.6344468231207335e-05,
      "loss": 0.4689,
      "step": 2310
    },
    {
      "epoch": 0.27556716949756505,
      "grad_norm": 17.05802345275879,
      "learning_rate": 4.632427192309246e-05,
      "loss": 0.3541,
      "step": 2320
    },
    {
      "epoch": 0.27675495902126146,
      "grad_norm": 8.757406234741211,
      "learning_rate": 4.630407561497758e-05,
      "loss": 0.3084,
      "step": 2330
    },
    {
      "epoch": 0.2779427485449578,
      "grad_norm": 14.270926475524902,
      "learning_rate": 4.628387930686271e-05,
      "loss": 0.425,
      "step": 2340
    },
    {
      "epoch": 0.27913053806865423,
      "grad_norm": 53.29812240600586,
      "learning_rate": 4.626368299874783e-05,
      "loss": 0.5205,
      "step": 2350
    },
    {
      "epoch": 0.28031832759235065,
      "grad_norm": 21.826309204101562,
      "learning_rate": 4.6243486690632956e-05,
      "loss": 0.3877,
      "step": 2360
    },
    {
      "epoch": 0.28150611711604706,
      "grad_norm": 21.623350143432617,
      "learning_rate": 4.6223290382518074e-05,
      "loss": 0.339,
      "step": 2370
    },
    {
      "epoch": 0.2826939066397434,
      "grad_norm": 7.923421382904053,
      "learning_rate": 4.62030940744032e-05,
      "loss": 0.4516,
      "step": 2380
    },
    {
      "epoch": 0.28388169616343983,
      "grad_norm": 16.60175132751465,
      "learning_rate": 4.6182897766288325e-05,
      "loss": 0.4398,
      "step": 2390
    },
    {
      "epoch": 0.28506948568713625,
      "grad_norm": 20.52156639099121,
      "learning_rate": 4.6162701458173444e-05,
      "loss": 0.4965,
      "step": 2400
    },
    {
      "epoch": 0.28625727521083266,
      "grad_norm": 10.168922424316406,
      "learning_rate": 4.6142505150058576e-05,
      "loss": 0.5497,
      "step": 2410
    },
    {
      "epoch": 0.287445064734529,
      "grad_norm": 10.445052146911621,
      "learning_rate": 4.6122308841943695e-05,
      "loss": 0.359,
      "step": 2420
    },
    {
      "epoch": 0.28863285425822544,
      "grad_norm": 9.222341537475586,
      "learning_rate": 4.610211253382882e-05,
      "loss": 0.3312,
      "step": 2430
    },
    {
      "epoch": 0.28982064378192185,
      "grad_norm": 3.2413759231567383,
      "learning_rate": 4.608191622571394e-05,
      "loss": 0.4143,
      "step": 2440
    },
    {
      "epoch": 0.29100843330561826,
      "grad_norm": 6.683810710906982,
      "learning_rate": 4.6061719917599064e-05,
      "loss": 0.4427,
      "step": 2450
    },
    {
      "epoch": 0.2921962228293146,
      "grad_norm": 8.63145923614502,
      "learning_rate": 4.604152360948419e-05,
      "loss": 0.3786,
      "step": 2460
    },
    {
      "epoch": 0.29338401235301104,
      "grad_norm": 8.000901222229004,
      "learning_rate": 4.602132730136931e-05,
      "loss": 0.36,
      "step": 2470
    },
    {
      "epoch": 0.29457180187670745,
      "grad_norm": 2.657128095626831,
      "learning_rate": 4.600113099325444e-05,
      "loss": 0.4778,
      "step": 2480
    },
    {
      "epoch": 0.29575959140040387,
      "grad_norm": 7.7156524658203125,
      "learning_rate": 4.598093468513956e-05,
      "loss": 0.3836,
      "step": 2490
    },
    {
      "epoch": 0.2969473809241002,
      "grad_norm": 6.074797630310059,
      "learning_rate": 4.5960738377024684e-05,
      "loss": 0.434,
      "step": 2500
    },
    {
      "epoch": 0.29813517044779664,
      "grad_norm": 27.329389572143555,
      "learning_rate": 4.59405420689098e-05,
      "loss": 0.4092,
      "step": 2510
    },
    {
      "epoch": 0.29932295997149305,
      "grad_norm": 3.836005449295044,
      "learning_rate": 4.592034576079493e-05,
      "loss": 0.2666,
      "step": 2520
    },
    {
      "epoch": 0.30051074949518947,
      "grad_norm": 6.92724609375,
      "learning_rate": 4.5900149452680054e-05,
      "loss": 0.515,
      "step": 2530
    },
    {
      "epoch": 0.3016985390188859,
      "grad_norm": 3.0361931324005127,
      "learning_rate": 4.587995314456517e-05,
      "loss": 0.2991,
      "step": 2540
    },
    {
      "epoch": 0.30288632854258224,
      "grad_norm": 19.627805709838867,
      "learning_rate": 4.5859756836450305e-05,
      "loss": 0.4448,
      "step": 2550
    },
    {
      "epoch": 0.30407411806627865,
      "grad_norm": 11.694130897521973,
      "learning_rate": 4.583956052833542e-05,
      "loss": 0.467,
      "step": 2560
    },
    {
      "epoch": 0.30526190758997507,
      "grad_norm": 4.8844685554504395,
      "learning_rate": 4.581936422022055e-05,
      "loss": 0.3543,
      "step": 2570
    },
    {
      "epoch": 0.3064496971136715,
      "grad_norm": 14.175518989562988,
      "learning_rate": 4.579916791210567e-05,
      "loss": 0.2952,
      "step": 2580
    },
    {
      "epoch": 0.30763748663736784,
      "grad_norm": 19.29123878479004,
      "learning_rate": 4.577897160399079e-05,
      "loss": 0.4614,
      "step": 2590
    },
    {
      "epoch": 0.30882527616106425,
      "grad_norm": 12.127927780151367,
      "learning_rate": 4.575877529587592e-05,
      "loss": 0.5072,
      "step": 2600
    },
    {
      "epoch": 0.31001306568476067,
      "grad_norm": 11.614591598510742,
      "learning_rate": 4.573857898776104e-05,
      "loss": 0.4973,
      "step": 2610
    },
    {
      "epoch": 0.3112008552084571,
      "grad_norm": 6.396824359893799,
      "learning_rate": 4.571838267964617e-05,
      "loss": 0.4304,
      "step": 2620
    },
    {
      "epoch": 0.31238864473215344,
      "grad_norm": 8.470419883728027,
      "learning_rate": 4.569818637153129e-05,
      "loss": 0.4762,
      "step": 2630
    },
    {
      "epoch": 0.31357643425584986,
      "grad_norm": 13.268900871276855,
      "learning_rate": 4.5677990063416406e-05,
      "loss": 0.3294,
      "step": 2640
    },
    {
      "epoch": 0.31476422377954627,
      "grad_norm": 12.65753173828125,
      "learning_rate": 4.565779375530153e-05,
      "loss": 0.3835,
      "step": 2650
    },
    {
      "epoch": 0.3159520133032427,
      "grad_norm": 28.63498878479004,
      "learning_rate": 4.563759744718666e-05,
      "loss": 0.346,
      "step": 2660
    },
    {
      "epoch": 0.31713980282693904,
      "grad_norm": 16.64824867248535,
      "learning_rate": 4.561740113907178e-05,
      "loss": 0.3465,
      "step": 2670
    },
    {
      "epoch": 0.31832759235063546,
      "grad_norm": 9.503334999084473,
      "learning_rate": 4.55972048309569e-05,
      "loss": 0.37,
      "step": 2680
    },
    {
      "epoch": 0.31951538187433187,
      "grad_norm": 11.527192115783691,
      "learning_rate": 4.5577008522842026e-05,
      "loss": 0.4103,
      "step": 2690
    },
    {
      "epoch": 0.3207031713980283,
      "grad_norm": 3.233851909637451,
      "learning_rate": 4.555681221472715e-05,
      "loss": 0.3451,
      "step": 2700
    },
    {
      "epoch": 0.32189096092172464,
      "grad_norm": 7.9146504402160645,
      "learning_rate": 4.553661590661227e-05,
      "loss": 0.2885,
      "step": 2710
    },
    {
      "epoch": 0.32307875044542106,
      "grad_norm": 1.9722609519958496,
      "learning_rate": 4.5516419598497396e-05,
      "loss": 0.2885,
      "step": 2720
    },
    {
      "epoch": 0.3242665399691175,
      "grad_norm": 14.059633255004883,
      "learning_rate": 4.549622329038252e-05,
      "loss": 0.4473,
      "step": 2730
    },
    {
      "epoch": 0.3254543294928139,
      "grad_norm": 10.714337348937988,
      "learning_rate": 4.547602698226765e-05,
      "loss": 0.5333,
      "step": 2740
    },
    {
      "epoch": 0.3266421190165103,
      "grad_norm": 21.0417423248291,
      "learning_rate": 4.5455830674152765e-05,
      "loss": 0.3656,
      "step": 2750
    },
    {
      "epoch": 0.32782990854020666,
      "grad_norm": 17.92778205871582,
      "learning_rate": 4.543563436603789e-05,
      "loss": 0.5642,
      "step": 2760
    },
    {
      "epoch": 0.3290176980639031,
      "grad_norm": 7.248648643493652,
      "learning_rate": 4.5415438057923016e-05,
      "loss": 0.4641,
      "step": 2770
    },
    {
      "epoch": 0.3302054875875995,
      "grad_norm": 5.496622562408447,
      "learning_rate": 4.5395241749808135e-05,
      "loss": 0.334,
      "step": 2780
    },
    {
      "epoch": 0.3313932771112959,
      "grad_norm": 9.370548248291016,
      "learning_rate": 4.537504544169326e-05,
      "loss": 0.4508,
      "step": 2790
    },
    {
      "epoch": 0.33258106663499226,
      "grad_norm": 9.950486183166504,
      "learning_rate": 4.5354849133578386e-05,
      "loss": 0.3373,
      "step": 2800
    },
    {
      "epoch": 0.3337688561586887,
      "grad_norm": 10.14198112487793,
      "learning_rate": 4.533465282546351e-05,
      "loss": 0.493,
      "step": 2810
    },
    {
      "epoch": 0.3349566456823851,
      "grad_norm": 15.428964614868164,
      "learning_rate": 4.531445651734863e-05,
      "loss": 0.4835,
      "step": 2820
    },
    {
      "epoch": 0.3361444352060815,
      "grad_norm": 7.583471775054932,
      "learning_rate": 4.5294260209233755e-05,
      "loss": 0.372,
      "step": 2830
    },
    {
      "epoch": 0.33733222472977786,
      "grad_norm": 2.755049705505371,
      "learning_rate": 4.527406390111888e-05,
      "loss": 0.2434,
      "step": 2840
    },
    {
      "epoch": 0.3385200142534743,
      "grad_norm": 20.16036033630371,
      "learning_rate": 4.5253867593004e-05,
      "loss": 0.3541,
      "step": 2850
    },
    {
      "epoch": 0.3397078037771707,
      "grad_norm": 2.6942453384399414,
      "learning_rate": 4.5233671284889124e-05,
      "loss": 0.3914,
      "step": 2860
    },
    {
      "epoch": 0.3408955933008671,
      "grad_norm": 13.700599670410156,
      "learning_rate": 4.521347497677425e-05,
      "loss": 0.2977,
      "step": 2870
    },
    {
      "epoch": 0.34208338282456346,
      "grad_norm": 11.469378471374512,
      "learning_rate": 4.5193278668659375e-05,
      "loss": 0.3483,
      "step": 2880
    },
    {
      "epoch": 0.3432711723482599,
      "grad_norm": 17.048847198486328,
      "learning_rate": 4.5173082360544494e-05,
      "loss": 0.3547,
      "step": 2890
    },
    {
      "epoch": 0.3444589618719563,
      "grad_norm": 12.716607093811035,
      "learning_rate": 4.515288605242962e-05,
      "loss": 0.3063,
      "step": 2900
    },
    {
      "epoch": 0.3456467513956527,
      "grad_norm": 4.693846702575684,
      "learning_rate": 4.5132689744314745e-05,
      "loss": 0.4137,
      "step": 2910
    },
    {
      "epoch": 0.34683454091934907,
      "grad_norm": 6.413184642791748,
      "learning_rate": 4.511249343619986e-05,
      "loss": 0.3467,
      "step": 2920
    },
    {
      "epoch": 0.3480223304430455,
      "grad_norm": 12.552629470825195,
      "learning_rate": 4.509229712808499e-05,
      "loss": 0.4323,
      "step": 2930
    },
    {
      "epoch": 0.3492101199667419,
      "grad_norm": 12.508402824401855,
      "learning_rate": 4.507210081997011e-05,
      "loss": 0.3145,
      "step": 2940
    },
    {
      "epoch": 0.3503979094904383,
      "grad_norm": 11.820070266723633,
      "learning_rate": 4.505190451185524e-05,
      "loss": 0.6181,
      "step": 2950
    },
    {
      "epoch": 0.3515856990141347,
      "grad_norm": 3.2258009910583496,
      "learning_rate": 4.503170820374036e-05,
      "loss": 0.292,
      "step": 2960
    },
    {
      "epoch": 0.3527734885378311,
      "grad_norm": 7.677980899810791,
      "learning_rate": 4.501151189562548e-05,
      "loss": 0.4825,
      "step": 2970
    },
    {
      "epoch": 0.3539612780615275,
      "grad_norm": 8.544417381286621,
      "learning_rate": 4.499131558751061e-05,
      "loss": 0.416,
      "step": 2980
    },
    {
      "epoch": 0.3551490675852239,
      "grad_norm": 14.969865798950195,
      "learning_rate": 4.497111927939573e-05,
      "loss": 0.3516,
      "step": 2990
    },
    {
      "epoch": 0.3563368571089203,
      "grad_norm": 3.8635447025299072,
      "learning_rate": 4.495092297128085e-05,
      "loss": 0.4023,
      "step": 3000
    },
    {
      "epoch": 0.3575246466326167,
      "grad_norm": 13.661275863647461,
      "learning_rate": 4.493072666316597e-05,
      "loss": 0.4602,
      "step": 3010
    },
    {
      "epoch": 0.3587124361563131,
      "grad_norm": 22.266080856323242,
      "learning_rate": 4.49105303550511e-05,
      "loss": 0.42,
      "step": 3020
    },
    {
      "epoch": 0.3599002256800095,
      "grad_norm": 11.463329315185547,
      "learning_rate": 4.489033404693622e-05,
      "loss": 0.2854,
      "step": 3030
    },
    {
      "epoch": 0.3610880152037059,
      "grad_norm": 6.0218048095703125,
      "learning_rate": 4.487013773882134e-05,
      "loss": 0.4267,
      "step": 3040
    },
    {
      "epoch": 0.3622758047274023,
      "grad_norm": 15.213096618652344,
      "learning_rate": 4.484994143070647e-05,
      "loss": 0.4615,
      "step": 3050
    },
    {
      "epoch": 0.3634635942510987,
      "grad_norm": 5.439938068389893,
      "learning_rate": 4.482974512259159e-05,
      "loss": 0.4258,
      "step": 3060
    },
    {
      "epoch": 0.3646513837747951,
      "grad_norm": 10.035447120666504,
      "learning_rate": 4.480954881447672e-05,
      "loss": 0.2348,
      "step": 3070
    },
    {
      "epoch": 0.3658391732984915,
      "grad_norm": 14.916609764099121,
      "learning_rate": 4.4789352506361836e-05,
      "loss": 0.4859,
      "step": 3080
    },
    {
      "epoch": 0.3670269628221879,
      "grad_norm": 23.3326416015625,
      "learning_rate": 4.476915619824696e-05,
      "loss": 0.4125,
      "step": 3090
    },
    {
      "epoch": 0.3682147523458843,
      "grad_norm": 1.0474132299423218,
      "learning_rate": 4.474895989013209e-05,
      "loss": 0.3933,
      "step": 3100
    },
    {
      "epoch": 0.3694025418695807,
      "grad_norm": 18.18515968322754,
      "learning_rate": 4.4728763582017205e-05,
      "loss": 0.2747,
      "step": 3110
    },
    {
      "epoch": 0.37059033139327713,
      "grad_norm": 6.331620216369629,
      "learning_rate": 4.470856727390234e-05,
      "loss": 0.308,
      "step": 3120
    },
    {
      "epoch": 0.3717781209169735,
      "grad_norm": 3.965709924697876,
      "learning_rate": 4.4688370965787456e-05,
      "loss": 0.3144,
      "step": 3130
    },
    {
      "epoch": 0.3729659104406699,
      "grad_norm": 9.964198112487793,
      "learning_rate": 4.466817465767258e-05,
      "loss": 0.2545,
      "step": 3140
    },
    {
      "epoch": 0.3741536999643663,
      "grad_norm": 7.4843668937683105,
      "learning_rate": 4.46479783495577e-05,
      "loss": 0.3575,
      "step": 3150
    },
    {
      "epoch": 0.37534148948806273,
      "grad_norm": 29.760860443115234,
      "learning_rate": 4.4627782041442826e-05,
      "loss": 0.2024,
      "step": 3160
    },
    {
      "epoch": 0.37652927901175914,
      "grad_norm": 26.95182991027832,
      "learning_rate": 4.460758573332795e-05,
      "loss": 0.4099,
      "step": 3170
    },
    {
      "epoch": 0.3777170685354555,
      "grad_norm": 8.9773588180542,
      "learning_rate": 4.458738942521307e-05,
      "loss": 0.4688,
      "step": 3180
    },
    {
      "epoch": 0.3789048580591519,
      "grad_norm": 7.424287796020508,
      "learning_rate": 4.45671931170982e-05,
      "loss": 0.3094,
      "step": 3190
    },
    {
      "epoch": 0.38009264758284833,
      "grad_norm": 4.381921291351318,
      "learning_rate": 4.454699680898332e-05,
      "loss": 0.3774,
      "step": 3200
    },
    {
      "epoch": 0.38128043710654475,
      "grad_norm": 25.510421752929688,
      "learning_rate": 4.4526800500868446e-05,
      "loss": 0.476,
      "step": 3210
    },
    {
      "epoch": 0.3824682266302411,
      "grad_norm": 17.589637756347656,
      "learning_rate": 4.4506604192753565e-05,
      "loss": 0.5483,
      "step": 3220
    },
    {
      "epoch": 0.3836560161539375,
      "grad_norm": 18.96725845336914,
      "learning_rate": 4.448640788463869e-05,
      "loss": 0.299,
      "step": 3230
    },
    {
      "epoch": 0.38484380567763393,
      "grad_norm": 19.48969078063965,
      "learning_rate": 4.4466211576523815e-05,
      "loss": 0.4174,
      "step": 3240
    },
    {
      "epoch": 0.38603159520133035,
      "grad_norm": 31.013425827026367,
      "learning_rate": 4.4446015268408934e-05,
      "loss": 0.4148,
      "step": 3250
    },
    {
      "epoch": 0.3872193847250267,
      "grad_norm": 12.710129737854004,
      "learning_rate": 4.442581896029406e-05,
      "loss": 0.4204,
      "step": 3260
    },
    {
      "epoch": 0.3884071742487231,
      "grad_norm": 10.037944793701172,
      "learning_rate": 4.4405622652179185e-05,
      "loss": 0.4746,
      "step": 3270
    },
    {
      "epoch": 0.38959496377241953,
      "grad_norm": 9.258882522583008,
      "learning_rate": 4.438542634406431e-05,
      "loss": 0.5033,
      "step": 3280
    },
    {
      "epoch": 0.39078275329611595,
      "grad_norm": 20.173011779785156,
      "learning_rate": 4.436523003594943e-05,
      "loss": 0.4631,
      "step": 3290
    },
    {
      "epoch": 0.3919705428198123,
      "grad_norm": 10.452534675598145,
      "learning_rate": 4.4345033727834554e-05,
      "loss": 0.395,
      "step": 3300
    },
    {
      "epoch": 0.3931583323435087,
      "grad_norm": 15.678132057189941,
      "learning_rate": 4.432483741971968e-05,
      "loss": 0.3774,
      "step": 3310
    },
    {
      "epoch": 0.39434612186720513,
      "grad_norm": 6.001423358917236,
      "learning_rate": 4.43046411116048e-05,
      "loss": 0.4218,
      "step": 3320
    },
    {
      "epoch": 0.39553391139090155,
      "grad_norm": 3.1600759029388428,
      "learning_rate": 4.4284444803489924e-05,
      "loss": 0.267,
      "step": 3330
    },
    {
      "epoch": 0.3967217009145979,
      "grad_norm": 15.701601028442383,
      "learning_rate": 4.426424849537505e-05,
      "loss": 0.4171,
      "step": 3340
    },
    {
      "epoch": 0.3979094904382943,
      "grad_norm": 14.965439796447754,
      "learning_rate": 4.424405218726017e-05,
      "loss": 0.51,
      "step": 3350
    },
    {
      "epoch": 0.39909727996199074,
      "grad_norm": 9.799454689025879,
      "learning_rate": 4.422385587914529e-05,
      "loss": 0.5128,
      "step": 3360
    },
    {
      "epoch": 0.40028506948568715,
      "grad_norm": 11.53872013092041,
      "learning_rate": 4.420365957103042e-05,
      "loss": 0.4137,
      "step": 3370
    },
    {
      "epoch": 0.40147285900938356,
      "grad_norm": 24.697078704833984,
      "learning_rate": 4.4183463262915544e-05,
      "loss": 0.3897,
      "step": 3380
    },
    {
      "epoch": 0.4026606485330799,
      "grad_norm": 2.031918525695801,
      "learning_rate": 4.416326695480066e-05,
      "loss": 0.3018,
      "step": 3390
    },
    {
      "epoch": 0.40384843805677634,
      "grad_norm": 12.18288516998291,
      "learning_rate": 4.414307064668579e-05,
      "loss": 0.3534,
      "step": 3400
    },
    {
      "epoch": 0.40503622758047275,
      "grad_norm": 10.754319190979004,
      "learning_rate": 4.4122874338570913e-05,
      "loss": 0.3373,
      "step": 3410
    },
    {
      "epoch": 0.40622401710416917,
      "grad_norm": 15.510159492492676,
      "learning_rate": 4.410267803045603e-05,
      "loss": 0.2861,
      "step": 3420
    },
    {
      "epoch": 0.4074118066278655,
      "grad_norm": 5.158355712890625,
      "learning_rate": 4.408248172234116e-05,
      "loss": 0.2839,
      "step": 3430
    },
    {
      "epoch": 0.40859959615156194,
      "grad_norm": 5.095500469207764,
      "learning_rate": 4.406228541422628e-05,
      "loss": 0.4358,
      "step": 3440
    },
    {
      "epoch": 0.40978738567525835,
      "grad_norm": 15.187273025512695,
      "learning_rate": 4.404208910611141e-05,
      "loss": 0.637,
      "step": 3450
    },
    {
      "epoch": 0.41097517519895477,
      "grad_norm": 3.205122947692871,
      "learning_rate": 4.402189279799653e-05,
      "loss": 0.2753,
      "step": 3460
    },
    {
      "epoch": 0.4121629647226511,
      "grad_norm": 14.393389701843262,
      "learning_rate": 4.400169648988165e-05,
      "loss": 0.3418,
      "step": 3470
    },
    {
      "epoch": 0.41335075424634754,
      "grad_norm": 25.65963363647461,
      "learning_rate": 4.398150018176678e-05,
      "loss": 0.4401,
      "step": 3480
    },
    {
      "epoch": 0.41453854377004395,
      "grad_norm": 16.838680267333984,
      "learning_rate": 4.3961303873651896e-05,
      "loss": 0.5664,
      "step": 3490
    },
    {
      "epoch": 0.41572633329374037,
      "grad_norm": 15.010178565979004,
      "learning_rate": 4.394110756553702e-05,
      "loss": 0.3574,
      "step": 3500
    },
    {
      "epoch": 0.4169141228174367,
      "grad_norm": 6.812884330749512,
      "learning_rate": 4.392091125742214e-05,
      "loss": 0.2718,
      "step": 3510
    },
    {
      "epoch": 0.41810191234113314,
      "grad_norm": 9.333667755126953,
      "learning_rate": 4.390071494930727e-05,
      "loss": 0.408,
      "step": 3520
    },
    {
      "epoch": 0.41928970186482956,
      "grad_norm": 15.54117488861084,
      "learning_rate": 4.388051864119239e-05,
      "loss": 0.4535,
      "step": 3530
    },
    {
      "epoch": 0.42047749138852597,
      "grad_norm": 17.10935401916504,
      "learning_rate": 4.386032233307752e-05,
      "loss": 0.3454,
      "step": 3540
    },
    {
      "epoch": 0.42166528091222233,
      "grad_norm": 12.55117130279541,
      "learning_rate": 4.384012602496264e-05,
      "loss": 0.2627,
      "step": 3550
    },
    {
      "epoch": 0.42285307043591874,
      "grad_norm": 14.86527156829834,
      "learning_rate": 4.381992971684776e-05,
      "loss": 0.4194,
      "step": 3560
    },
    {
      "epoch": 0.42404085995961516,
      "grad_norm": 20.373779296875,
      "learning_rate": 4.3799733408732886e-05,
      "loss": 0.4185,
      "step": 3570
    },
    {
      "epoch": 0.42522864948331157,
      "grad_norm": 6.7415690422058105,
      "learning_rate": 4.3779537100618005e-05,
      "loss": 0.3785,
      "step": 3580
    },
    {
      "epoch": 0.426416439007008,
      "grad_norm": 26.398717880249023,
      "learning_rate": 4.375934079250314e-05,
      "loss": 0.3156,
      "step": 3590
    },
    {
      "epoch": 0.42760422853070434,
      "grad_norm": 20.97412109375,
      "learning_rate": 4.3739144484388256e-05,
      "loss": 0.4025,
      "step": 3600
    },
    {
      "epoch": 0.42879201805440076,
      "grad_norm": 12.382195472717285,
      "learning_rate": 4.371894817627338e-05,
      "loss": 0.4884,
      "step": 3610
    },
    {
      "epoch": 0.4299798075780972,
      "grad_norm": 11.388267517089844,
      "learning_rate": 4.3698751868158506e-05,
      "loss": 0.3592,
      "step": 3620
    },
    {
      "epoch": 0.4311675971017936,
      "grad_norm": 15.474785804748535,
      "learning_rate": 4.3678555560043625e-05,
      "loss": 0.311,
      "step": 3630
    },
    {
      "epoch": 0.43235538662548995,
      "grad_norm": 2.701507091522217,
      "learning_rate": 4.365835925192875e-05,
      "loss": 0.2764,
      "step": 3640
    },
    {
      "epoch": 0.43354317614918636,
      "grad_norm": 24.826433181762695,
      "learning_rate": 4.363816294381387e-05,
      "loss": 0.4323,
      "step": 3650
    },
    {
      "epoch": 0.4347309656728828,
      "grad_norm": 19.545717239379883,
      "learning_rate": 4.3617966635699e-05,
      "loss": 0.4339,
      "step": 3660
    },
    {
      "epoch": 0.4359187551965792,
      "grad_norm": 12.077818870544434,
      "learning_rate": 4.359777032758412e-05,
      "loss": 0.3767,
      "step": 3670
    },
    {
      "epoch": 0.43710654472027555,
      "grad_norm": 12.40162181854248,
      "learning_rate": 4.357757401946924e-05,
      "loss": 0.3163,
      "step": 3680
    },
    {
      "epoch": 0.43829433424397196,
      "grad_norm": 18.187673568725586,
      "learning_rate": 4.355737771135437e-05,
      "loss": 0.2156,
      "step": 3690
    },
    {
      "epoch": 0.4394821237676684,
      "grad_norm": 20.061111450195312,
      "learning_rate": 4.353718140323949e-05,
      "loss": 0.4384,
      "step": 3700
    },
    {
      "epoch": 0.4406699132913648,
      "grad_norm": 4.047170162200928,
      "learning_rate": 4.3516985095124615e-05,
      "loss": 0.3721,
      "step": 3710
    },
    {
      "epoch": 0.44185770281506115,
      "grad_norm": 5.222775459289551,
      "learning_rate": 4.349678878700973e-05,
      "loss": 0.3858,
      "step": 3720
    },
    {
      "epoch": 0.44304549233875756,
      "grad_norm": 10.884535789489746,
      "learning_rate": 4.347659247889486e-05,
      "loss": 0.3702,
      "step": 3730
    },
    {
      "epoch": 0.444233281862454,
      "grad_norm": 16.281879425048828,
      "learning_rate": 4.3456396170779984e-05,
      "loss": 0.4362,
      "step": 3740
    },
    {
      "epoch": 0.4454210713861504,
      "grad_norm": 2.4041261672973633,
      "learning_rate": 4.34361998626651e-05,
      "loss": 0.2976,
      "step": 3750
    },
    {
      "epoch": 0.44660886090984675,
      "grad_norm": 6.777461528778076,
      "learning_rate": 4.3416003554550235e-05,
      "loss": 0.3414,
      "step": 3760
    },
    {
      "epoch": 0.44779665043354316,
      "grad_norm": 7.669863700866699,
      "learning_rate": 4.3395807246435354e-05,
      "loss": 0.3954,
      "step": 3770
    },
    {
      "epoch": 0.4489844399572396,
      "grad_norm": 8.656243324279785,
      "learning_rate": 4.337561093832048e-05,
      "loss": 0.4409,
      "step": 3780
    },
    {
      "epoch": 0.450172229480936,
      "grad_norm": 10.858545303344727,
      "learning_rate": 4.33554146302056e-05,
      "loss": 0.4721,
      "step": 3790
    },
    {
      "epoch": 0.4513600190046324,
      "grad_norm": 1.6581653356552124,
      "learning_rate": 4.333521832209072e-05,
      "loss": 0.4162,
      "step": 3800
    },
    {
      "epoch": 0.45254780852832877,
      "grad_norm": 1.1774976253509521,
      "learning_rate": 4.331502201397585e-05,
      "loss": 0.3506,
      "step": 3810
    },
    {
      "epoch": 0.4537355980520252,
      "grad_norm": 4.5789103507995605,
      "learning_rate": 4.329482570586097e-05,
      "loss": 0.3356,
      "step": 3820
    },
    {
      "epoch": 0.4549233875757216,
      "grad_norm": 7.276851654052734,
      "learning_rate": 4.327462939774609e-05,
      "loss": 0.3483,
      "step": 3830
    },
    {
      "epoch": 0.456111177099418,
      "grad_norm": 3.2167487144470215,
      "learning_rate": 4.325443308963122e-05,
      "loss": 0.2807,
      "step": 3840
    },
    {
      "epoch": 0.45729896662311437,
      "grad_norm": 13.19559097290039,
      "learning_rate": 4.323423678151634e-05,
      "loss": 0.3456,
      "step": 3850
    },
    {
      "epoch": 0.4584867561468108,
      "grad_norm": 25.430503845214844,
      "learning_rate": 4.321404047340146e-05,
      "loss": 0.2899,
      "step": 3860
    },
    {
      "epoch": 0.4596745456705072,
      "grad_norm": 10.575112342834473,
      "learning_rate": 4.319384416528659e-05,
      "loss": 0.3278,
      "step": 3870
    },
    {
      "epoch": 0.4608623351942036,
      "grad_norm": 7.5166802406311035,
      "learning_rate": 4.317364785717171e-05,
      "loss": 0.3293,
      "step": 3880
    },
    {
      "epoch": 0.46205012471789997,
      "grad_norm": 24.12334442138672,
      "learning_rate": 4.315345154905683e-05,
      "loss": 0.5226,
      "step": 3890
    },
    {
      "epoch": 0.4632379142415964,
      "grad_norm": 3.545163869857788,
      "learning_rate": 4.313325524094196e-05,
      "loss": 0.3248,
      "step": 3900
    },
    {
      "epoch": 0.4644257037652928,
      "grad_norm": 33.887996673583984,
      "learning_rate": 4.311305893282708e-05,
      "loss": 0.3371,
      "step": 3910
    },
    {
      "epoch": 0.4656134932889892,
      "grad_norm": 12.632829666137695,
      "learning_rate": 4.309286262471221e-05,
      "loss": 0.369,
      "step": 3920
    },
    {
      "epoch": 0.46680128281268557,
      "grad_norm": 16.778484344482422,
      "learning_rate": 4.3072666316597326e-05,
      "loss": 0.3016,
      "step": 3930
    },
    {
      "epoch": 0.467989072336382,
      "grad_norm": 3.4431657791137695,
      "learning_rate": 4.305247000848245e-05,
      "loss": 0.2662,
      "step": 3940
    },
    {
      "epoch": 0.4691768618600784,
      "grad_norm": 16.091875076293945,
      "learning_rate": 4.303227370036758e-05,
      "loss": 0.2483,
      "step": 3950
    },
    {
      "epoch": 0.4703646513837748,
      "grad_norm": 6.015840530395508,
      "learning_rate": 4.3012077392252696e-05,
      "loss": 0.3688,
      "step": 3960
    },
    {
      "epoch": 0.47155244090747117,
      "grad_norm": 6.25945520401001,
      "learning_rate": 4.299188108413782e-05,
      "loss": 0.3638,
      "step": 3970
    },
    {
      "epoch": 0.4727402304311676,
      "grad_norm": 17.756319046020508,
      "learning_rate": 4.2971684776022947e-05,
      "loss": 0.4301,
      "step": 3980
    },
    {
      "epoch": 0.473928019954864,
      "grad_norm": 5.478827953338623,
      "learning_rate": 4.295148846790807e-05,
      "loss": 0.4089,
      "step": 3990
    },
    {
      "epoch": 0.4751158094785604,
      "grad_norm": 19.53929328918457,
      "learning_rate": 4.293129215979319e-05,
      "loss": 0.393,
      "step": 4000
    }
  ],
  "logging_steps": 10,
  "max_steps": 25257,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 10163896320000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
