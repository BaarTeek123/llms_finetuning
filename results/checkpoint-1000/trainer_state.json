{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.1187789523696401,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001187789523696401,
      "grad_norm": 1.571284294128418,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.691,
      "step": 10
    },
    {
      "epoch": 0.002375579047392802,
      "grad_norm": 2.3712053298950195,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.7104,
      "step": 20
    },
    {
      "epoch": 0.003563368571089203,
      "grad_norm": 2.394946813583374,
      "learning_rate": 3e-06,
      "loss": 0.7076,
      "step": 30
    },
    {
      "epoch": 0.004751158094785604,
      "grad_norm": 3.6025919914245605,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.714,
      "step": 40
    },
    {
      "epoch": 0.005938947618482005,
      "grad_norm": 1.937501311302185,
      "learning_rate": 5e-06,
      "loss": 0.6987,
      "step": 50
    },
    {
      "epoch": 0.007126737142178406,
      "grad_norm": 3.3844287395477295,
      "learning_rate": 6e-06,
      "loss": 0.7021,
      "step": 60
    },
    {
      "epoch": 0.008314526665874808,
      "grad_norm": 3.468790054321289,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.699,
      "step": 70
    },
    {
      "epoch": 0.009502316189571208,
      "grad_norm": 1.6214921474456787,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.6835,
      "step": 80
    },
    {
      "epoch": 0.010690105713267608,
      "grad_norm": 1.8596107959747314,
      "learning_rate": 9e-06,
      "loss": 0.6895,
      "step": 90
    },
    {
      "epoch": 0.01187789523696401,
      "grad_norm": 2.3269872665405273,
      "learning_rate": 1e-05,
      "loss": 0.6835,
      "step": 100
    },
    {
      "epoch": 0.01306568476066041,
      "grad_norm": 4.275064945220947,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.6999,
      "step": 110
    },
    {
      "epoch": 0.014253474284356813,
      "grad_norm": 1.7505264282226562,
      "learning_rate": 1.2e-05,
      "loss": 0.6941,
      "step": 120
    },
    {
      "epoch": 0.015441263808053213,
      "grad_norm": 1.5032916069030762,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.6868,
      "step": 130
    },
    {
      "epoch": 0.016629053331749615,
      "grad_norm": 3.410565137863159,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.703,
      "step": 140
    },
    {
      "epoch": 0.017816842855446016,
      "grad_norm": 2.50521183013916,
      "learning_rate": 1.5e-05,
      "loss": 0.7025,
      "step": 150
    },
    {
      "epoch": 0.019004632379142416,
      "grad_norm": 3.892134428024292,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.6794,
      "step": 160
    },
    {
      "epoch": 0.020192421902838816,
      "grad_norm": 1.7042125463485718,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.6917,
      "step": 170
    },
    {
      "epoch": 0.021380211426535217,
      "grad_norm": 4.2205915451049805,
      "learning_rate": 1.8e-05,
      "loss": 0.6971,
      "step": 180
    },
    {
      "epoch": 0.02256800095023162,
      "grad_norm": 1.5046566724777222,
      "learning_rate": 1.9e-05,
      "loss": 0.6874,
      "step": 190
    },
    {
      "epoch": 0.02375579047392802,
      "grad_norm": 1.69389009475708,
      "learning_rate": 2e-05,
      "loss": 0.6692,
      "step": 200
    },
    {
      "epoch": 0.02494357999762442,
      "grad_norm": 1.7059226036071777,
      "learning_rate": 2.1e-05,
      "loss": 0.6717,
      "step": 210
    },
    {
      "epoch": 0.02613136952132082,
      "grad_norm": 1.818788766860962,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.6601,
      "step": 220
    },
    {
      "epoch": 0.02731915904501722,
      "grad_norm": 2.0799472332000732,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.6629,
      "step": 230
    },
    {
      "epoch": 0.028506948568713626,
      "grad_norm": 1.823838472366333,
      "learning_rate": 2.4e-05,
      "loss": 0.6681,
      "step": 240
    },
    {
      "epoch": 0.029694738092410026,
      "grad_norm": 2.8223283290863037,
      "learning_rate": 2.5e-05,
      "loss": 0.6993,
      "step": 250
    },
    {
      "epoch": 0.030882527616106426,
      "grad_norm": 2.522125244140625,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.7037,
      "step": 260
    },
    {
      "epoch": 0.03207031713980283,
      "grad_norm": 1.8430627584457397,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.6947,
      "step": 270
    },
    {
      "epoch": 0.03325810666349923,
      "grad_norm": 3.059164524078369,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.6869,
      "step": 280
    },
    {
      "epoch": 0.03444589618719563,
      "grad_norm": 1.9441590309143066,
      "learning_rate": 2.9e-05,
      "loss": 0.6886,
      "step": 290
    },
    {
      "epoch": 0.03563368571089203,
      "grad_norm": 1.770355224609375,
      "learning_rate": 3e-05,
      "loss": 0.6928,
      "step": 300
    },
    {
      "epoch": 0.03682147523458843,
      "grad_norm": 1.7763140201568604,
      "learning_rate": 3.1e-05,
      "loss": 0.6743,
      "step": 310
    },
    {
      "epoch": 0.03800926475828483,
      "grad_norm": 1.8267428874969482,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.6672,
      "step": 320
    },
    {
      "epoch": 0.03919705428198123,
      "grad_norm": 1.6273711919784546,
      "learning_rate": 3.3e-05,
      "loss": 0.6543,
      "step": 330
    },
    {
      "epoch": 0.04038484380567763,
      "grad_norm": 4.313201904296875,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.6706,
      "step": 340
    },
    {
      "epoch": 0.04157263332937403,
      "grad_norm": 4.140109062194824,
      "learning_rate": 3.5e-05,
      "loss": 0.6651,
      "step": 350
    },
    {
      "epoch": 0.04276042285307043,
      "grad_norm": 2.572695255279541,
      "learning_rate": 3.6e-05,
      "loss": 0.67,
      "step": 360
    },
    {
      "epoch": 0.04394821237676684,
      "grad_norm": 2.187011957168579,
      "learning_rate": 3.7e-05,
      "loss": 0.6432,
      "step": 370
    },
    {
      "epoch": 0.04513600190046324,
      "grad_norm": 4.77373743057251,
      "learning_rate": 3.8e-05,
      "loss": 0.671,
      "step": 380
    },
    {
      "epoch": 0.04632379142415964,
      "grad_norm": 2.0730113983154297,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.6803,
      "step": 390
    },
    {
      "epoch": 0.04751158094785604,
      "grad_norm": 3.850599765777588,
      "learning_rate": 4e-05,
      "loss": 0.6695,
      "step": 400
    },
    {
      "epoch": 0.04869937047155244,
      "grad_norm": 3.2260897159576416,
      "learning_rate": 4.1e-05,
      "loss": 0.6611,
      "step": 410
    },
    {
      "epoch": 0.04988715999524884,
      "grad_norm": 2.1476058959960938,
      "learning_rate": 4.2e-05,
      "loss": 0.6543,
      "step": 420
    },
    {
      "epoch": 0.05107494951894524,
      "grad_norm": 5.54133415222168,
      "learning_rate": 4.3e-05,
      "loss": 0.6637,
      "step": 430
    },
    {
      "epoch": 0.05226273904264164,
      "grad_norm": 3.140399932861328,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.6532,
      "step": 440
    },
    {
      "epoch": 0.05345052856633804,
      "grad_norm": 3.0184481143951416,
      "learning_rate": 4.5e-05,
      "loss": 0.613,
      "step": 450
    },
    {
      "epoch": 0.05463831809003444,
      "grad_norm": 3.482469320297241,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.6442,
      "step": 460
    },
    {
      "epoch": 0.055826107613730844,
      "grad_norm": 4.6692423820495605,
      "learning_rate": 4.7e-05,
      "loss": 0.6508,
      "step": 470
    },
    {
      "epoch": 0.05701389713742725,
      "grad_norm": 5.267462253570557,
      "learning_rate": 4.8e-05,
      "loss": 0.6357,
      "step": 480
    },
    {
      "epoch": 0.05820168666112365,
      "grad_norm": 5.792267322540283,
      "learning_rate": 4.9e-05,
      "loss": 0.6523,
      "step": 490
    },
    {
      "epoch": 0.05938947618482005,
      "grad_norm": 3.983430862426758,
      "learning_rate": 5e-05,
      "loss": 0.639,
      "step": 500
    },
    {
      "epoch": 0.06057726570851645,
      "grad_norm": 5.588817596435547,
      "learning_rate": 4.997980369188512e-05,
      "loss": 0.64,
      "step": 510
    },
    {
      "epoch": 0.06176505523221285,
      "grad_norm": 7.067869186401367,
      "learning_rate": 4.995960738377025e-05,
      "loss": 0.633,
      "step": 520
    },
    {
      "epoch": 0.06295284475590926,
      "grad_norm": 3.3713228702545166,
      "learning_rate": 4.993941107565537e-05,
      "loss": 0.6247,
      "step": 530
    },
    {
      "epoch": 0.06414063427960566,
      "grad_norm": 6.371031761169434,
      "learning_rate": 4.99192147675405e-05,
      "loss": 0.6461,
      "step": 540
    },
    {
      "epoch": 0.06532842380330206,
      "grad_norm": 2.8868744373321533,
      "learning_rate": 4.9899018459425616e-05,
      "loss": 0.5931,
      "step": 550
    },
    {
      "epoch": 0.06651621332699846,
      "grad_norm": 4.016470909118652,
      "learning_rate": 4.987882215131074e-05,
      "loss": 0.6165,
      "step": 560
    },
    {
      "epoch": 0.06770400285069486,
      "grad_norm": 4.354010105133057,
      "learning_rate": 4.985862584319587e-05,
      "loss": 0.6168,
      "step": 570
    },
    {
      "epoch": 0.06889179237439126,
      "grad_norm": 4.343535423278809,
      "learning_rate": 4.9838429535080985e-05,
      "loss": 0.6168,
      "step": 580
    },
    {
      "epoch": 0.07007958189808766,
      "grad_norm": 5.812617301940918,
      "learning_rate": 4.981823322696612e-05,
      "loss": 0.6248,
      "step": 590
    },
    {
      "epoch": 0.07126737142178406,
      "grad_norm": 5.959688663482666,
      "learning_rate": 4.9798036918851236e-05,
      "loss": 0.6154,
      "step": 600
    },
    {
      "epoch": 0.07245516094548046,
      "grad_norm": 4.963962554931641,
      "learning_rate": 4.977784061073636e-05,
      "loss": 0.6111,
      "step": 610
    },
    {
      "epoch": 0.07364295046917686,
      "grad_norm": 6.3539605140686035,
      "learning_rate": 4.975764430262148e-05,
      "loss": 0.6219,
      "step": 620
    },
    {
      "epoch": 0.07483073999287326,
      "grad_norm": 4.269138813018799,
      "learning_rate": 4.9737447994506606e-05,
      "loss": 0.5873,
      "step": 630
    },
    {
      "epoch": 0.07601852951656966,
      "grad_norm": 5.565948963165283,
      "learning_rate": 4.971725168639173e-05,
      "loss": 0.5671,
      "step": 640
    },
    {
      "epoch": 0.07720631904026606,
      "grad_norm": 9.216350555419922,
      "learning_rate": 4.969705537827685e-05,
      "loss": 0.5477,
      "step": 650
    },
    {
      "epoch": 0.07839410856396246,
      "grad_norm": 2.9129035472869873,
      "learning_rate": 4.9676859070161975e-05,
      "loss": 0.5838,
      "step": 660
    },
    {
      "epoch": 0.07958189808765886,
      "grad_norm": 13.75711727142334,
      "learning_rate": 4.96566627620471e-05,
      "loss": 0.5386,
      "step": 670
    },
    {
      "epoch": 0.08076968761135526,
      "grad_norm": 4.186140537261963,
      "learning_rate": 4.9636466453932226e-05,
      "loss": 0.6116,
      "step": 680
    },
    {
      "epoch": 0.08195747713505167,
      "grad_norm": 2.5830237865448,
      "learning_rate": 4.9616270145817344e-05,
      "loss": 0.54,
      "step": 690
    },
    {
      "epoch": 0.08314526665874807,
      "grad_norm": 8.397902488708496,
      "learning_rate": 4.959607383770247e-05,
      "loss": 0.6059,
      "step": 700
    },
    {
      "epoch": 0.08433305618244447,
      "grad_norm": 8.332036972045898,
      "learning_rate": 4.9575877529587595e-05,
      "loss": 0.5963,
      "step": 710
    },
    {
      "epoch": 0.08552084570614087,
      "grad_norm": 4.008006572723389,
      "learning_rate": 4.9555681221472714e-05,
      "loss": 0.4987,
      "step": 720
    },
    {
      "epoch": 0.08670863522983727,
      "grad_norm": 8.097256660461426,
      "learning_rate": 4.953548491335784e-05,
      "loss": 0.5773,
      "step": 730
    },
    {
      "epoch": 0.08789642475353368,
      "grad_norm": 6.3633551597595215,
      "learning_rate": 4.9515288605242965e-05,
      "loss": 0.5049,
      "step": 740
    },
    {
      "epoch": 0.08908421427723008,
      "grad_norm": 6.642234802246094,
      "learning_rate": 4.949509229712809e-05,
      "loss": 0.4989,
      "step": 750
    },
    {
      "epoch": 0.09027200380092648,
      "grad_norm": 6.825439453125,
      "learning_rate": 4.947489598901321e-05,
      "loss": 0.551,
      "step": 760
    },
    {
      "epoch": 0.09145979332462288,
      "grad_norm": 4.613224029541016,
      "learning_rate": 4.9454699680898334e-05,
      "loss": 0.4616,
      "step": 770
    },
    {
      "epoch": 0.09264758284831928,
      "grad_norm": 11.66037368774414,
      "learning_rate": 4.943450337278346e-05,
      "loss": 0.6176,
      "step": 780
    },
    {
      "epoch": 0.09383537237201568,
      "grad_norm": 8.434765815734863,
      "learning_rate": 4.941430706466858e-05,
      "loss": 0.4825,
      "step": 790
    },
    {
      "epoch": 0.09502316189571208,
      "grad_norm": 12.122628211975098,
      "learning_rate": 4.9394110756553704e-05,
      "loss": 0.5341,
      "step": 800
    },
    {
      "epoch": 0.09621095141940848,
      "grad_norm": 8.43961238861084,
      "learning_rate": 4.937391444843883e-05,
      "loss": 0.5171,
      "step": 810
    },
    {
      "epoch": 0.09739874094310488,
      "grad_norm": 7.21361780166626,
      "learning_rate": 4.9353718140323954e-05,
      "loss": 0.496,
      "step": 820
    },
    {
      "epoch": 0.09858653046680128,
      "grad_norm": 9.11520004272461,
      "learning_rate": 4.933352183220907e-05,
      "loss": 0.5466,
      "step": 830
    },
    {
      "epoch": 0.09977431999049768,
      "grad_norm": 6.771602153778076,
      "learning_rate": 4.93133255240942e-05,
      "loss": 0.4673,
      "step": 840
    },
    {
      "epoch": 0.10096210951419408,
      "grad_norm": 9.030261993408203,
      "learning_rate": 4.9293129215979324e-05,
      "loss": 0.5149,
      "step": 850
    },
    {
      "epoch": 0.10214989903789048,
      "grad_norm": 6.3041462898254395,
      "learning_rate": 4.927293290786444e-05,
      "loss": 0.446,
      "step": 860
    },
    {
      "epoch": 0.10333768856158689,
      "grad_norm": 8.052353858947754,
      "learning_rate": 4.925273659974957e-05,
      "loss": 0.4965,
      "step": 870
    },
    {
      "epoch": 0.10452547808528329,
      "grad_norm": 6.866906642913818,
      "learning_rate": 4.923254029163469e-05,
      "loss": 0.526,
      "step": 880
    },
    {
      "epoch": 0.10571326760897969,
      "grad_norm": 5.992532730102539,
      "learning_rate": 4.921234398351981e-05,
      "loss": 0.5709,
      "step": 890
    },
    {
      "epoch": 0.10690105713267609,
      "grad_norm": 9.69338607788086,
      "learning_rate": 4.919214767540494e-05,
      "loss": 0.4764,
      "step": 900
    },
    {
      "epoch": 0.10808884665637249,
      "grad_norm": 6.739308834075928,
      "learning_rate": 4.917195136729006e-05,
      "loss": 0.5039,
      "step": 910
    },
    {
      "epoch": 0.10927663618006889,
      "grad_norm": 6.0438408851623535,
      "learning_rate": 4.915175505917519e-05,
      "loss": 0.4738,
      "step": 920
    },
    {
      "epoch": 0.11046442570376529,
      "grad_norm": 14.130400657653809,
      "learning_rate": 4.913155875106031e-05,
      "loss": 0.4798,
      "step": 930
    },
    {
      "epoch": 0.11165221522746169,
      "grad_norm": 5.511341571807861,
      "learning_rate": 4.911136244294543e-05,
      "loss": 0.5008,
      "step": 940
    },
    {
      "epoch": 0.1128400047511581,
      "grad_norm": 13.602300643920898,
      "learning_rate": 4.909116613483056e-05,
      "loss": 0.5073,
      "step": 950
    },
    {
      "epoch": 0.1140277942748545,
      "grad_norm": 5.304278373718262,
      "learning_rate": 4.9070969826715676e-05,
      "loss": 0.4298,
      "step": 960
    },
    {
      "epoch": 0.1152155837985509,
      "grad_norm": 3.1956429481506348,
      "learning_rate": 4.90507735186008e-05,
      "loss": 0.442,
      "step": 970
    },
    {
      "epoch": 0.1164033733222473,
      "grad_norm": 14.201797485351562,
      "learning_rate": 4.903057721048592e-05,
      "loss": 0.5348,
      "step": 980
    },
    {
      "epoch": 0.1175911628459437,
      "grad_norm": 11.228254318237305,
      "learning_rate": 4.901038090237105e-05,
      "loss": 0.5108,
      "step": 990
    },
    {
      "epoch": 0.1187789523696401,
      "grad_norm": 11.285070419311523,
      "learning_rate": 4.899018459425617e-05,
      "loss": 0.5736,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 25257,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2540974080000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
