# Comparison of fine-tuning performance


This Python project evaluates different model adaptation techniques applied to large language models (LLMs) using PyTorch. The methods include soft prompts, prefix tuning, full fine-tuning, LoRA (Low Rank Adaptation), fine-tuning with a single added layer, combined approaches of soft prompts with LoRA, and prefix with LoRA.

## Methods Overview

- **Soft Prompts** 
- **Prefix Tuning** 
- **Full Fine-Tuning**
- **LoRA (Low Rank Adaptation)**
- **Single Layer Fine-Tuning**
- **Soft Prompt + LoRA** 
- **Prefix + LoRA** 




